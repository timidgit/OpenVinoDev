
================================================================================
ENHANCED PHI-3 OPENVINO GENAI CHAT APPLICATION - CONTEXT FILE
================================================================================

Generated: 2025-08-15 11:01:12
Purpose: Comprehensive context for external LLM analysis and development assistance
Project: Production-ready, modular Phi-3-mini-128k-instruct chat application

================================================================================
PROJECT OVERVIEW
================================================================================

ARCHITECTURE TYPE: Modular, production-ready implementation
PRIMARY MODEL: microsoft/Phi-3-mini-128k-instruct
OPTIMIZATION TARGET: Intel NPU with OpenVINO GenAI
FEATURES: RAG document processing, ReAct agents, real-time performance monitoring

KEY ARCHITECTURAL PRINCIPLES:
• 4-tier configuration priority (CLI > Env > JSON > Defaults)
• Stateful OpenVINO API usage (conversation state managed internally)
• Modular separation of concerns (config, model, chat, ui, streaming)
• Comprehensive error handling with device fallback (NPU → CPU)
• Security-focused input validation and sanitization

CRITICAL TECHNICAL CONSTRAINTS:
• NPU Context Limit: ~8,192 tokens (hardware limitation)
• Full 128k Context: Available on CPU device only
• NPUW Configuration: Must use FAST_COMPILE/BEST_PERF hints
• Gradio Compatibility: Requires List[Dict[str, str]] streaming format
• Legacy Naming: Some functions retain 'qwen3' names for backward compatibility

PRODUCTION QUALITY GATES:
✅ NPU compilation succeeds with target configuration
✅ CPU fallback operates correctly when NPU unavailable
✅ Gradio streaming works without format errors
✅ Performance metrics meet targets (>15 tok/sec NPU, >5 tok/sec CPU)
✅ Memory usage stays within device constraints
✅ Full 128k context utilization on CPU without crashes

================================================================================
ARCHITECTURE DIAGRAM
================================================================================

main.py (Entry Point)
├── app.config → ConfigurationLoader (4-tier priority system)
│   ├── CLI arguments (highest priority)
│   ├── Environment variables
│   ├── JSON configuration file
│   └── Built-in defaults (lowest priority)
│
├── app.model → deploy_llm_pipeline() (multi-tier NPU fallback)
│   ├── Enhanced Phi-3 patterns (npu_patterns.py)
│   ├── Manual NPUW configuration (fallback)
│   ├── Basic OpenVINO properties (minimal)
│   └── CPU fallback (automatic device switching)
│
├── app.ui → create_enhanced_interface() (Gradio with ChatInterface)
│   ├── Dynamic system prompts
│   ├── RAG document upload/processing
│   ├── Performance metrics dashboard
│   └── Real-time streaming responses
│
└── app.chat → enhanced_llm_chat() (stateful OpenVINO + RAG)
    ├── app.streamer → EnhancedLLMStreamer (Phi-3 token filtering)
    ├── app.agent → ReAct pattern (optional tool usage)
    ├── DocumentRAGSystem (vector search + cross-encoder reranking)
    └── InputValidator (security-focused sanitization)

================================================================================
CRITICAL ARCHITECTURE PATTERNS
================================================================================

1. CONFIGURATION ARCHITECTURE (4-tier priority):
   Priority Order: CLI → Environment → JSON → Defaults (4-tier system)
   
2. OPENVINO API USAGE:
   Pattern: Stateful usage (start_chat/generate/finish_chat)
   Critical: Never reconstruct full conversation history
   
3. NPUW CONFIGURATION:
   Hints: FAST_COMPILE for prefill, BEST_PERF for generate (critical for NPU)
   Warning: Using generic PERFORMANCE_HINT causes compilation errors
   
4. GRADIO STREAMING:
   Format: List[Dict[str, str]] with role/content keys (streaming compliance)
   Critical: Must yield from generator, not return generator object
   
5. DEVICE FALLBACK:
   Strategy: NPU → CPU automatic switching with appropriate configs
   Context Limits: NPU: 8k tokens (hardware), CPU: 128k tokens (full Phi-3)

================================================================================
RECENT CRITICAL FIXES & DEBUGGING INSIGHTS
================================================================================

1. GRADIO STREAMING FORMAT ERROR (RESOLVED):
   Problem: "Data incompatible with messages format" errors
   Root Cause: UI event handler returning generator instead of yielding
   Solution: Changed return enhanced_llm_chat(...) to yield from enhanced_llm_chat(...)
   
2. NPUW CONFIGURATION DISCOVERY:
   Problem: NPU compilation failures with generic performance hints
   Root Cause: Generic PERFORMANCE_HINT conflicts with NPUW-specific hints
   Solution: Use only NPUW-specific hints (FAST_COMPILE, BEST_PERF)
   
3. DEPENDENCY MANAGEMENT OPTIMIZATION:
   Problem: Heavy dependencies punishing new users
   Solution: Separated core (requirements.txt) and optional (requirements-rag.txt)
   
4. LEGACY FUNCTION DEPRECATION:
   Issue: Qwen3 naming in Phi-3 codebase causing confusion
   Solution: Added deprecation wrappers with clear migration guidance

================================================================================
FILE CATEGORIES AND STRUCTURE
================================================================================

================================================================================
CATEGORY: CORE DOCUMENTATION
================================================================================

DESCRIPTION: Essential documentation for understanding the project architecture and usage

────────────────────────────────────────────────────────────
FILE: CLAUDE.md
PATH: CLAUDE.md
PURPOSE: Primary development guidelines and architecture patterns
SIZE: 10,546 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## **Project Overview**

This is a **production-ready, modular Phi-3-mini-128k-instruct chat application** using OpenVINO GenAI with Intel NPU optimization. The project has evolved from a legacy Qwen3 monolithic implementation to a professional modular architecture with comprehensive features including RAG document processing, ReAct agent capabilities, and advanced performance monitoring.

## **Architecture Overview**

### **Unified Entry Point**
- **`main.py`**: Single entry point with comprehensive CLI interface and 4-tier configuration priority
- **Modular `app/` directory**: Production codebase with clear separation of concerns
- **`_context_archive/`**: Legacy implementations preserved for reference (not used in production)

### **Core Module Dependencies**
```
main.py
├── app.config → ConfigurationLoader (4-tier priority system)
├── app.model → deploy_llm_pipeline() (multi-tier NPU fallback)  
├── app.ui → create_enhanced_interface() (Gradio with ChatInterface)
└── app.chat → enhanced_llm_chat() (stateful OpenVINO + RAG)
    ├── app.streamer → EnhancedLLMStreamer (Phi-3 token filtering)
    ├── app.agent → ReAct pattern (optional tool usage)
    └── DocumentRAGSystem (vector search + cross-encoder reranking)
```

## **Common Commands**

### **Development & Testing**
```bash
# Primary application launch
python main.py

# System validation (essential for new environments)
python main.py --validate-only

# Debug mode with comprehensive logging
python main.py --debug

# Device testing and profiling
python main.py --device CPU --debug
python main.py --device NPU --npu-profile balanced --debug

# Model utilities
python check_model_config.py
python export_model_for_npu.py --model microsoft/Phi-3-mini-128k-instruct --output phi3-128k-npu
```

### **CI/CD Integration**
```bash
# Linting (matches CI pipeline)
flake8 app/ --count --select=E9,F63,F7,F82 --show-source --statistics
flake8 app/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

# Testing (matches CI expectations)  
pytest tests/ -v --cov=app --cov-report=xml --cov-report=html

# Type checking
mypy app/ --ignore-missing-imports --no-strict-optional

# Security scanning
bandit -r app/
safety check
```

### **Installation**
```bash
# Core dependencies (required)
pip install -r requirements.txt

# RAG capabilities (optional but recommended)
pip install langchain faiss-cpu sentence-transformers

# Agent capabilities (optional)
pip install langchain-core langchain-experimental requests python-dateutil
```

## **Critical Architecture Patterns**

### **Configuration Architecture (4-Tier Priority)**
The system implements a sophisticated configuration merger that prevents common configuration bugs:

```python
# Priority: CLI → Environment → JSON → Defaults
config = ConfigurationLoader()
config.load_from_json("config.json")
config.load_from_environment() 
config.override_from_cli(args)
```

**Critical insight**: Environment variables use `MODEL_PATH` (modern) but legacy `QWEN3_MODEL_PATH` is still supported for backward compatibility.

### **NPU Deployment Strategy (Multi-Tier Fallback)**
```python
# app/model.py implements sophisticated deployment logic:
1. Enhanced Phi-3 patterns (app/npu_patterns.py) 
2. Manual NPUW configuration (fallback)
3. Basic OpenVINO properties (minimal)
4. CPU fallback (automatic device switching)
```

**Critical constraint**: NPU requires specific NPUW hints. The codebase has been corrected to use:
- `"NPUW_LLM_PREFILL_HINT": "FAST_COMPILE"` (NOT "LATENCY" - causes compilation errors)
- `"NPUW_LLM_GENERATE_HINT": "BEST_PERF"` (NOT "LATENCY" - unsupported by current drivers)

### **Stateful OpenVINO API Pattern**
OpenVINO GenAI pipelines manage conversation state internally - **never** reconstruct full conversation history:

```python
# CORRECT: Stateful usage
pipe.start_chat(SYSTEM_PROMPT)        # Initialize with system prompt
pipe.generate(new_message, config)    # Send only the new user message
pipe.finish_chat()                    # Clear conversation state

# WRONG: Stateless usage (causes token limit errors)
full_prompt = apply_chat_template(history + [new_message])  
pipe.generate(full_prompt)  # Re-processes entire conversation
```

### **Gradio ChatInterface Compatibility (CRITICAL FIX)**
The UI uses `gr.Chatbot(type='messages')` which requires strict data format compliance. **Recent major fix resolved persistent streaming errors:**

```python
# REQUIRED format: List[Dict[str, str]]
history = [{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi"}]

# CRITICAL FIX: UI event handler must yield from generator, not return it
def handle_send(message, history):
    yield from enhanced_llm_chat(message, history, generation_settings)  # CORRECT
    # return enhanced_llm_chat(...)  # WRONG - returns generator object

# All streaming functions must yield properly formatted List[Dict] objects
def enhanced_llm_chat(...) -> Iterator[ChatHistory]:
    yield [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
```

**Debug Pattern**: Use `json.dumps(history, indent=2, default=str)` before every `yield` to verify format compliance.

## **Advanced Features Architecture**

### **RAG System (3-Phase Implementation)**
```python
# Phase 1: Basic text processing (.txt, .md, .py, .js, .html, .css, .json)
# Phase 2: Advanced parsing (unstructured library for .pdf, .docx, .pptx)  
# Phase 3: Cross-encoder reranking (BAAI/bge-reranker-base for quality)

# Usage pattern:
rag_context = rag_system.retrieve_context(query, k=3)
if rag_context:
    augmented_prompt = f"Context: {rag_context}\n\nQuestion: {query}"
```

### **Agent System (ReAct Pattern)**
```python
# Tool detection based on keywords triggers agent vs regular chat
if should_use_agent(message):  # Detects math, dates, analysis requests
    response = agent.process_with_tools(message)
else:
    response = enhanced_llm_chat(message, history)  # Regular streaming chat
```

### **Token Streaming with Phi-3 Filtering**
```python
# app/streamer.py implements Phi-3-specific token filtering:
SPECIAL_TOKENS = ["<|system|>", "<|user|>", "<|assistant|>", "<|end|>", "<|endoftext|>"]
# Filters these during streaming to prevent UI corruption
```

## **Development Guidelines**

### **Model Migration Pattern**
When switching models (current: Phi-3, legacy: Qwen3):
1. **Update `config.json`**: Model path, token limits, cache directory
2. **Update `app/npu_patterns.py`**: Model-specific NPUW configurations  
3. **Update `app/streamer.py`**: Special tokens for new model
4. **Update UI labels**: Model name in `app/ui.py`
5. **Test thoroughly**: `python main.py --validate-only` then `--debug`

### **NPU Development Constraints**
- **Token limits**: NPU has hard-coded prompt length limits (current: 8192 for Phi-3)
- **Defensive programming**: Always validate input length before NPU processing
- **NPUW configuration**: Use supported hint values, avoid generic PERFORMANCE_HINT
- **Memory constraints**: NPU has specific memory limitations requiring careful management

### **Testing Strategy**
```bash
# Component testing
python main.py --validate-only  # System requirements validation
python main.py --debug          # Full system debugging

# Device-specific testing  
python main.py --device CPU     # Test CPU fallback path
python main.py --device NPU     # Test NPU compilation and execution

# Configuration testing
python main.py --model-path "path" --debug  # Test custom model loading
```

### **Error Handling Architecture**
The codebase implements multi-tier error handling:
1. **Input validation**: Security-focused sanitization in `app/chat.py`
2. **Device fallback**: NPU → CPU automatic switching in `app/model.py`
3. **User-friendly errors**: Technical details hidden, actionable messages shown
4. **Graceful degradation**: Features disable cleanly when dependencies missing

## **Recent Critical Fixes & Debugging Insights**

### **Gradio Streaming Format Error (RESOLVED)**
**Problem**: `"Data incompatible with messages format"` errors during streaming
**Root Cause**: UI event handler was returning generator object instead of yielding individual values
**Solution**: Changed `return enhanced_llm_chat(...)` to `yield from enhanced_llm_chat(...)`
**Impact**: Complete resolution of persistent chat interface crashes

### **NPUW Configuration Discovery**
**Problem**: NPU compilation failures with certain hint configurations  
**Root Cause**: Generic `PERFORMANCE_HINT` conflicts with NPUW-specific hints
**Solution**: Use only NPUW-specific hints (`FAST_COMPILE`, `BEST_PERF`) with NPU device

### **History Format Normalization**
**Problem**: Inconsistent message format handling between different Gradio versions
**Solution**: Bulletproof `prepare_chat_input()` function that explicitly rebuilds history on every turn

## **Quality Gates & Production Readiness**

A feature is production-ready when:
- ✅ NPU compilation succeeds with target configuration
- ✅ CPU fallback operates correctly when NPU unavailable
- ✅ User receives clear feedback about active device/configuration
- ✅ Performance metrics meet targets (>15 tokens/sec NPU, >5 tokens/sec CPU)
- ✅ Memory usage stays within device constraints
- ✅ Conversations leverage full 128k context without crashes
- ✅ **Gradio streaming works without format errors**
- ✅ All CI/CD checks pass (linting, testing, type checking, security)

## **Critical Technical Debt & Legacy Naming**

**Note**: Some function/class names retain legacy "qwen3" naming for backward compatibility:
- `enhanced_qwen3_chat()` → processes Phi-3 model
- `EnhancedQwen3Streamer()` → handles Phi-3 token streaming  
- `deploy_qwen3_pipeline()` → deploys Phi-3 pipeline

This naming is intentionally preserved to maintain configuration compatibility and reduce breaking changes during the model transition.

## **Archive System**

The `_context_archive/` directory demonstrates professional technical debt management:
- **Legacy implementations**: Previous monolithic architecture preserved
- **Reference patterns**: Gradio examples and testing frameworks
- **Documentation**: Historical context and enhancement guides  
- **Docker configs**: Containerization examples for different deployment scenarios

This approach enables safe architectural evolution while preserving institutional knowledge.
────────────────────────────────────────────────────────────
END OF FILE: CLAUDE.md
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: README.md
PATH: README.md
PURPOSE: User-facing documentation and installation guide
SIZE: 16,635 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

# Enhanced Phi-3 OpenVINO GenAI Chat Application

A production-ready, modular implementation of microsoft/Phi-3-mini-128k-instruct chat interface using OpenVINO GenAI with Intel NPU optimization, RAG document processing, and comprehensive performance monitoring. Built with professional software engineering practices and a modular architecture for maximum maintainability and scalability.

![License](https://img.shields.io/badge/License-MIT-blue.svg)
![Python](https://img.shields.io/badge/Python-3.8+-green.svg)
![OpenVINO](https://img.shields.io/badge/OpenVINO-2024.4+-orange.svg)

## ✨ Features

### 🚀 **Performance Optimization**
- **Complete Intel NPU optimization** with NPUW (NPU Wrapper) configuration
- **Phi-3-specific tuning** with model architecture awareness and 128k context support
- **Intelligent device fallback** (NPU → CPU) with appropriate configurations
- **Professional performance monitoring** with real-time metrics
- **Fixed NPUW configuration issues** for stable NPU compilation

### 🎯 **Advanced Capabilities**
- **🎯 Dynamic System Prompts**: Real-time AI behavior customization
- **📚 RAG Document Processing**: Upload and query your own documents
- **🔍 Special Token Filtering** for clean Phi-3 output
- **📊 Real-time Performance Monitoring** with comprehensive metrics
- **⚙️ Multi-source Configuration** (CLI, env vars, JSON)

### 🖥️ **Professional User Experience**
- **Modern Gradio interface** with professional theming and accordions
- **Real-time streaming responses** with token-level filtering
- **Smart message processing** with intelligent truncation
- **Document upload and processing** for context-aware conversations
- **Security-focused input validation** and sanitization

### 🏗️ **Enterprise-Ready Architecture**
- **Modular codebase** with clear separation of concerns
- **Command-line interface** with comprehensive argument support
- **Configuration management** with environment override support
- **Professional error handling** with detailed diagnostics
- **Comprehensive logging** and debugging capabilities

## 🚀 Quick Start

**New users start here!** Get up and running in 5 minutes:

1. **Install core dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set your model path**
   ```bash
   export MODEL_PATH="/path/to/your/phi3-128k-npu"
   # or set in config.json
   ```

3. **Run the application**
   ```bash
   python main.py
   ```

4. **Open your browser** to `http://127.0.0.1:7860`

**For advanced features** (document upload, RAG processing), also install:
```bash
pip install -r requirements-rag.txt
```

## 🛠️ Complete Installation Guide

### Prerequisites
- **Python 3.8+**
- **OpenVINO 2024.4+** with GenAI support
- **Intel NPU drivers** (for NPU acceleration)
- **microsoft/Phi-3-mini-128k-instruct model** in OpenVINO format

### Quick Setup

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd OpenVinoDev
```

2. **Install dependencies**
```bash
# Core dependencies (required)
pip install -r requirements.txt

# RAG and advanced features (optional)
pip install -r requirements-rag.txt
```

3. **Set up OpenVINO environment**
```bash
# Source OpenVINO environment variables
source /opt/intel/openvino/setupvars.sh  # Linux
# or
"C:\Program Files (x86)\Intel\openvino\setupvars.bat"  # Windows
```

4. **Configure the application**

**Option A: Environment Variables**
```bash
export MODEL_PATH="/path/to/your/phi3-128k-npu"
export TARGET_DEVICE="NPU"  # or "CPU"
export NPU_PROFILE="balanced"  # conservative, balanced, aggressive
```

**Option B: Configuration File (Recommended)**
```bash
# Copy example configuration
cp config.example.json config.json

# Edit config.json with your settings
nano config.json
```

**Option C: Command-Line Arguments**
```bash
# Use CLI arguments for quick configuration changes
python main.py --device CPU --npu-profile conservative
```

5. **Run the application**
```bash
# Modular entry point (unified architecture)
python main.py
```

## 📋 Requirements

### Hardware
- **Recommended**: Intel NPU-enabled system (Meteor Lake, Arrow Lake, or later)
- **Alternative**: Intel CPU with AVX-512 support
- **Memory**: 8GB+ RAM for optimal performance

### Software
**Core Dependencies** (in `requirements.txt`):
```
openvino-genai>=2024.4
gradio>=4.0.0
transformers>=4.30.0
numpy>=1.21.0
typing-extensions>=4.0.0
psutil>=5.8.0
```

**RAG Dependencies** (optional, in `requirements-rag.txt`):
```
langchain>=0.1.0
faiss-cpu>=1.7.4
sentence-transformers>=2.7.0
torch>=2.0.0
```

**Installation**: Install core dependencies with `pip install -r requirements.txt`. For advanced features, also install `pip install -r requirements-rag.txt`.

## 🚀 Usage

### Basic Usage
```bash
# Start with default configuration
python main.py

# Use specific device
python main.py --device CPU

# Custom model path and profile
python main.py --model-path ./models/phi3-128k-npu --npu-profile balanced

# Enable public sharing (use with caution)
python main.py --share

# Show all available options
python main.py --help
```

The application will:
1. **Auto-detect** available hardware (NPU/CPU)
2. **Deploy** Phi-3 model with optimal configuration
3. **Launch** web interface at `http://127.0.0.1:7860`

### Configuration Options

**Environment Variables:**
```bash
export MODEL_PATH="/path/to/model"            # Model location
export CACHE_DIR="/path/to/cache"             # Cache directory
export NPU_PROFILE="balanced"                 # balanced|conservative|aggressive
export MAX_MESSAGE_LENGTH="2000"              # Max input length (increased for Phi-3 128k context)
```

**NPU Profiles:**
- **Conservative**: Lower memory usage, smaller contexts
- **Balanced**: Optimal for most use cases (default)
- **Aggressive**: Maximum performance, higher memory usage

## ⚡ Long Context on NPU

**Important**: While Phi-3 supports 128k context length, **Intel NPU has hardware limitations**:

- **NPU Context Limit**: ~8,192 tokens (hardware constraint)
- **Full 128k Context**: Available on CPU device only
- **Automatic Fallback**: Long conversations automatically switch to CPU

**Best Practices:**
```bash
# For long documents/conversations, use CPU explicitly
python main.py --device CPU

# For quick interactions, NPU provides faster response
python main.py --device NPU  # Default behavior
```

**Context Management:**
- NPU: Ideal for < 6k token conversations (~15-20 exchanges)
- CPU: Required for full 128k context utilization
- Auto-truncation prevents NPU overflows with user warnings

## 🏗️ Architecture

### Core Components

```
OpenVinoDev/
├── app/                        # Modular application architecture
│   ├── __init__.py            # Package initialization & exports
│   ├── config.py              # Configuration management
│   ├── model.py               # Model deployment & system initialization
│   ├── streamer.py            # Token streaming & filtering
│   ├── chat.py                # Core chat processing & RAG
│   └── ui.py                  # Gradio interface & event handling
├── _context_archive/          # Archived legacy code and context files
│   ├── archive/               # Legacy implementations
│   ├── context/               # Reference examples and patterns
│   ├── tests/                 # Unit tests
│   └── documentation/         # Additional docs and licenses
├── main.py                    # Application entry point with CLI
├── config.json               # Configuration file
└── requirements.txt          # Dependencies
```

### Key Components

**Configuration Management (`app/config.py`)**
- **`ConfigurationLoader`**: Multi-source configuration (JSON, env vars, CLI)
- **JSON/Environment/CLI priority system** for flexible configuration

**Model Management (`app/model.py`)**
- **`deploy_qwen3_pipeline()`**: NPU-optimized model deployment (legacy naming)
- **`initialize_system_with_validation()`**: Comprehensive system initialization
- **Configuration management**: Device and profile-specific configurations for Phi-3

**Streaming & Performance (`app/streamer.py`)**
- **`EnhancedQwen3Streamer`**: Real-time response streaming with token filtering (legacy naming)
- **`StreamingMetrics`**: Performance monitoring and diagnostics

**Chat Processing (`app/chat.py`)**
- **`enhanced_qwen3_chat()`**: Main chat processing with RAG integration (legacy naming)
- **`DocumentRAGSystem`**: Document upload and context retrieval
- **`InputValidator`**: Security-focused input validation

**User Interface (`app/ui.py`)**
- **`create_enhanced_interface()`**: Professional Gradio interface
- **Dynamic system prompts, RAG uploads, performance monitoring**

## 🔧 Advanced Configuration

### NPU Optimization

The application includes comprehensive NPUW configuration for Intel NPU:

```python
npu_config = {
    "NPU_USE_NPUW": "YES",
    "NPUW_LLM": "YES", 
    "NPUW_LLM_MAX_PROMPT_LEN": 8192,  # Increased for Phi-3 128k context
    "NPUW_LLM_MIN_RESPONSE_LEN": 256,
    "NPUW_LLM_PREFILL_HINT": "FAST_COMPILE",  # Corrected: FAST_COMPILE for stable compilation
    "NPUW_LLM_GENERATE_HINT": "BEST_PERF"     # Corrected: BEST_PERF for optimal generation
}
```

### Custom Profiles

Create custom optimization profiles:

```python
custom_profile = NPUPerformanceProfile(
    max_prompt_len=4096,
    min_response_len=512, 
    cache_mode="OPTIMIZE_SPEED",
    performance_hint="LATENCY",
    compilation_strategy="OPTIMAL"
)
```

## 📊 Performance Monitoring

Access real-time performance metrics through the UI:
- **Response times** and token generation rates
- **Device utilization** and memory usage  
- **Error rates** and compilation diagnostics
- **Token filtering** statistics

## 🛠️ Troubleshooting

### Common Issues

**NPU Compilation Errors:**
```bash
# Check NPUW configuration
export NPUW_LLM=YES
export NPU_USE_NPUW=YES

# Verify driver installation
ov-device-list
```

**Memory Issues:**
- Reduce `MAX_PROMPT_LEN` in configuration
- Use "conservative" NPU profile
- Enable `NPU_LOW_MEMORY_MODE`

**Slow Performance:**
- Verify NPU drivers are installed
- Check cache directory permissions
- Use "aggressive" profile for maximum speed

### Debug Mode

Enable detailed logging:
```bash
# Use debug flag with modular version
python main.py --debug

```

### Command-Line Diagnostics

The new modular version includes comprehensive CLI diagnostics:

```bash
# Validate system requirements only
python main.py --validate-only

# Debug mode with verbose logging
python main.py --debug

# Test specific configurations
python main.py --device CPU --debug
python main.py --npu-profile conservative --debug
```

## 🎯 New Features Guide

### Dynamic System Prompts

Customize AI behavior in real-time:

1. **Access**: Click "🎯 System Prompt Configuration" in the interface
2. **Edit**: Modify the prompt to change AI behavior, expertise, and style
3. **Apply**: Click "✅ Apply & Clear Chat" to activate changes
4. **Reset**: Use "🔄 Reset to Default" to restore original prompt

**Example Custom Prompt:**
```
You are a Python coding expert specializing in data science and machine learning.

Key behaviors:
- Provide complete, working code examples
- Explain complex algorithms step-by-step
- Include relevant imports and dependencies
- Focus on best practices and optimization

You excel at: pandas, numpy, scikit-learn, and deep learning frameworks.
```

### RAG Document Processing

Upload and query your own documents:

1. **Upload**: Click "📚 Document Upload (RAG)" and select files
2. **Supported**: `.txt`, `.md`, `.py`, `.js`, `.html`, `.css`, `.json` files
3. **Process**: Wait for "✅ Successfully processed" confirmation
4. **Query**: Ask questions about your uploaded content
5. **Context**: AI will reference relevant document sections in responses

**Example Queries:**
- "What does the uploaded document say about error handling?"
- "Summarize the key points from the technical specification"
- "How should I implement the feature described in the documentation?"

### Performance Monitoring

Access real-time metrics:

1. **View**: Click "📊 Performance Metrics" 
2. **Monitor**: Real-time response times, tokens/second, device utilization
3. **Analyze**: Special tokens filtered, compilation errors, cache hits
4. **Reset**: Use "🔄 Reset Metrics" to clear historical data

## 🤝 Contributing

Contributions are welcome! This is a hobby project aimed at showcasing OpenVINO GenAI capabilities.

### Areas for Enhancement
- Additional model support (Llama, ChatGLM, etc.)
- Multi-modal capabilities (vision, audio)
- Distributed inference support
- Advanced UI features

### Development Setup
1. Fork the repository
2. Create feature branch
3. Follow existing code patterns
4. Add tests for new functionality
5. Submit pull request

## 📚 Documentation

- **[ACKNOWLEDGMENTS.md](ACKNOWLEDGMENTS.md)**: Third-party attributions
- **[DISCLAIMER.md](DISCLAIMER.md)**: Usage guidelines and disclaimers
- **[CLAUDE.md](CLAUDE.md)**: Development guidelines and context
- **`context/README.md`**: Enhanced context system documentation

## 🔍 Examples

### Simple Chat
```python
# Basic usage
message = "Explain quantum computing"
response = enhanced_qwen3_chat(message, [])  # Legacy function naming
```

### Performance Monitoring
```python
# Access metrics
metrics = system_metrics.to_display_dict()
print(f"Avg response time: {metrics['Avg Response Time']}")
```

### Custom Configuration  
```python
# Deploy with custom config
from app.npu_patterns import initialize_phi3_pipeline
pipeline = initialize_phi3_pipeline(model_path, profile="aggressive")
```

## 📈 Performance Targets

### Intel NPU (Typical)
- **Load Time**: 60-90 seconds (first run), 30-60 seconds (cached)
- **First Token**: <2 seconds latency
- **Generation**: 10-20 tokens/second (Phi-3 is smaller than Qwen3, slightly different performance)
- **Memory**: Optimized for NPU constraints
- **Context**: Up to 128k tokens supported

### CPU Fallback
- **Load Time**: 10-25 seconds (Phi-3 is 3.8B vs 8B parameters)
- **Generation**: 5-12 tokens/second  
- **Memory**: 6GB+ recommended (lower than Qwen3 due to smaller model size)

## 🔄 Version History

- **v1.0 (Enhanced)**: Production-ready with complete optimization
- **v0.9 (Refined)**: Hybrid architecture with consultant insights
- **v0.8 (Debug)**: NPU-optimized with debugging features
- **v0.7**: Basic Gradio integration

## ⚖️ Legal & Licensing

### License
This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

### Third-Party Components
This project integrates with several third-party components, each with their own licenses:
- **OpenVINO GenAI**: Apache 2.0 License (Intel Corporation)
- **Phi-3 Models**: MIT License (Microsoft Corporation)
- **Gradio**: Apache 2.0 License (HuggingFace)
- **Transformers**: Apache 2.0 License (HuggingFace)

See [ACKNOWLEDGMENTS.md](ACKNOWLEDGMENTS.md) for detailed attributions.

### Model Usage
This software is designed to work with microsoft/Phi-3-mini-128k-instruct. Users must:
1. **Obtain Phi-3 models** through official channels (HuggingFace, etc.)
2. **Comply with Phi-3 model license terms** (MIT License with responsible AI guidelines)
3. **Follow responsible AI usage guidelines**

### Disclaimer
This is a **hobby/educational project** and is not intended for commercial use. Users are responsible for ensuring compliance with all applicable licenses and terms of service.

See [DISCLAIMER.md](DISCLAIMER.md) for complete usage guidelines.

## 🙏 Acknowledgments

This project builds upon the excellent work of many open-source contributors:

- **Intel Corporation** for OpenVINO GenAI and NPU optimization techniques
- **Microsoft Corporation** for the Phi-3 model architecture and specifications
- **HuggingFace** for Gradio, Transformers, and the broader AI ecosystem
- **The open-source AI community** for sharing knowledge and best practices

See [ACKNOWLEDGMENTS.md](ACKNOWLEDGMENTS.md) for complete attributions.

---

## 🔗 Links

- **OpenVINO GenAI**: https://github.com/openvinotoolkit/openvino.genai
- **Phi-3 Models**: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct  
- **Gradio**: https://gradio.app/
- **Intel NPU**: https://www.intel.com/content/www/us/en/products/docs/processors/core/core-processors-with-intel-npu.html

---

*⭐ If this project helps you, please consider giving it a star!*

*🐛 Found a bug? Please open an issue!*

*💡 Have an idea? Contributions are welcome!*
────────────────────────────────────────────────────────────
END OF FILE: README.md
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: requirements.txt
PATH: requirements.txt
PURPOSE: Core dependencies for basic functionality
SIZE: 1,248 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

# Enhanced Phi-3-mini-128k-instruct OpenVINO GenAI Chat Application
# Core Requirements for basic chat functionality

# Core Dependencies (Required)
openvino-genai>=2024.4.0
gradio>=4.0.0
transformers>=4.30.0

# Essential Python Libraries
numpy>=1.21.0
typing-extensions>=4.0.0

# Optional Performance Libraries (recommended)
psutil>=5.8.0  # For system monitoring and resource tracking

# RAG Dependencies (Optional - install separately)
# For advanced document processing, embeddings, and agentic capabilities:
# pip install -r requirements-rag.txt

# Development Dependencies (optional)
# Uncomment for development/testing:
pytest>=6.0.0
pytest-cov>=4.0.0
pytest-mock>=3.10.0
flake8>=4.0.0
bandit>=1.7.0
safety>=2.3.0
mypy>=1.0.0

# Note: OpenVINO toolkit should be installed separately following official instructions:
# https://docs.openvino.ai/latest/openvino_docs_install_guides_overview.html
#
# For NPU support, ensure Intel NPU drivers are installed:
# https://www.intel.com/content/www/us/en/products/docs/processors/core/core-processors-with-intel-npu.html
#
# Model Requirements:
# - microsoft/Phi-3-mini-128k-instruct in OpenVINO format
# - Download from official sources (HuggingFace)
# - Ensure compliance with model licensing terms
────────────────────────────────────────────────────────────
END OF FILE: requirements.txt
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: requirements-rag.txt
PATH: requirements-rag.txt
PURPOSE: Optional dependencies for advanced RAG features
SIZE: 1,071 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

# Enhanced Phi-3 OpenVINO GenAI Chat Application
# RAG Dependencies (Optional)
# 
# Install these for advanced document processing and agentic capabilities:
# pip install -r requirements-rag.txt

# Core RAG Dependencies
langchain>=0.1.0
langchain-huggingface>=0.1.0  # New recommended package for HuggingFace integrations
faiss-cpu>=1.7.4
sentence-transformers>=2.7.0  # Unified version for embeddings and reranking

# Advanced Document Parsing (Phase 3.1)
unstructured[local-inference]==0.15.7
langchain-unstructured==0.1.2

# Cross-Encoder Reranking (Phase 3.2)
torch>=2.0.0  # Required for sentence-transformers models

# Agentic Architecture (Phase 3.3)
langchain-core>=0.1.0
langchain-experimental>=0.3.0
requests>=2.25.0  # For web search tool
python-dateutil>=2.8.0  # For date/time tools

# Note: These dependencies are optional but provide enhanced functionality:
# - Document upload and processing for RAG
# - Cross-encoder reranking for better retrieval quality
# - Agent capabilities with tool usage
# - Support for multiple document formats (PDF, DOCX, etc.)
────────────────────────────────────────────────────────────
END OF FILE: requirements-rag.txt
────────────────────────────────────────────────────────────


================================================================================
CATEGORY: APPLICATION ENTRY POINTS
================================================================================

DESCRIPTION: Main application entry points and configuration

────────────────────────────────────────────────────────────
FILE: main.py
PATH: main.py
PURPOSE: Primary application entry point with CLI interface
SIZE: 12,120 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

#!/usr/bin/env python3
"""
Enhanced Phi-3 OpenVINO GenAI Chat Application
==============================================

Main entry point for the modular, production-ready implementation of Phi-3-mini-128k-instruct
chat interface using OpenVINO GenAI with Intel NPU optimization and RAG capabilities.

Copyright (c) 2025 sbran
Licensed under the MIT License - see LICENSE file for details

Usage:
    python main.py                          # Use defaults from config.json
    python main.py --device CPU             # Force CPU device
    python main.py --model-path /path/model # Use custom model path
    python main.py --help                   # Show all options

Features:
- Complete Phi-3 NPUW optimization with 128k context support
- Dynamic system prompt configuration
- RAG document processing
- Professional performance monitoring  
- Robust error handling and diagnostics
- Modular architecture for maintainability
"""

import argparse
import os
import sys
from typing import Optional

# Import application modules
from app.config import initialize_config


class ConfigurationError(Exception):
    """Raised when configuration validation fails."""
    pass
from app.model import initialize_system_with_validation
from app.chat import initialize_globals as init_chat_globals
from app.ui import create_enhanced_interface, initialize_ui_globals
from app.streamer import streaming_metrics


def create_argument_parser() -> argparse.ArgumentParser:
    """
    Create command-line argument parser with comprehensive options.
    
    Returns:
        Configured ArgumentParser instance
    """
    parser = argparse.ArgumentParser(
        description="Enhanced Phi-3 OpenVINO GenAI Chat Application",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py                              # Use config.json defaults
  python main.py --device CPU                 # Force CPU device  
  python main.py --npu-profile conservative  # Use conservative NPU settings
  python main.py --model-path ./models/phi3-128k-npu # Custom model location
  python main.py --share                     # Enable public sharing (use with caution)
  python main.py --port 8080                 # Use custom port

Configuration Priority:
  1. Command-line arguments (highest priority)
  2. Environment variables
  3. config.json file
  4. Built-in defaults (lowest priority)

Environment Variables:
  MODEL_PATH           - Model directory path
  QWEN3_MODEL_PATH     - Model directory path (deprecated, use MODEL_PATH)
  TARGET_DEVICE        - Target device (NPU, CPU, GPU, AUTO)
  NPU_PROFILE          - NPU optimization profile
  CACHE_DIR            - Cache directory location
  MAX_MESSAGE_LENGTH   - Maximum message length
  GENERATION_TIMEOUT   - Generation timeout in seconds
  GRADIO_SHARE         - Enable public sharing (true/false)
        """
    )
    
    # Model configuration
    model_group = parser.add_argument_group("Model Configuration")
    model_group.add_argument(
        "--model-path",
        type=str,
        help="Path to the OpenVINO model directory (overrides config.json and env var)"
    )
    
    # Device configuration
    device_group = parser.add_argument_group("Device Configuration")
    device_group.add_argument(
        "--device",
        type=str,
        choices=["NPU", "CPU", "GPU", "AUTO"],
        help="Target device for inference (default: NPU)"
    )
    device_group.add_argument(
        "--npu-profile",
        type=str,
        choices=["conservative", "balanced", "aggressive"],
        help="NPU optimization profile (default: balanced)"
    )
    device_group.add_argument(
        "--cache-dir",
        type=str,
        help="OpenVINO cache directory path"
    )
    
    # Application configuration
    app_group = parser.add_argument_group("Application Configuration")
    app_group.add_argument(
        "--config",
        type=str,
        default="config.json",
        help="Path to configuration file (default: config.json)"
    )
    app_group.add_argument(
        "--max-message-length",
        type=int,
        help="Maximum message length in characters"
    )
    app_group.add_argument(
        "--generation-timeout",
        type=float,
        help="Generation timeout in seconds"
    )
    
    # Gradio interface configuration
    gradio_group = parser.add_argument_group("Interface Configuration")
    gradio_group.add_argument(
        "--host",
        type=str,
        default="127.0.0.1",
        help="Host to bind the interface to (default: 127.0.0.1)"
    )
    gradio_group.add_argument(
        "--port",
        type=int,
        default=7860,
        help="Port to bind the interface to (default: 7860)"
    )
    gradio_group.add_argument(
        "--share",
        action="store_true",
        help="Enable public sharing via Gradio (WARNING: Security risk)"
    )
    gradio_group.add_argument(
        "--auth",
        type=str,
        help="Basic authentication (format: username:password)"
    )
    gradio_group.add_argument(
        "--max-file-size",
        type=str,
        default="10mb",
        help="Maximum file upload size (default: 10mb)"
    )
    
    # Development options
    dev_group = parser.add_argument_group("Development Options")
    dev_group.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug mode with verbose logging"
    )
    dev_group.add_argument(
        "--validate-only",
        action="store_true",
        help="Only validate system requirements, don't start interface"
    )
    dev_group.add_argument(
        "--reset-metrics",
        action="store_true",
        help="Reset performance metrics on startup"
    )
    
    return parser


def validate_arguments(args: argparse.Namespace) -> None:
    """
    Validate command-line arguments.
    
    Args:
        args: Parsed command-line arguments
        
    Raises:
        ConfigurationError: If validation fails
    """
    # Validate model path if provided
    if args.model_path and not os.path.exists(args.model_path):
        raise ConfigurationError(f"Model path does not exist: {args.model_path}")
    
    # Validate cache directory if provided
    if args.cache_dir:
        cache_parent = os.path.dirname(args.cache_dir)
        if cache_parent and not os.path.exists(cache_parent):
            try:
                os.makedirs(cache_parent, exist_ok=True)
            except (PermissionError, OSError) as e:
                raise ConfigurationError(f"Cannot create cache directory parent: {cache_parent} ({e})")
    
    # Validate authentication format
    if args.auth and ':' not in args.auth:
        raise ConfigurationError("Authentication must be in format 'username:password'")
    
    # Validate port range
    if not (1024 <= args.port <= 65535):
        raise ConfigurationError(f"Port must be between 1024 and 65535, got {args.port}")
    
    # Warn about security risks (these are warnings, not errors)
    if args.share:
        print("⚠️ WARNING: Public sharing enabled. Your application will be accessible from the internet.")
        print("   Ensure you trust all users who might access it.")
    
    if args.host != "127.0.0.1":
        print(f"⚠️ WARNING: Binding to {args.host}. Make sure your firewall is properly configured.")


def setup_launch_config(args: argparse.Namespace) -> dict:
    """
    Setup Gradio launch configuration from arguments.
    
    Args:
        args: Parsed command-line arguments
        
    Returns:
        Dictionary with launch configuration
    """
    launch_config = {
        "server_name": args.host,
        "server_port": args.port,
        "share": args.share or os.getenv("GRADIO_SHARE", "").lower() in ('true', '1', 'yes'),
        "show_error": True,
        "quiet": not args.debug,
        "max_file_size": args.max_file_size,
        "allowed_paths": []  # No file access by default for security
    }
    
    # Add authentication if provided
    if args.auth:
        username, password = args.auth.split(':', 1)
        launch_config["auth"] = (username, password)
        print(f"✅ Basic authentication enabled for user: {username}")
    
    return launch_config


def main():
    """Main application entry point"""
    
    # Parse command-line arguments
    parser = create_argument_parser()
    args = parser.parse_args()
    
    try:
        # Validate arguments
        validate_arguments(args)
    except ConfigurationError as e:
        print(f"❌ Configuration Error: {e}")
        sys.exit(1)
    
    # Setup debug logging if requested
    if args.debug:
        print("🔧 Debug mode enabled")
        import logging
        logging.basicConfig(level=logging.DEBUG)
    
    print("🚀 Starting Enhanced Phi-3 Chat Application")
    print("=" * 60)
    
    try:
        # Initialize configuration system
        print("🔧 Loading configuration...")
        config = initialize_config(args.config, args)
        
        if args.validate_only:
            print("🔍 Validation-only mode - checking system requirements...")
            # Import validation function
            from app.model import validate_system_requirements
            issues = validate_system_requirements()
            
            if issues:
                print("❌ Validation failed:")
                for i, issue in enumerate(issues, 1):
                    print(f"   {i}. {issue}")
                sys.exit(1)
            else:
                print("✅ All system requirements validated successfully")
                sys.exit(0)
        
        # Reset metrics if requested
        if args.reset_metrics:
            streaming_metrics.reset()
            print("📊 Performance metrics reset")
        
        # Initialize system with validation
        print("🚀 Initializing system...")
        pipeline, tokenizer, device_used, config_used, load_time = initialize_system_with_validation()
        
        # Initialize global instances
        init_chat_globals(pipeline, tokenizer)
        initialize_ui_globals(pipeline, device_used, config_used, load_time)
        
        # Create Gradio interface
        print("🌐 Creating Gradio interface...")
        demo = create_enhanced_interface()
        
        # Setup launch configuration
        launch_config = setup_launch_config(args)
        
        # Display startup information
        print("✨ Enhanced Phi-3 Chat System Ready!")
        print("=" * 60)
        print("Features Enabled:")
        print("   🎯 Dynamic System Prompts")
        print("   📚 RAG Document Processing") 
        print("   🔍 Advanced Token Filtering")
        print("   📊 Real-time Performance Monitoring")
        print("   🛡️ Security & Input Validation")
        print("   🏗️ Modular Architecture")
        print("=" * 60)
        print(f"🌐 Starting server on {args.host}:{args.port}")
        print(f"🎯 Device: {device_used} | Config: {config_used}")
        print(f"⏱️ Load Time: {load_time:.1f}s")
        
        if launch_config["share"]:
            print("🔗 Public sharing enabled - link will be displayed after startup")
        
        print("=" * 60)
        
        # Launch the application
        demo.queue(
            max_size=20,
            default_concurrency_limit=1  # NPU works best with single concurrent requests
        ).launch(**launch_config)
        
    except KeyboardInterrupt:
        print("\n🛑 Application stopped by user")
        sys.exit(0)
    
    except SystemExit:
        # Re-raise SystemExit to preserve exit codes
        raise
    
    except Exception as e:
        print(f"💥 Unexpected error: {e}")
        
        if args.debug:
            import traceback
            traceback.print_exc()
        else:
            print("   Use --debug flag for detailed error information")
        
        print("\n🆘 This may be a configuration or system issue")
        print("   Check your model path, device drivers, and dependencies")
        sys.exit(1)


if __name__ == "__main__":
    main()
────────────────────────────────────────────────────────────
END OF FILE: main.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: config.json
PATH: config.json
PURPOSE: Production configuration file
SIZE: 1,074 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

{
  "model": {
    "path": "C:\\Users\\sbran\\OneDrive\\Desktop\\OpenVinoDev\\phi3-128k-npu-fixed",
    "name": "Phi-3-mini-128k-instruct",
    "type": "phi3"
  },
  "deployment": {
    "target_device": "NPU",
    "npu_profile": "balanced",
    "fallback_device": "CPU",
    "cache_directory": "./cache/.ovcache_phi3"
  },
  "generation": {
    "max_new_tokens": 2048,
    "temperature": 0.6,
    "top_p": 0.95,
    "top_k": 20,
    "repetition_penalty": 1.1,
    "do_sample": true
  },
  "ui": {
    "max_message_length": 2000,
    "max_conversation_tokens": 8000,
    "emergency_limit": 16384,
    "show_performance_metrics": true,
    "theme": "soft"
  },
  "performance": {
    "generation_timeout": 30.0,
    "truncation_warning_delay": 0.5,
    "ui_update_interval": 0.1,
    "thread_pool_size": 2
  },
  "logging": {
    "log_level": "INFO",
    "enable_performance_logging": true,
    "enable_token_filtering_logs": false
  },
  "security": {
    "validate_input": true,
    "max_file_uploads": 0,
    "allowed_file_types": [],
    "rate_limit_requests": false
  }
}
────────────────────────────────────────────────────────────
END OF FILE: config.json
────────────────────────────────────────────────────────────


================================================================================
CATEGORY: CORE MODULES
================================================================================

DESCRIPTION: Core application modules implementing the main functionality

────────────────────────────────────────────────────────────
FILE: __init__.py
PATH: app\__init__.py
PURPOSE: Package initialization and public API
SIZE: 2,151 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

"""
Enhanced OpenVINO GenAI Chat Application
========================================

A modular, production-ready implementation of Phi-3-mini-128k-instruct chat interface
using OpenVINO GenAI with Intel NPU optimization and RAG capabilities.

Copyright (c) 2025 sbran
Licensed under the MIT License - see LICENSE file for details

Modules:
--------
- config: Configuration management and environment handling
- model: Pipeline deployment and system initialization  
- streamer: Token streaming and filtering for LLMs
- chat: Core chat processing with RAG integration
- ui: Gradio interface creation and event handling
"""

import warnings
from typing import Any

__version__ = "2.0.0"
__author__ = "sbran"
__license__ = "MIT"

# Import key components for easy access
from .config import ConfigurationLoader
from .model import deploy_llm_pipeline, initialize_system_with_validation
from .streamer import EnhancedLLMStreamer
from .chat import enhanced_llm_chat
from .ui import create_enhanced_interface


def enhanced_qwen3_chat(*args, **kwargs) -> Any:
    """
    Deprecated wrapper for enhanced_llm_chat.
    
    This function maintains backward compatibility for legacy code that uses
    the 'qwen3' naming convention. It now processes Phi-3 models.
    
    Args:
        *args: Arguments passed to enhanced_llm_chat
        **kwargs: Keyword arguments passed to enhanced_llm_chat
        
    Returns:
        Result from enhanced_llm_chat
        
    Deprecated:
        Use enhanced_llm_chat instead. This wrapper will be removed in v3.0.0
    """
    warnings.warn(
        "enhanced_qwen3_chat is deprecated and will be removed in v3.0.0. "
        "Use enhanced_llm_chat instead. Note: This function now processes Phi-3 models, "
        "not Qwen3 models.",
        DeprecationWarning,
        stacklevel=2
    )
    return enhanced_llm_chat(*args, **kwargs)


__all__ = [
    "ConfigurationLoader",
    "deploy_llm_pipeline", 
    "initialize_system_with_validation",
    "EnhancedLLMStreamer",
    "enhanced_llm_chat",
    "create_enhanced_interface",
    # Deprecated functions (will be removed in v3.0.0)
    "enhanced_qwen3_chat"
]
────────────────────────────────────────────────────────────
END OF FILE: __init__.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: config.py
PATH: app\config.py
PURPOSE: Configuration management with 4-tier priority
SIZE: 6,886 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

"""
Configuration Management Module
=============================

Handles configuration loading from multiple sources including JSON files,
environment variables, and command-line arguments.
"""

import json
import os
from typing import Any, Dict
from typing_extensions import TypedDict


# Type definitions
ConfigDict = Dict[str, Any]


class ConfigurationLoader:
    """Load and manage application configuration from multiple sources"""
    
    def __init__(self, config_file: str = "config.json") -> None:
        """
        Initialize configuration loader.
        
        Args:
            config_file: Path to JSON configuration file
        """
        self.config_file = config_file
        self._config: ConfigDict = {}
        self.load_configuration()
    
    def load_configuration(self) -> None:
        """Load configuration from file and environment variables"""
        # Load default configuration
        self._config = self._get_default_config()
        
        # Try to load from file
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    file_config = json.load(f)
                self._merge_config(file_config)
                print(f"✅ Loaded configuration from {self.config_file}")
            else:
                print(f"📝 Using default configuration (no {self.config_file} found)")
        except Exception as e:
            print(f"⚠️ Failed to load {self.config_file}: {e}")
            print("📝 Using default configuration")
        
        # Override with environment variables
        self._apply_env_overrides()
    
    def _get_default_config(self) -> ConfigDict:
        """Get default configuration values"""
        return {
            "model": {
                "path": "C:\\OpenVinoModels\\phi3-128k-npu",
                "name": "Phi-3-mini-128k-instruct",
                "type": "phi3"
            },
            "deployment": {
                "target_device": "NPU",
                "npu_profile": "balanced",
                "fallback_device": "CPU",
                "cache_directory": "./cache/.ovcache_phi3"
            },
            "generation": {
                "max_new_tokens": 1024,
                "temperature": 0.6,
                "top_p": 0.95,
                "top_k": 20,
                "repetition_penalty": 1.1,
                "do_sample": True
            },
            "ui": {
                "max_message_length": 400,
                "max_conversation_tokens": 1800,
                "emergency_limit": 2048,
                "show_performance_metrics": True,
                "theme": "soft"
            },
            "performance": {
                "generation_timeout": 30.0,
                "truncation_warning_delay": 0.5,
                "ui_update_interval": 0.1
            }
        }
    
    def _merge_config(self, new_config: ConfigDict) -> None:
        """Merge new configuration with existing configuration"""
        def merge_dict(base: Dict[str, Any], update: Dict[str, Any]) -> None:
            for key, value in update.items():
                if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                    merge_dict(base[key], value)
                else:
                    base[key] = value
        
        merge_dict(self._config, new_config)
    
    def _apply_env_overrides(self) -> None:
        """Apply environment variable overrides"""
        env_mappings = {
            "MODEL_PATH": ("model", "path"),        # Primary model path variable
            "QWEN3_MODEL_PATH": ("model", "path"),  # Legacy compatibility (deprecated)
            "TARGET_DEVICE": ("deployment", "target_device"),
            "NPU_PROFILE": ("deployment", "npu_profile"),
            "CACHE_DIR": ("deployment", "cache_directory"),
            "MAX_MESSAGE_LENGTH": ("ui", "max_message_length"),
            "GENERATION_TIMEOUT": ("performance", "generation_timeout")
        }
        
        for env_var, (section, key) in env_mappings.items():
            value = os.getenv(env_var)
            if value is not None:
                # Type conversion based on key
                if key in ["max_message_length"]:
                    value = int(value)
                elif key in ["generation_timeout"]:
                    value = float(value)
                elif key in ["show_performance_metrics", "do_sample"]:
                    value = value.lower() in ('true', '1', 'yes', 'on')
                
                self._config[section][key] = value
                if env_var == "QWEN3_MODEL_PATH":
                    print(f"⚠️ Legacy environment variable: {env_var} = {value} (use MODEL_PATH instead)")
                else:
                    print(f"🔧 Environment override: {env_var} = {value}")
    
    def update_from_args(self, args) -> None:
        """
        Update configuration from command-line arguments.
        
        Args:
            args: Parsed arguments from argparse
        """
        if hasattr(args, 'model_path') and args.model_path:
            self._config['model']['path'] = args.model_path
            
        if hasattr(args, 'device') and args.device:
            self._config['deployment']['target_device'] = args.device
            
        if hasattr(args, 'npu_profile') and args.npu_profile:
            self._config['deployment']['npu_profile'] = args.npu_profile
            
        if hasattr(args, 'cache_dir') and args.cache_dir:
            self._config['deployment']['cache_directory'] = args.cache_dir
    
    def get(self, section: str, key: str, default: Any = None) -> Any:
        """Get configuration value"""
        return self._config.get(section, {}).get(key, default)
    
    def get_section(self, section: str) -> Dict[str, Any]:
        """Get entire configuration section"""
        return self._config.get(section, {})
    
    @property
    def config(self) -> ConfigDict:
        """Get full configuration"""
        return self._config.copy()


# Global configuration instance - will be initialized by main.py
config: ConfigurationLoader = None


def initialize_config(config_file: str = "config.json", args=None) -> ConfigurationLoader:
    """
    Initialize global configuration instance.
    
    Args:
        config_file: Path to configuration file
        args: Command-line arguments from argparse
        
    Returns:
        Initialized ConfigurationLoader instance
    """
    global config
    config = ConfigurationLoader(config_file)
    
    if args:
        config.update_from_args(args)
    
    return config


def get_config() -> ConfigurationLoader:
    """Get the global configuration instance"""
    if config is None:
        raise RuntimeError("Configuration not initialized. Call initialize_config() first.")
    return config
────────────────────────────────────────────────────────────
END OF FILE: config.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: model.py
PATH: app\model.py
PURPOSE: Pipeline deployment and system initialization
SIZE: 16,280 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

"""
Model Deployment and System Initialization
=========================================

Handles OpenVINO GenAI pipeline deployment with comprehensive error handling,
NPU optimization, and system validation.
"""

import os
import time
from typing import Any, Tuple, List
from typing_extensions import Literal

import openvino_genai as ov_genai
from transformers import AutoTokenizer

from .config import get_config

# Try to import OpenVINO properties with fallback
try:
    import openvino.properties as props
    import openvino.properties.hint as hints
    OPENVINO_PROPERTIES_AVAILABLE = True
    print("✅ OpenVINO properties imported successfully")
except ImportError as e:
    print(f"⚠️ OpenVINO properties not available: {e}")
    print("🔄 Using fallback configuration...")
    OPENVINO_PROPERTIES_AVAILABLE = False
    
    # Create mock objects for compatibility
    class MockHints:
        class PerformanceMode:
            LATENCY = "LATENCY"
            THROUGHPUT = "THROUGHPUT"
    
    class MockProps:
        class cache_dir:
            pass
        class streams:
            class num:
                pass
        class inference_num_threads:
            pass
    
    hints = MockHints()
    props = MockProps()

# Import Phi-3 NPU optimization patterns
try:
    from .npu_patterns import (
        get_npu_config_balanced,
        get_npu_config_conservative, 
        get_npu_config_aggressive,
        initialize_phi3_pipeline,
        PHI3_SPECIAL_TOKENS
    )
    ENHANCED_CONTEXT_AVAILABLE = True
    print("✅ Enhanced Phi-3 NPU context loaded successfully")
except ImportError as e:
    print(f"⚠️ Enhanced Phi-3 context not available: {e}")
    print("📝 Using fallback patterns - consider updating context path")
    ENHANCED_CONTEXT_AVAILABLE = False


# Type definitions
DeviceType = Literal["NPU", "CPU", "GPU", "AUTO"]
ProfileType = Literal["conservative", "balanced", "aggressive"]
ConfigDict = dict[str, Any]


class LLMConfigurationManager:
    """Advanced configuration management with Phi-3 optimization"""
    
    def __init__(self, profile: ProfileType = "balanced") -> None:
        """
        Initialize configuration manager with specified profile.
        
        Args:
            profile: NPU optimization profile (conservative, balanced, aggressive)
        """
        self.profile = profile
        self.enhanced_available = ENHANCED_CONTEXT_AVAILABLE
    
    def get_npu_config(self) -> ConfigDict:
        """
        Get complete NPU configuration with NPUW optimization.
        
        Returns:
            Dictionary containing NPU-specific configuration parameters
        """
        if ENHANCED_CONTEXT_AVAILABLE:
            # Use enhanced Phi-3-specific configuration
            if self.profile == "conservative":
                return get_npu_config_conservative()
            elif self.profile == "balanced":
                return get_npu_config_balanced()
            elif self.profile == "aggressive":
                return get_npu_config_aggressive()
            else:
                return get_npu_config_balanced()  # Default fallback
        else:
            # Fallback configuration optimized for Phi-3 128k context
            # Critical: Use correct NPUW hints to prevent compilation errors
            config = {
                "NPU_USE_NPUW": "YES",
                "NPUW_LLM": "YES", 
                "NPUW_LLM_BATCH_DIM": 0,
                "NPUW_LLM_SEQ_LEN_DIM": 1,
                "NPUW_LLM_MAX_PROMPT_LEN": 8192,  # Increased for Phi-3 128k context
                "NPUW_LLM_MIN_RESPONSE_LEN": 512,  # Increased for better responses
                "CACHE_MODE": "OPTIMIZE_SPEED",
                "NPUW_LLM_PREFILL_HINT": "FAST_COMPILE",  # Corrected: FAST_COMPILE for stable compilation
                "NPUW_LLM_GENERATE_HINT": "BEST_PERF"     # Corrected: BEST_PERF for optimal generation
            }
            
            # Add OpenVINO properties if available (no generic PERFORMANCE_HINT for NPU)
            if OPENVINO_PROPERTIES_AVAILABLE:
                cache_dir = get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3")
                config.update({
                    props.cache_dir: cache_dir
                })
            else:
                config.update({
                    "CACHE_DIR": get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3")
                })
            
            return config
    
    def get_cpu_config(self) -> ConfigDict:
        """
        Get optimized CPU configuration.
        
        Returns:
            Dictionary containing CPU-specific configuration parameters
        """
        if ENHANCED_CONTEXT_AVAILABLE:
            # Use Phi-3 optimized CPU configuration
            return {
                "MAX_PROMPT_LEN": 32768,  # Use Phi-3's 128k context capability
                "MIN_RESPONSE_LEN": 512,
                "CACHE_DIR": get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3") + "_cpu"
            }
        else:
            config = {
                "MAX_PROMPT_LEN": 16384,  # Much larger context for Phi-3 on CPU
                "MIN_RESPONSE_LEN": 512
            }
            
            # Add OpenVINO properties if available
            if OPENVINO_PROPERTIES_AVAILABLE:
                cache_dir = get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3") + "_cpu"
                config.update({
                    hints.performance_mode: hints.PerformanceMode.THROUGHPUT,
                    props.cache_dir: cache_dir,
                    props.streams.num: 2,
                    props.inference_num_threads: 0  # Auto-detect
                })
            else:
                config.update({
                    "PERFORMANCE_HINT": "THROUGHPUT",
                    "CACHE_DIR": get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3") + "_cpu",
                    "NUM_STREAMS": 2,
                    "INFERENCE_NUM_THREADS": 0  # Auto-detect
                })
            
            return config


def deploy_llm_pipeline(
    model_path: str, 
    target_device: DeviceType, 
    profile: ProfileType = "balanced"
) -> Tuple[Any, str, str, float]:
    """
    Deploy language model pipeline (Phi-3) with comprehensive error handling and optimization.
    
    Args:
        model_path: Path to the Phi-3 OpenVINO model directory
        target_device: Target device for deployment (NPU, CPU, GPU, AUTO)
        profile: NPU optimization profile
        
    Returns:
        Tuple of (pipeline, device_used, config_used, load_time)
        
    Raises:
        RuntimeError: If all deployment configurations fail
    """
    load_start_time = time.time()
    
    if ENHANCED_CONTEXT_AVAILABLE:
        print(f"🚀 Deploying Phi-3 with enhanced NPU context (profile: {profile})")
        
        # Use enhanced Phi-3 deployment with NPU patterns
        pipeline = initialize_phi3_pipeline(model_path, target_device, profile)
        
        if pipeline:
            load_time = time.time() - load_start_time
            return pipeline, target_device, f"enhanced_{profile}", load_time
        else:
            print("⚠️ Enhanced deployment failed, falling back to manual configuration")
    
    # Fallback to manual configuration
    print(f"🔄 Using manual pipeline deployment (target: {target_device})")
    
    config_manager = LLMConfigurationManager(profile)
    
    configurations = []
    
    # Create basic configurations with compatibility handling
    cache_dir = get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3")
    
    if OPENVINO_PROPERTIES_AVAILABLE:
        basic_npu_config = {hints.performance_mode: hints.PerformanceMode.LATENCY, props.cache_dir: cache_dir}
        basic_cpu_config = {hints.performance_mode: hints.PerformanceMode.THROUGHPUT, props.cache_dir: cache_dir}
    else:
        basic_npu_config = {"PERFORMANCE_HINT": "LATENCY", "CACHE_DIR": cache_dir}
        basic_cpu_config = {"PERFORMANCE_HINT": "THROUGHPUT", "CACHE_DIR": cache_dir}
    
    if target_device == "NPU":
        configurations = [
            ("enhanced_npu_phi3", target_device, config_manager.get_npu_config()),
            ("basic_npu", target_device, basic_npu_config),
            ("minimal_npu", target_device, {}),
            ("cpu_fallback", "CPU", config_manager.get_cpu_config())
        ]
    else:
        configurations = [
            ("optimized_cpu_phi3", target_device, config_manager.get_cpu_config()),
            ("basic_cpu", target_device, basic_cpu_config),
            ("minimal_cpu", target_device, {})
        ]
    
    for config_name, device, config in configurations:
        try:
            print(f"🔄 Trying {device} with {config_name} configuration...")
            
            if ENHANCED_CONTEXT_AVAILABLE:
                # Use enhanced Phi-3 initialization if available
                pipeline = initialize_phi3_pipeline(model_path, device, profile, **config)
            else:
                # Fallback initialization
                if config:
                    pipeline = ov_genai.LLMPipeline(model_path, device, **config)
                else:
                    pipeline = ov_genai.LLMPipeline(model_path, device)
                
            load_time = time.time() - load_start_time
            print(f"✅ Success: {device} with {config_name} ({load_time:.1f}s)")
            return pipeline, device, config_name, load_time
            
        except Exception as e:
            print(f"⚠️ {config_name} failed: {e}")
            continue
    
    raise RuntimeError("All configurations failed. Check model path, device drivers, and NPUW configuration.")


def validate_system_requirements() -> List[str]:
    """Validate system requirements and return list of issues."""
    issues = []
    
    config = get_config()
    model_path = config.get("model", "path")
    target_device = config.get("deployment", "target_device")
    cache_dir = config.get("deployment", "cache_directory")
    
    # Check model path
    if not os.path.exists(model_path):
        issues.append(f"Model path does not exist: {model_path}")
    elif not os.path.isdir(model_path):
        issues.append(f"Model path is not a directory: {model_path}")
    else:
        # Check for required OpenVINO files
        required_files = ['openvino_model.xml', 'openvino_model.bin']
        for file_name in required_files:
            if not os.path.exists(os.path.join(model_path, file_name)):
                issues.append(f"Missing OpenVINO model file: {file_name}")
    
    # Check cache directory
    cache_parent = os.path.dirname(cache_dir)
    if not os.path.exists(cache_parent):
        try:
            os.makedirs(cache_parent, exist_ok=True)
        except PermissionError:
            issues.append(f"Cannot create cache directory: {cache_parent} (permission denied)")
        except Exception as e:
            issues.append(f"Cannot create cache directory: {cache_parent} ({str(e)})")
    
    # Check OpenVINO installation
    try:
        import openvino as ov
        core = ov.Core()
        available_devices = core.available_devices
        if target_device not in available_devices and target_device != "AUTO":
            issues.append(f"Target device '{target_device}' not available. Available: {available_devices}")
    except Exception as e:
        issues.append(f"OpenVINO not properly installed: {str(e)}")
    
    return issues


def initialize_system_with_validation():
    """Initialize system with comprehensive validation and error handling."""
    config = get_config()
    
    print("🔍 Validating system requirements...")
    issues = validate_system_requirements()
    
    if issues:
        print("❌ System validation failed:")
        for i, issue in enumerate(issues, 1):
            print(f"   {i}. {issue}")
        print("\n🔧 Suggested fixes:")
        print("   • Set MODEL_PATH environment variable to correct model location")
        print("   • Install OpenVINO with: pip install openvino")
        print("   • For NPU: Install Intel NPU drivers from official site")
        print("   • Ensure model is in OpenVINO format (.xml/.bin files)")
        raise SystemExit(1)
    
    try:
        print("🚀 Initializing Enhanced Phi-3 Chat System...")
        
        # Get configuration values
        model_path = config.get("model", "path")
        target_device = config.get("deployment", "target_device", "NPU")
        npu_profile = config.get("deployment", "npu_profile", "balanced")
        
        print(f"📂 Model: {model_path}")
        print(f"🎯 Target Device: {target_device}")
        print(f"📊 Optimization Profile: {npu_profile}")
        print(f"🔧 Enhanced Context: {'Available' if ENHANCED_CONTEXT_AVAILABLE else 'Fallback Mode'}")
        
        # Deploy pipeline with comprehensive error handling
        pipeline, device_used, config_used, load_time = deploy_llm_pipeline(
            model_path, target_device, npu_profile
        )
        
        # Initialize tokenizer with error handling
        print("📚 Loading Phi-3 tokenizer...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            # Configure tokenizer for Phi-3
            if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
                tokenizer.pad_token_id = tokenizer.eos_token_id
                
        except Exception as tokenizer_error:
            print(f"⚠️ Tokenizer loading failed: {tokenizer_error}")
            print("🔄 Attempting fallback tokenizer initialization...")
            try:
                # Fallback: try without trust_remote_code
                tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=False)
                if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
                    tokenizer.pad_token_id = tokenizer.eos_token_id
                print("✅ Fallback tokenizer loaded successfully")
            except Exception as fallback_error:
                print(f"❌ Fallback tokenizer also failed: {fallback_error}")
                raise RuntimeError("Unable to initialize tokenizer with any method") from fallback_error
        
        print(f"✅ System Ready!")
        print(f"   Device: {device_used}")
        print(f"   Config: {config_used}")
        print(f"   Load Time: {load_time:.1f}s")
        print(f"   Model Path: {model_path}")
        print(f"   Tokenizer: {tokenizer.__class__.__name__}")
        if ENHANCED_CONTEXT_AVAILABLE:
            print(f"   Special Tokens: {len(PHI3_SPECIAL_TOKENS)} Phi-3 special tokens available")
        print("=" * 60)
        
        return pipeline, tokenizer, device_used, config_used, load_time
        
    except Exception as e:
        print(f"❌ System initialization failed: {e}")
        print("\n🔧 Detailed diagnostics:")
        print(f"   Model Path: {config.get('model', 'path')}")
        print(f"   Target Device: {config.get('deployment', 'target_device')}")
        print(f"   Cache Directory: {config.get('deployment', 'cache_directory')}")
        print(f"   Enhanced Context: {ENHANCED_CONTEXT_AVAILABLE}")
        
        # Provide specific guidance based on error type
        error_str = str(e).lower()
        if "compile" in error_str:
            print("\n💡 NPU Compilation Error - Try:")
            print("   • Verify NPU drivers are installed")
            print("   • Check NPUW configuration compatibility")
            print("   • Try CPU fallback with: --device CPU")
        elif "file" in error_str or "path" in error_str:
            print("\n💡 File/Path Error - Try:")
            print("   • Verify model path contains .xml and .bin files")
            print("   • Check file permissions and access rights")
        elif "memory" in error_str:
            print("\n💡 Memory Error - Try:")
            print("   • Use conservative NPU profile")
            print("   • Ensure sufficient system RAM")
            print("   • Close other applications")
        
        raise SystemExit(1)
────────────────────────────────────────────────────────────
END OF FILE: model.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: chat.py
PATH: app\chat.py
PURPOSE: Core chat processing with RAG integration
SIZE: 31,498 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

"""
Chat Processing Module
=====================

Core chat processing logic with RAG integration, input validation,
security features, and comprehensive error handling.
"""

import time
import threading
import re
from typing import Iterator, List, Dict, Tuple, Any
from typing_extensions import TypedDict

import openvino_genai as ov_genai

from .config import get_config
from .streamer import EnhancedLLMStreamer, streaming_metrics

# Agent system import (Phase 3.3)
try:
    from .agent import get_agent, AGENT_AVAILABLE
    print("✅ Agent system loaded")
except ImportError as e:
    print(f"⚠️ Agent system not available: {e}")
    AGENT_AVAILABLE = False
    get_agent = lambda: None

# Type definitions for Gradio ChatInterface compatibility
# ChatHistory is a list of message dictionaries with role and content
ChatHistory = List[Dict[str, str]]

# RAG system imports with fallback
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    try:
        # Try modern import first
        from langchain_huggingface import HuggingFaceEmbeddings
    except ImportError:
        # Fallback to legacy import
        from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    
    # Advanced document parsing (Phase 3.1)
    try:
        from langchain_unstructured import UnstructuredLoader
        from unstructured.partition.auto import partition
        ADVANCED_PARSING_AVAILABLE = True
        print("✅ Advanced document parsing (unstructured) loaded successfully")
    except ImportError:
        ADVANCED_PARSING_AVAILABLE = False
        print("📝 Advanced parsing not available - install with: pip install unstructured[local-inference] langchain-unstructured")
    
    # Cross-encoder reranking (Phase 3.2)
    try:
        from sentence_transformers import CrossEncoder
        RERANKING_AVAILABLE = True
        print("✅ Cross-encoder reranking loaded successfully")
    except ImportError:
        RERANKING_AVAILABLE = False
        print("📝 Reranking not available - already have sentence-transformers but need torch")
    
    RAG_AVAILABLE = True
    print("✅ RAG dependencies loaded successfully")
except ImportError as e:
    print(f"⚠️ RAG dependencies not available: {e}")
    print("📝 Install with: pip install langchain faiss-cpu sentence-transformers")
    RAG_AVAILABLE = False
    ADVANCED_PARSING_AVAILABLE = False
    RERANKING_AVAILABLE = False


class DocumentRAGSystem:
    """Retrieval-Augmented Generation system for document processing"""
    
    def __init__(self):
        """Initialize RAG system with fallback handling"""
        self.vector_store = None
        self.embeddings = None
        self.text_splitter = None
        self.processed_docs_count = 0
        self.available = RAG_AVAILABLE
        
        # Advanced features
        self.advanced_parsing = ADVANCED_PARSING_AVAILABLE
        self.reranking = RERANKING_AVAILABLE
        self.cross_encoder = None
        
        if RAG_AVAILABLE:
            try:
                # Initialize embeddings model (lightweight for fast loading)
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2",
                    model_kwargs={'device': 'cpu'}
                )
                
                # Initialize text splitter with optimized settings
                self.text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800,  # Smaller chunks for better retrieval
                    chunk_overlap=100,  # Overlap for context preservation
                    separators=["\n\n", "\n", ". ", " ", ""]
                )
                
                # Initialize cross-encoder for reranking (Phase 3.2)
                if RERANKING_AVAILABLE:
                    try:
                        self.cross_encoder = CrossEncoder('BAAI/bge-reranker-base')
                        print("✅ Cross-encoder reranker initialized")
                    except Exception as e:
                        print(f"⚠️ Cross-encoder initialization failed: {e}")
                        self.reranking = False
                
                print("✅ RAG system initialized successfully")
                print(f"📊 Features: Advanced parsing={self.advanced_parsing}, Reranking={self.reranking}")
                
            except Exception as e:
                print(f"⚠️ RAG initialization failed: {e}")
                self.available = False
        else:
            print("📝 RAG system not available - install dependencies to enable")
    
    def process_uploaded_file(self, file_path: str, file_name: str) -> str:
        """
        Process uploaded file for RAG retrieval.
        
        Args:
            file_path: Path to the uploaded file
            file_name: Original name of the file
            
        Returns:
            Status message about processing result
        """
        if not self.available:
            return "❌ RAG system not available. Install langchain and faiss-cpu to enable document processing."
        
        try:
            # Use advanced parsing if available (Phase 3.1)
            if self.advanced_parsing and file_name.lower().endswith(('.pdf', '.docx', '.pptx', '.html')):
                try:
                    # Use unstructured for advanced document parsing
                    elements = partition(filename=file_path, strategy="hi_res")
                    text = "\n\n".join([str(element) for element in elements])
                    parsing_method = "Advanced (unstructured)"
                    print(f"📚 Using advanced parsing for {file_name}")
                except Exception as e:
                    print(f"⚠️ Advanced parsing failed for {file_name}, falling back to basic: {e}")
                    # Fallback to basic parsing
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            text = f.read()
                    except UnicodeDecodeError:
                        with open(file_path, 'r', encoding='latin-1') as f:
                            text = f.read()
                    parsing_method = "Basic (fallback)"
            else:
                # Basic text parsing for supported formats
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                except UnicodeDecodeError:
                    # Try with different encoding
                    with open(file_path, 'r', encoding='latin-1') as f:
                        text = f.read()
                parsing_method = "Basic (text)"
            
            if not text.strip():
                return f"⚠️ File '{file_name}' appears to be empty."
            
            # Split text into chunks
            chunks = self.text_splitter.split_text(text)
            
            if not chunks:
                return f"⚠️ No processable content found in '{file_name}'."
            
            # Create or update vector store
            if self.vector_store is None:
                self.vector_store = FAISS.from_texts(
                    texts=chunks, 
                    embedding=self.embeddings,
                    metadatas=[{"source": file_name, "chunk": i} for i in range(len(chunks))]
                )
            else:
                # Add new documents to existing store
                new_store = FAISS.from_texts(
                    texts=chunks, 
                    embedding=self.embeddings,
                    metadatas=[{"source": file_name, "chunk": i} for i in range(len(chunks))]
                )
                self.vector_store.merge_from(new_store)
            
            self.processed_docs_count += 1
            
            return f"✅ Successfully processed '{file_name}' using {parsing_method}: {len(chunks)} chunks created from {len(text):,} characters. Ready to answer questions about this document."
            
        except Exception as e:
            return f"❌ Error processing '{file_name}': {str(e)}"
    
    def retrieve_context(self, query: str, k: int = 3) -> str:
        """
        Retrieve relevant context for a query with optional cross-encoder reranking.
        
        Args:
            query: User question to find relevant context for
            k: Number of top chunks to retrieve
            
        Returns:
            Concatenated context from relevant document chunks
        """
        if not self.available or self.vector_store is None:
            return ""
        
        try:
            # Use two-stage retrieval with reranking if available (Phase 3.2)
            if self.reranking and self.cross_encoder is not None:
                # Stage 1: Retrieve larger candidate set (e.g., top 20)
                candidate_docs = self.vector_store.similarity_search(query, k=min(20, k*6))
                
                if not candidate_docs:
                    return ""
                
                # Stage 2: Rerank with cross-encoder
                try:
                    query_doc_pairs = [(query, doc.page_content) for doc in candidate_docs]
                    scores = self.cross_encoder.predict(query_doc_pairs)
                    
                    # Sort by reranking scores and take top k
                    scored_docs = list(zip(candidate_docs, scores))
                    scored_docs.sort(key=lambda x: x[1], reverse=True)
                    docs = [doc for doc, score in scored_docs[:k]]
                    
                    print(f"🔄 Reranked {len(candidate_docs)} candidates → {k} results")
                    
                except Exception as e:
                    print(f"⚠️ Reranking failed, using vector search results: {e}")
                    docs = candidate_docs[:k]
            else:
                # Standard single-stage retrieval
                docs = self.vector_store.similarity_search(query, k=k)
            
            if not docs:
                return ""
            
            # Format context with source attribution
            context_parts = []
            for doc in docs:
                source = doc.metadata.get("source", "Unknown")
                content = doc.page_content.strip()
                context_parts.append(f"[From {source}]\n{content}")
            
            return "\n\n---\n\n".join(context_parts)
            
        except Exception as e:
            print(f"⚠️ Context retrieval error: {e}")
            return ""
    
    def clear_documents(self) -> str:
        """Clear all processed documents"""
        self.vector_store = None
        self.processed_docs_count = 0
        return "✅ All documents cleared from memory."
    
    def get_status(self) -> dict:
        """Get current RAG system status with advanced features"""
        return {
            "Available": self.available,
            "Documents Processed": self.processed_docs_count,
            "Vector Store": "Loaded" if self.vector_store is not None else "Empty",
            "Embedding Model": "all-MiniLM-L6-v2" if self.available else "None",
            "Advanced Parsing": "✅ unstructured" if self.advanced_parsing else "❌ basic text only",
            "Cross-Encoder Reranking": "✅ BAAI/bge-reranker-base" if self.reranking else "❌ vector search only",
            "Supported Formats": "PDF, DOCX, PPTX, HTML, TXT, MD, PY, JS, CSS, JSON" if self.advanced_parsing else "TXT, MD, PY, JS, CSS, JSON"
        }


class InputValidator:
    """Security-focused input validation and sanitization"""
    
    @staticmethod
    def validate_message(message: str) -> Tuple[bool, str]:
        """
        Validate user message for security and content policy compliance.
        
        Args:
            message: User input to validate
            
        Returns:
            Tuple of (is_valid, reason_if_invalid)
        """
        if not message or not isinstance(message, str):
            return False, "Empty or invalid message"
        
        # Check for excessively long messages (security)
        if len(message) > 10000:  # Much higher than UI limit
            return False, "Message exceeds maximum length"
        
        # Check for potential injection patterns
        suspicious_patterns = [
            r'<script[^>]*>',  # Script injection
            r'javascript:',     # JavaScript URLs
            r'data:.*base64',   # Data URLs
            r'eval\s*\(',      # Eval calls
            r'exec\s*\(',      # Exec calls
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return False, "Message contains potentially unsafe content"
        
        # Check for excessive special characters (potential encoding attacks)
        special_char_ratio = len([c for c in message if not c.isalnum() and not c.isspace()]) / len(message)
        if special_char_ratio > 0.5:  # More than 50% special characters
            return False, "Message contains excessive special characters"
        
        return True, ""
    
    @staticmethod
    def sanitize_message(message: str) -> str:
        """
        Sanitize user message while preserving readability.
        
        Args:
            message: Raw user input
            
        Returns:
            Sanitized message safe for processing
        """
        # Remove null bytes and control characters (except newlines and tabs)
        sanitized = ''.join(char for char in message if ord(char) >= 32 or char in '\n\t')
        
        # Normalize whitespace
        sanitized = ' '.join(sanitized.split())
        
        # Limit consecutive repeated characters (potential DoS protection)
        sanitized = re.sub(r'(.)\1{10,}', r'\1\1\1', sanitized)
        
        return sanitized.strip()


# Global RAG system instance
rag_system = DocumentRAGSystem()

# Global instances (will be set by main.py)
pipe = None
tokenizer = None


def normalize_history(history) -> ChatHistory:
    """Ensure history is in the correct Gradio messages format."""
    normalized = []
    for entry in history or []:
        if isinstance(entry, dict) and "role" in entry and "content" in entry:
            normalized.append({"role": str(entry["role"]), "content": str(entry["content"])})
        elif isinstance(entry, (list, tuple)) and len(entry) == 2:
            normalized.append({"role": "user", "content": str(entry[0])})
            normalized.append({"role": "assistant", "content": str(entry[1])})
    return normalized


def create_phi3_generation_config(generation_params: Dict[str, Any] = None) -> ov_genai.GenerationConfig:
    """
    Create optimized generation configuration for Phi-3 with dynamic parameters.
    
    Args:
        generation_params: Optional dict with temperature, top_p, max_new_tokens
    
    Returns:
        Configured GenerationConfig with security-conscious defaults
    """
    gen_config = ov_genai.GenerationConfig()
    config = get_config()
    
    # Load generation settings from configuration (defaults)
    gen_settings = config.get_section("generation")
    
    # Use dynamic parameters if provided, otherwise fall back to config defaults
    if generation_params:
        temperature = generation_params.get('temperature', gen_settings.get("temperature", 0.6))
        top_p = generation_params.get('top_p', gen_settings.get("top_p", 0.95))
        max_new_tokens = generation_params.get('max_new_tokens', gen_settings.get("max_new_tokens", 1024))
    else:
        temperature = gen_settings.get("temperature", 0.6)
        top_p = gen_settings.get("top_p", 0.95)
        max_new_tokens = gen_settings.get("max_new_tokens", 1024)
    
    gen_config.do_sample = gen_settings.get("do_sample", True)
    gen_config.temperature = min(float(temperature), 2.0)  # Security: cap temperature
    gen_config.top_p = min(float(top_p), 1.0)  # Security: cap top_p
    gen_config.top_k = min(gen_settings.get("top_k", 20), 100)  # Security: reasonable top_k
    gen_config.max_new_tokens = min(int(max_new_tokens), 2048)  # Security: limit tokens
    gen_config.repetition_penalty = max(1.0, min(gen_settings.get("repetition_penalty", 1.1), 2.0))  # Security: reasonable range
    
    return gen_config


def process_user_message(message: str, history: ChatHistory) -> Tuple[str, bool]:
    """
    Process user message with smart truncation handling.
    
    Args:
        message: Raw user input message
        history: Current chat conversation history
        
    Returns:
        Tuple of (processed_message, was_truncated)
    """
    config = get_config()
    max_message_length = config.get("ui", "max_message_length", 400)
    original_length = len(message)
    
    # Handle overly long messages
    if original_length > max_message_length:
        # Smart truncation
        if '.' in message:
            sentences = message.split('.')
            truncated = []
            current_length = 0
            
            for sentence in sentences:
                if current_length + len(sentence) + 1 <= max_message_length * 0.85:
                    truncated.append(sentence)
                    current_length += len(sentence) + 1
                else:
                    break
            
            if truncated:
                processed = '. '.join(truncated) + '.'
                if len(processed) < original_length * 0.5:
                    processed = message[:max_message_length-50] + "..."
            else:
                processed = message[:max_message_length-50] + "..."
        else:
            processed = message[:max_message_length-50] + "..."
        
        print(f"📏 Message truncated: {original_length} → {len(processed)} chars")
        return processed, True
    
    return message, False


def prepare_chat_input(message: str, history: ChatHistory) -> Tuple[str, bool, ChatHistory]:
    """
    Prepare and validate chat input with smart message handling and security validation.
    This version includes the definitive fix for the Gradio data format error.
    """
    # Input validation
    if not message.strip():
        # Pass the original history back if the message is empty
        return message, False, history
    
    # Security validation
    is_valid, reason = InputValidator.validate_message(message)
    if not is_valid:
        error_history = [
            {"role": str(item.get("role")), "content": str(item.get("content"))} for item in history
        ]
        error_history.extend([
            {"role": "user", "content": message},
            {"role": "assistant", "content": f"🚫 Message rejected: {reason}. Please try a different message."}
        ])
        # Note: We are raising an error, but if this were to yield, error_history is now safe
        raise ValueError(f"Security validation failed: {reason}")
    
    # Sanitize and process the user message
    sanitized_message = InputValidator.sanitize_message(message)
    processed_message, was_truncated = process_user_message(sanitized_message, history)
    
    # --- START OF THE DEFINITIVE FIX ---
    # Rebuild the history from scratch on every turn to guarantee a clean state.
    # This prevents any possibility of format corruption from previous turns.
    updated_history = [
        {"role": str(item.get("role")), "content": str(item.get("content"))}
        for item in history if isinstance(item, dict) and "role" in item and "content" in item
    ]
    # --- END OF THE DEFINITIVE FIX ---
    
    if was_truncated:
        # Add truncation warning as a separate exchange
        updated_history.extend([
            {"role": "user", "content": message},
            {"role": "assistant", "content": f"⚠️ Your message was truncated from {len(message):,} to {len(processed_message)} characters due to NPU memory limits. Processing the truncated version..."}
        ])
    
    # Add current user message with empty bot response placeholder
    updated_history.extend([
        {"role": "user", "content": processed_message},
        {"role": "assistant", "content": ""}
    ])
    
    return processed_message, was_truncated, updated_history


def execute_generation(processed_message: str, streamer: EnhancedLLMStreamer, generation_params: Dict[str, Any] = None) -> bool:
    """
    Execute model generation in a controlled manner.
    
    Args:
        processed_message: Message to generate response for
        streamer: Configured streamer for token processing
        generation_params: Optional dict with temperature, top_p, max_new_tokens
        
    Returns:
        True if generation succeeded, False otherwise
    """
    try:
        generation_config = create_phi3_generation_config(generation_params)
        pipe.generate(processed_message, generation_config, streamer)
        return True
    except Exception as e:
        print(f"❌ Generation error: {e}")
        # Send error through streamer
        error_msg = f"❌ Generation error: {str(e)[:100]}..."
        streamer.text_queue.put(error_msg)
        streamer.text_queue.put(None)
        return False


def stream_response_to_history(streamer: EnhancedLLMStreamer, history: ChatHistory) -> Iterator[ChatHistory]:
    """
    Stream model response tokens to chat history with bulletproof format validation.
    Creates a clean copy on every yield to ensure Gradio compatibility.
    """
    def create_safe_history_copy(hist):
        """Create a guaranteed-safe copy of history for Gradio"""
        safe_history = []
        for entry in hist or []:
            if isinstance(entry, dict) and "role" in entry and "content" in entry:
                safe_history.append({
                    "role": str(entry["role"]),
                    "content": str(entry["content"])
                })
            elif isinstance(entry, (list, tuple)) and len(entry) >= 2:
                safe_history.extend([
                    {"role": "user", "content": str(entry[0])},
                    {"role": "assistant", "content": str(entry[1])}
                ])
        return safe_history
    
    try:
        # Always work with a safe copy
        working_history = create_safe_history_copy(history)
        
        # Ensure we have an empty assistant response to fill
        if not working_history or working_history[-1].get("role") != "assistant":
            working_history.append({"role": "assistant", "content": ""})
        
        # Stream chunks and build response
        for chunk in streamer:
            if chunk and isinstance(chunk, str):
                # Update the assistant's response
                working_history[-1]["content"] += chunk
                
                # Create a fresh, validated copy for yielding
                yield_history = create_safe_history_copy(working_history)
                print(f"🔄 Yielding {len(yield_history)} messages")
                yield yield_history

    except Exception as e:
        print(f"❌ Streaming error: {e}")
        # Create error response with safe format
        error_history = create_safe_history_copy(history or [])
        if not error_history or error_history[-1].get("role") != "assistant":
            error_history.append({"role": "assistant", "content": ""})
        
        error_history[-1]["content"] = f"❌ Streaming error: {str(e)[:100]}..."
        yield error_history


def handle_chat_error(error: Exception, history: ChatHistory) -> ChatHistory:
    """
    Handle chat errors with user-friendly messages.
    
    Args:
        error: Exception that occurred
        history: Current chat history
        
    Returns:
        Updated history with error message
    """
    print(f"❌ Chat function error: {error}")
    
    # Ensure history is properly formatted
    if not isinstance(history, list):
        history = []
    
    # Determine error type and provide helpful message
    error_message = "❌ An error occurred. "
    error_str = str(error).lower()
    
    if "memory" in error_str:
        error_message += "Memory limit reached. Try starting a new conversation."
    elif "token" in error_str or "length" in error_str:
        error_message += "Message too long. Please try a shorter message."
    elif "compile" in error_str:
        error_message += "NPU compilation issue. Check NPUW configuration."
    elif "timeout" in error_str:
        error_message += "Generation timed out. Try a simpler request."
    elif "device" in error_str:
        error_message += "Device error. NPU may not be available."
    else:
        error_message += f"Details: {str(error)[:100]}..."
    
    # Normalize history to ensure proper format
    updated_history = normalize_history(history)
    
    # Add error to history in the correct format
    if (not updated_history or 
        not isinstance(updated_history[-1], dict) or 
        updated_history[-1].get("role") != "assistant" or 
        updated_history[-1].get("content")):
        # If no history or last message has content, add new exchange
        updated_history.append({"role": "assistant", "content": error_message})
    else:
        # Update the empty bot response
        updated_history[-1]["content"] = error_message
    
    return updated_history


def should_use_agent(message: str) -> bool:
    """
    Determine if the user message should be processed by the agent system.
    
    Args:
        message: User input message
        
    Returns:
        True if agent should be used, False for regular chat
    """
    # Keywords that suggest tool usage
    agent_keywords = [
        'calculate', 'compute', 'math', 'equation', 'solve',
        'what time', 'what date', 'today', 'tomorrow', 'yesterday', 
        'search', 'look up', 'find information',
        'analyze text', 'word count', 'character count',
        'tool', 'function', 'help me with'
    ]
    
    message_lower = message.lower()
    return any(keyword in message_lower for keyword in agent_keywords)


def enhanced_llm_chat(message: str, history: ChatHistory, generation_params: Dict[str, Any] = None) -> Iterator[ChatHistory]:
    """
    Enhanced chat function with comprehensive Phi-3 optimization and RAG support.
    This version includes the fix for the Gradio streaming data format error.
    """
    request_start_time = time.time()
    streaming_metrics.start_request()

    # Step 1: Normalize the incoming history to the required List[Dict] format
    # This correctly handles the List[List] format from the Gradio component
    if history is None:
        history = []
    normalized_history = normalize_history(history)

    try:
        # Step 2: Prepare and validate the user's input
        processed_message, was_truncated, updated_history = prepare_chat_input(message, normalized_history)
        
        # Early return for empty messages
        if not processed_message.strip():
            import json
            print("RETURNING (empty message):\n", json.dumps(updated_history, indent=2, default=str))
            yield updated_history
            return

        # If the message was truncated, yield the warning and pause
        if was_truncated:
            import json
            print("RETURNING (truncated):\n", json.dumps(updated_history, indent=2, default=str))
            yield updated_history
            time.sleep(0.5)

        # Step 3: Decide whether to use the ReAct agent or regular chat
        agent = get_agent()
        use_agent = AGENT_AVAILABLE and agent and should_use_agent(processed_message)

        if use_agent:
            print(f"🤖 Using agent processing for: {processed_message[:50]}...")
            try:
                # Agent processing is not streamed, so we yield the final result once
                agent_response = agent.process_with_tools(processed_message, generation_params)
                updated_history[-1]["content"] = agent_response
                import json
                print("RETURNING (agent):\n", json.dumps(updated_history, indent=2, default=str))
                yield updated_history
                return
            except Exception as e:
                print(f"⚠️ Agent processing failed, falling back to regular chat: {e}")

        # Step 4: RAG Context Retrieval for regular chat
        rag_context = rag_system.retrieve_context(processed_message)
        if rag_context:
            augmented_message = f"""Based on the following context from uploaded documents, please answer the user's question. If the context doesn't contain relevant information, please indicate that and provide a general response.

Context:
{rag_context}

Question: {processed_message}"""
            processed_message = augmented_message
            print(f"📚 Using RAG context: {len(rag_context)} characters from documents")
        
        # Step 5: Set up the streamer and generation thread
        def metrics_callback(metric_name: str, value):
            streaming_metrics.update_metric(metric_name, value)
        
        streamer = EnhancedLLMStreamer(tokenizer, metrics_callback)

        def generation_worker():
            success = execute_generation(processed_message, streamer, generation_params)
            elapsed_time = time.time() - request_start_time
            streaming_metrics.end_request(success, elapsed_time)
        
        generation_thread = threading.Thread(target=generation_worker, daemon=True)
        generation_thread.start()

        # Step 6: *** CORRECTED STREAMING LOGIC ***
        # Yield the initial state with the user message and empty bot response
        import json
        print("RETURNING (initial):\n", json.dumps(updated_history, indent=2, default=str))
        yield updated_history

        # Stream the response tokens, appending to the last message
        for chunk in streamer:
            if chunk and isinstance(chunk, str):
                updated_history[-1]["content"] += chunk
                print("RETURNING (streaming):\n", json.dumps(updated_history, indent=2, default=str))
                yield updated_history # Yield the mutated, complete history object each time

        # Wait for the generation thread to finish
        generation_thread.join(timeout=30.0)
        if generation_thread.is_alive():
            print("⚠️ Generation timeout - thread still running")
        
        print(f"📊 Request complete: {time.time() - request_start_time:.2f}s total")

    except Exception as e:
        elapsed_time = time.time() - request_start_time
        streaming_metrics.end_request(False, elapsed_time)
        # Use the already normalized history for error reporting
        error_history = handle_chat_error(e, normalized_history)
        import json
        print("RETURNING (error):\n", json.dumps(error_history, indent=2, default=str))
        yield error_history


def initialize_globals(pipeline, tokenizer_instance):
    """Initialize global pipeline and tokenizer instances"""
    global pipe, tokenizer
    pipe = pipeline
    tokenizer = tokenizer_instance
    
    # Initialize agent system if available
    if AGENT_AVAILABLE:
        try:
            from .agent import initialize_agent
            initialize_agent(pipeline, tokenizer_instance)
            print("✅ Agent system initialized with LLM pipeline")
        except Exception as e:
            print(f"⚠️ Agent initialization failed: {e}")
────────────────────────────────────────────────────────────
END OF FILE: chat.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: ui.py
PATH: app\ui.py
PURPOSE: Gradio interface creation and event handling
SIZE: 28,974 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

"""
User Interface Module  
====================

Gradio interface creation with advanced features including dynamic system prompts,
RAG document upload, performance monitoring, and professional styling.
"""

import os
import gradio as gr
from typing import Dict, Any

from .config import get_config
from .chat import enhanced_llm_chat, rag_system, RAG_AVAILABLE

# Agent system import
try:
    from .agent import AGENT_AVAILABLE, get_agent
    print("✅ Agent UI components loaded")
except ImportError:
    AGENT_AVAILABLE = False
    get_agent = lambda: None
    print("⚠️ Agent UI components not available")
from .streamer import streaming_metrics

# Import enhanced context patterns
import sys
context_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "context")
sys.path.insert(0, context_path)

# Import Phi-3-specific optimizations
try:
    from .npu_patterns import (
        PHI3_PERFORMANCE_PROFILES,
        PHI3_SPECIAL_TOKENS
    )
    ENHANCED_CONTEXT_AVAILABLE = True
except ImportError:
    ENHANCED_CONTEXT_AVAILABLE = False
    PHI3_PERFORMANCE_PROFILES = {}
    PHI3_SPECIAL_TOKENS = {}


# System prompt management
DEFAULT_SYSTEM_PROMPT = """You are a helpful, concise AI assistant powered by Phi-3-mini-128k-instruct running on Intel NPU via OpenVINO GenAI. 

Key behaviors:
- Provide accurate, well-structured responses
- Be concise but comprehensive 
- Use clear formatting when helpful
- Acknowledge when you're uncertain
- Optimize for NPU constraints (prefer shorter, focused responses)

You excel at: reasoning, coding, analysis, creative writing, and technical explanations."""

# Current system prompt (can be modified by user)
SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT

# Global references (will be set by main.py)
pipe = None
device_used = "NPU"
config_used = "enhanced"
load_time = 0.0


def create_enhanced_interface():
    """Create production-ready Gradio interface with advanced features"""
    
    config = get_config()
    
    # Custom CSS for professional appearance
    custom_css = """
    .gradio-container { max-width: 1400px; margin: auto; }
    .chatbot { height: 650px; }
    .metrics-panel { 
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        padding: 15px; 
        border-radius: 8px; 
        border: 1px solid #dee2e6;
    }
    .system-info {
        background: #f8f9fa;
        padding: 10px;
        border-radius: 6px;
        border-left: 4px solid #28a745;
        margin: 10px 0;
    }
    .warning-banner {
        background: #fff3cd;
        padding: 8px;
        border-radius: 4px;
        border-left: 4px solid #ffc107;
        margin: 5px 0;
    }
    """
    
    with gr.Blocks(
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="gray",
            neutral_hue="slate"
        ),
        title="Enhanced Phi-3 Chat",
        css=custom_css,
    ) as demo:
        
        # Header with system status
        with gr.Row():
            with gr.Column(scale=3):
                gr.Markdown(f"""
                # 🤖 Enhanced Phi-3 Chat System
                
                **Production-Ready Implementation with Complete Optimization**
                """)
            
            with gr.Column(scale=2, elem_classes=["system-info"]):
                system_status = gr.Markdown(f"""
                **Device**: {device_used} | **Config**: {config_used}  
                **Model**: Phi-3-mini-128k-instruct INT4 | **Load Time**: {load_time:.1f}s  
                **Enhanced Context**: {'✅ Active' if ENHANCED_CONTEXT_AVAILABLE else '⚠️ Fallback'}  
                **Profile**: {config.get('deployment', 'npu_profile', 'balanced').title()}
                """)
        
        # Warning banner if fallback mode
        if not ENHANCED_CONTEXT_AVAILABLE:
            gr.Markdown("""
            <div class="warning-banner">
            ⚠️ <strong>Fallback Mode</strong>: Enhanced context not loaded. Some optimizations may be limited.
            </div>
            """)
        
        # Main chat interface using official ChatInterface pattern
        chatbot = gr.Chatbot(
            label=f"Conversation (Phi-3-mini-128k on {device_used})",
            height=650,
            type='messages',
            avatar_images=(None, "🤖"),
            show_copy_button=True,
            show_share_button=False,
            bubble_full_width=False,
            render_markdown=True
        )
        
        # System prompt control
        with gr.Accordion("🎯 System Prompt Configuration", open=False):
            system_prompt_input = gr.Textbox(
                value=SYSTEM_PROMPT,
                lines=6,
                label="System Prompt",
                placeholder="Configure the AI's behavior and persona...",
                interactive=True,
                info="This prompt sets the AI's behavior, expertise, and response style. Changes take effect after clearing the chat."
            )
            
            with gr.Row():
                reset_prompt_btn = gr.Button("🔄 Reset to Default", size="sm")
                apply_prompt_btn = gr.Button("✅ Apply & Clear Chat", variant="primary", size="sm")
        
        # Document upload for RAG
        with gr.Accordion("📚 Document Upload (RAG)", open=False):
            with gr.Row():
                with gr.Column(scale=3):
                    # Dynamic file types based on parsing capabilities
                    if rag_system.advanced_parsing:
                        supported_types = [".txt", ".md", ".py", ".js", ".html", ".css", ".json", ".pdf", ".docx", ".pptx"]
                        upload_label = "Upload Documents (Advanced Parsing Enabled)"
                    else:
                        supported_types = [".txt", ".md", ".py", ".js", ".html", ".css", ".json"]
                        upload_label = "Upload Documents (Basic Text Processing)"
                    
                    file_upload = gr.File(
                        label=upload_label,
                        file_types=supported_types,
                        file_count="multiple",
                        interactive=True
                    )
                    
                with gr.Column(scale=2):
                    upload_status = gr.Textbox(
                        label="Upload Status",
                        interactive=False,
                        placeholder="No documents uploaded"
                    )
            
            with gr.Row():
                clear_docs_btn = gr.Button("🗑️ Clear Documents", variant="secondary", size="sm")
                rag_status_btn = gr.Button("📊 RAG Status", size="sm")
            
            # Dynamic check for RAG availability 
            try:
                try:
                    from langchain_huggingface import HuggingFaceEmbeddings
                except ImportError:
                    from langchain_community.embeddings import HuggingFaceEmbeddings
                rag_available_now = True
            except ImportError:
                rag_available_now = False
            
            if not rag_available_now:
                gr.Markdown("""
                ⚠️ **RAG not available**: Install dependencies with:
                ```bash
                # Basic RAG functionality
                pip install langchain faiss-cpu sentence-transformers
                
                # Advanced features (Phase 3)
                pip install unstructured[local-inference] langchain-unstructured torch
                ```
                """)

        # Agent Tools Section (Phase 3.3)
        with gr.Accordion("🤖 AI Agent Tools", open=False):
            if AGENT_AVAILABLE:
                agent_tools_display = gr.Markdown("""
                **Available Agent Tools:**
                
                🧮 **Calculator**: Perform mathematical calculations  
                📅 **Date/Time**: Get current time, dates, and calculate date differences  
                🔍 **Web Search**: Search for information (mock implementation)  
                📊 **Text Analysis**: Analyze text for word count, readability, etc.
                
                **Usage**: Simply ask naturally! The agent will automatically use tools when needed.
                
                **Examples:**
                - "What's 25 * 4 + 10?"
                - "What time is it?"
                - "Analyze this text: [your text]"
                - "Search for information about AI"
                """)
                
                with gr.Row():
                    agent_status_btn = gr.Button("🤖 Agent Status", variant="secondary", size="sm")
                    agent_tools_btn = gr.Button("🛠️ List Tools", variant="secondary", size="sm")
                
                agent_status_output = gr.Textbox(
                    label="Agent System Status",
                    interactive=False,
                    visible=False,
                    max_lines=5
                )
            else:
                gr.Markdown("""
                ⚠️ **Agent system not available**: Install dependencies with:
                ```bash
                pip install langchain-core langchain-experimental requests python-dateutil
                ```
                
                Once installed, the AI will be able to use tools like calculators, web search, 
                and text analysis automatically based on your requests.
                """)

        # Advanced Generation Settings
        with gr.Accordion("⚙️ Advanced Generation Settings", open=False):
            with gr.Row():
                with gr.Column(scale=1):
                    temperature_slider = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        step=0.1,
                        value=config.get('generation', 'temperature', 0.6),
                        label="Temperature",
                        info="Controls randomness (0.1=focused, 2.0=creative)"
                    )
                    
                with gr.Column(scale=1):
                    top_p_slider = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        step=0.05,
                        value=config.get('generation', 'top_p', 0.95),
                        label="Top-p (Nucleus Sampling)",
                        info="Cumulative probability threshold"
                    )
                
                with gr.Column(scale=1):
                    max_tokens_number = gr.Number(
                        minimum=50,
                        maximum=2048,
                        step=50,
                        value=config.get('generation', 'max_new_tokens', 512),
                        label="Max New Tokens",
                        info="Maximum tokens to generate"
                    )
            
            # Generation settings controls
            with gr.Row():
                reset_gen_settings_btn = gr.Button("🔄 Reset to Defaults", variant="secondary", size="sm")
                apply_gen_settings_btn = gr.Button("✅ Apply Settings", variant="primary", size="sm")
            
            # Current generation settings display
            gen_settings_status = gr.Textbox(
                value=f"Temperature: {config.get('generation', 'temperature', 0.6)}, Top-p: {config.get('generation', 'top_p', 0.95)}, Max tokens: {config.get('generation', 'max_new_tokens', 512)}",
                label="Current Settings",
                interactive=False,
                max_lines=1
            )

        # Input controls
        with gr.Row():
            msg_input = gr.Textbox(
                placeholder=f"Chat with Phi-3 on {device_used} (max {config.get('ui', 'max_message_length', 2000)} chars)...",
                scale=7,
                max_lines=4,
                show_label=False,
                container=False
            )
            
            with gr.Column(scale=1):
                send_btn = gr.Button("💬 Send", variant="primary", size="lg")
                clear_btn = gr.Button("🗑️ Clear", variant="secondary", size="sm")
        
        # Advanced controls panel
        with gr.Row():
            with gr.Column(scale=2):
                metrics_btn = gr.Button("📊 Performance Metrics", variant="secondary")
                system_btn = gr.Button("ℹ️ System Info", variant="secondary")
            
            with gr.Column(scale=2):
                if ENHANCED_CONTEXT_AVAILABLE:
                    profile_selector = gr.Dropdown(
                        choices=["conservative", "balanced", "aggressive"],
                        value=config.get('deployment', 'npu_profile', 'balanced'),
                        label="NPU Profile",
                        interactive=False  # Would need restart to change
                    )
                
                reset_metrics_btn = gr.Button("🔄 Reset Metrics", variant="secondary")
        
        # Collapsible metrics panel
        with gr.Row(visible=False) as metrics_row:
            with gr.Column(elem_classes=["metrics-panel"]):
                gr.Markdown("### 📊 Real-time Performance Metrics")
                metrics_json = gr.JSON(label="System Metrics", container=True)
                
                if ENHANCED_CONTEXT_AVAILABLE:
                    gr.Markdown("### 🎯 Model-Specific Stats")
                    phi3_stats = gr.JSON(label="Token Filtering & Processing", container=True)
                else:
                    phi3_stats = None
        
        # Examples section
        with gr.Row():
            gr.Examples(
                examples=[
                    "Explain quantum computing in simple terms",
                    "Write a Python function to implement quicksort", 
                    "What are the advantages of using Intel NPU for AI inference?",
                    "Compare different neural network architectures",
                    "Help me debug this code: def factorial(n): return n * factorial(n)",
                    "Explain the concept of attention in transformer models",
                    "What does the uploaded document say about...?",
                    "Summarize the key points from the uploaded files",
                    # Agent examples
                    "What's 15 * 23 + 47?",
                    "What time is it right now?", 
                    "Calculate the square root of 144",
                    "Analyze this text: The quick brown fox jumps over the lazy dog",
                    "What's tomorrow's date?"
                ] if AGENT_AVAILABLE else [
                    "Explain quantum computing in simple terms",
                    "Write a Python function to implement quicksort",
                    "What are the advantages of using Intel NPU for AI inference?",
                    "Compare different neural network architectures", 
                    "Help me debug this code: def factorial(n): return n * factorial(n)",
                    "Explain the concept of attention in transformer models",
                    "What does the uploaded document say about...?",
                    "Summarize the key points from the uploaded files"
                ],
                inputs=msg_input,
                label="💡 Example Questions (Upload documents for context-aware answers)"
            )
        
        # Global generation settings storage
        generation_settings = {
            'temperature': config.get('generation', 'temperature', 0.6),
            'top_p': config.get('generation', 'top_p', 0.95),
            'max_new_tokens': config.get('generation', 'max_new_tokens', 512)
        }
        
        # Event handlers with enhanced functionality
        def handle_send(message, history):
            """
            Handle send with proper session management.
            Passes the already-formatted history directly to the chat processor.
            """
            # Ensure history is a list, even if it's the first turn.
            if history is None:
                history = []
            
            # The 'history' object from a `type='messages'` chatbot is already
            # in the correct List[Dict[str, str]] format. No conversion is needed.
            # We need to yield from the generator, not return it
            yield from enhanced_llm_chat(message, history, generation_settings)
        
        def handle_clear(current_system_prompt):
            """Handle clear with proper session reset"""
            global SYSTEM_PROMPT
            try:
                # Update global system prompt if changed
                if current_system_prompt.strip():
                    SYSTEM_PROMPT = current_system_prompt.strip()
                else:
                    SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT
                
                # End current session and start new one
                pipe.finish_chat()
                pipe.start_chat(SYSTEM_PROMPT)
                print("🔄 Chat session reset with updated system prompt")
                return [], "", SYSTEM_PROMPT
            except Exception as e:
                print(f"⚠️ Session reset error: {e}")
                return [], "", current_system_prompt
        
        def show_metrics():
            """Display comprehensive performance metrics"""
            base_metrics = streaming_metrics.get_summary()
            
            phi3_specific = {}
            if ENHANCED_CONTEXT_AVAILABLE:
                phi3_specific = {
                    "Enhanced Features": "Active",
                    "NPUW Profile": config.get('deployment', 'npu_profile', 'balanced'),
                    "Model Architecture": f"Phi-3-mini-128k-instruct",
                    "Max Context": "128k tokens",
                    "Special Tokens Available": len(PHI3_SPECIAL_TOKENS) if PHI3_SPECIAL_TOKENS else 0
                }
            else:
                phi3_specific = {
                    "Enhanced Features": "Fallback Mode",
                    "Note": "Install enhanced context for full optimization"
                }
            
            return (
                gr.update(value=base_metrics, visible=True),
                gr.update(value=phi3_specific, visible=True) if ENHANCED_CONTEXT_AVAILABLE else gr.update(visible=False),
                gr.update(visible=True)
            )
        
        def show_system_info():
            """Display comprehensive system information"""
            config = get_config()
            model_path = config.get("model", "path")
            cache_dir = config.get("deployment", "cache_directory")
            npu_profile = config.get("deployment", "npu_profile", "balanced")
            
            info_text = f"""
            ## 🖥️ System Configuration
            
            **Hardware & Device:**
            - Target Device: {device_used}
            - Configuration: {config_used}
            - Cache Directory: `{cache_dir}`
            - NPU Profile: {npu_profile}
            
            **Model Details:**
            - Model: Phi-3-mini-128k-instruct INT4 Quantized
            - Path: `{model_path}`
            - Load Time: {load_time:.1f} seconds
            - Tokenizer: HuggingFace AutoTokenizer
            
            **OpenVINO GenAI Configuration:**
            - API Mode: Stateful (start_chat/finish_chat)
            - Conversation Management: Automatic KV-cache
            - Token Limits: {config.get('ui', 'max_conversation_tokens', 1800)} (conversation), {config.get('ui', 'max_message_length', 400)} (message)
            - Generation: Temperature={config.get('generation', 'temperature', 0.6)}, Top-p={config.get('generation', 'top_p', 0.95)}
            
            **Enhanced Features:**
            {"✅ Complete Phi-3 NPUW optimization" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Basic NPUW configuration"}
            {"✅ Advanced special token filtering" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Basic token filtering"}
            {"✅ Phi-3-specific optimizations" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Standard templates"}
            {"✅ Advanced performance monitoring" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Basic metrics"}
            {"✅ RAG document processing" if rag_system.available else "⚠️ RAG not available"}
            {"✅ AI Agent with function-calling" if AGENT_AVAILABLE else "⚠️ Agent system not available"}
            
            **Performance Targets (NPU):**
            - Load Time: <90s (first run), <30s (cached)
            - First Token: <2s latency
            - Generation: 15-25 tokens/second
            - Memory: Optimized for NPU constraints
            """
            
            gr.Info(info_text)
        
        def reset_metrics():
            """Reset performance metrics"""
            streaming_metrics.reset()
            gr.Info("📊 Performance metrics reset successfully")
        
        def reset_system_prompt():
            """Reset system prompt to default"""
            return DEFAULT_SYSTEM_PROMPT
        
        def apply_system_prompt(new_prompt):
            """Apply new system prompt and clear chat"""
            global SYSTEM_PROMPT
            if new_prompt.strip():
                SYSTEM_PROMPT = new_prompt.strip()
            else:
                SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT
            
            try:
                pipe.finish_chat()
                pipe.start_chat(SYSTEM_PROMPT)
                gr.Info("✅ System prompt updated and chat cleared")
                return [], "", SYSTEM_PROMPT
            except Exception as e:
                gr.Warning(f"⚠️ Error applying prompt: {e}")
                return [], "", new_prompt
        
        def handle_file_upload(files):
            """Handle uploaded files for RAG processing"""
            if not files:
                return "No files selected"
            
            results = []
            for file in files:
                if file is None:
                    continue
                
                file_name = os.path.basename(file.name)
                result = rag_system.process_uploaded_file(file.name, file_name)
                results.append(result)
            
            return "\n\n".join(results)
        
        def clear_documents():
            """Clear all uploaded documents"""
            result = rag_system.clear_documents()
            gr.Info(result)
            return result
        
        def show_rag_status():
            """Show RAG system status"""
            status = rag_system.get_status()
            gr.Info(f"RAG Status: {status}")
            return str(status)
        
        def reset_generation_settings():
            """Reset generation settings to defaults"""
            default_temp = config.get('generation', 'temperature', 0.6)
            default_top_p = config.get('generation', 'top_p', 0.95)
            default_max_tokens = config.get('generation', 'max_new_tokens', 512)
            
            generation_settings.update({
                'temperature': default_temp,
                'top_p': default_top_p,
                'max_new_tokens': default_max_tokens
            })
            
            status_text = f"Temperature: {default_temp}, Top-p: {default_top_p}, Max tokens: {default_max_tokens}"
            gr.Info("🔄 Generation settings reset to defaults")
            return default_temp, default_top_p, default_max_tokens, status_text
        
        def apply_generation_settings(temp, top_p, max_tokens):
            """Apply new generation settings"""
            generation_settings.update({
                'temperature': temp,
                'top_p': top_p,
                'max_new_tokens': int(max_tokens)
            })
            
            status_text = f"Temperature: {temp}, Top-p: {top_p}, Max tokens: {int(max_tokens)}"
            gr.Info("✅ Generation settings applied")
            return status_text
        
        def show_agent_status():
            """Show agent system status and capabilities"""
            if not AGENT_AVAILABLE:
                status = "❌ Agent system not available. Install dependencies to enable."
                gr.Info(status)
                return status, gr.update(visible=True)
            
            agent = get_agent()
            if agent:
                status = f"""✅ Agent system active
                
Available Tools: {len(agent.tools)}
Framework: ReAct (Reasoning + Acting)
Integration: OpenVINO GenAI + LangChain

The agent automatically detects when to use tools based on your questions.
Ask naturally and it will use the appropriate tools to help you!"""
            else:
                status = "⚠️ Agent system loaded but not initialized"
            
            gr.Info("Agent status displayed")
            return status, gr.update(visible=True)
        
        def list_agent_tools():
            """List all available agent tools with descriptions"""
            if not AGENT_AVAILABLE:
                status = "❌ No tools available - agent system not loaded"
                gr.Info(status)
                return status, gr.update(visible=True)
            
            agent = get_agent()
            if agent:
                tools_info = []
                for name, info in agent.tools.items():
                    tools_info.append(f"🔧 **{name}**: {info['description']}")
                
                status = "Available Agent Tools:\n\n" + "\n\n".join(tools_info)
            else:
                status = "⚠️ Agent not initialized"
            
            gr.Info("Tools list displayed")
            return status, gr.update(visible=True)
        
        # Wire up event handlers
        msg_input.submit(handle_send, [msg_input, chatbot], chatbot).then(
            lambda: gr.update(value=""), None, [msg_input]
        )
        
        send_btn.click(handle_send, [msg_input, chatbot], chatbot).then(
            lambda: gr.update(value=""), None, [msg_input]
        )
        
        clear_btn.click(handle_clear, [system_prompt_input], [chatbot, msg_input, system_prompt_input])
        
        # Build outputs list for metrics button (filter out None values)
        metrics_outputs = [metrics_json, metrics_row]
        if ENHANCED_CONTEXT_AVAILABLE and phi3_stats is not None:
            metrics_outputs.insert(1, phi3_stats)
        
        metrics_btn.click(
            show_metrics, 
            None, 
            metrics_outputs
        )
        
        system_btn.click(show_system_info, None, None)
        reset_metrics_btn.click(reset_metrics, None, None)
        
        # System prompt event handlers
        reset_prompt_btn.click(reset_system_prompt, None, [system_prompt_input])
        apply_prompt_btn.click(
            apply_system_prompt, 
            [system_prompt_input], 
            [chatbot, msg_input, system_prompt_input]
        )
        
        # RAG event handlers - always enable, will show error if RAG not available
        file_upload.upload(handle_file_upload, [file_upload], [upload_status])
        clear_docs_btn.click(clear_documents, None, [upload_status])
        rag_status_btn.click(show_rag_status, None, [upload_status])
        
        # Generation settings event handlers
        reset_gen_settings_btn.click(
            reset_generation_settings,
            None,
            [temperature_slider, top_p_slider, max_tokens_number, gen_settings_status]
        )
        
        apply_gen_settings_btn.click(
            apply_generation_settings,
            [temperature_slider, top_p_slider, max_tokens_number],
            [gen_settings_status]
        )
        
        # Agent event handlers (if available)
        if AGENT_AVAILABLE:
            agent_status_btn.click(
                show_agent_status,
                None,
                [agent_status_output, agent_status_output]
            )
            
            agent_tools_btn.click(
                list_agent_tools,
                None,
                [agent_status_output, agent_status_output]
            )
        
        # Initialize chat session when interface loads
        def initialize_session():
            """Initialize chat session with system prompt"""
            try:
                pipe.start_chat(SYSTEM_PROMPT)
                print("✅ Chat session initialized with system prompt")
            except Exception as e:
                print(f"⚠️ Session initialization error: {e}")
        
        demo.load(initialize_session, None, None)
    
    return demo


def initialize_ui_globals(pipeline, device, config, load_time_val):
    """Initialize global UI variables"""
    global pipe, device_used, config_used, load_time
    pipe = pipeline
    device_used = device
    config_used = config
    load_time = load_time_val
────────────────────────────────────────────────────────────
END OF FILE: ui.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: streamer.py
PATH: app\streamer.py
PURPOSE: Token streaming and filtering for LLMs
SIZE: 8,226 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

"""
Token Streaming and Filtering
============================

Enhanced streaming implementation with Phi-3-specific token filtering,
performance monitoring, and robust error handling.
"""

import time
import queue
from typing import Optional
import openvino_genai as ov_genai
from transformers import AutoTokenizer

# Import Phi-3-specific filtering
try:
    from .npu_patterns import (
        is_phi3_special_token,
        PHI3_SPECIAL_TOKENS
    )
    ENHANCED_CONTEXT_AVAILABLE = True
    print("✅ Enhanced Phi-3 token filtering loaded")
except ImportError:
    print("⚠️ Enhanced token filtering not available - using fallback")
    ENHANCED_CONTEXT_AVAILABLE = False


class EnhancedLLMStreamer(ov_genai.StreamerBase):
    """
    Production-ready streamer with Phi-3-specific optimizations:
    - Proper special token filtering (26+ tokens)
    - Performance monitoring
    - Robust error handling
    - Token-level streaming control
    """
    
    def __init__(self, tokenizer: AutoTokenizer, metrics_callback=None):
        """
        Initialize enhanced streamer.
        
        Args:
            tokenizer: HuggingFace tokenizer for decoding
            metrics_callback: Optional callback to update global metrics
        """
        super().__init__()
        self.tokenizer = tokenizer
        self.text_queue = queue.Queue()
        self.accumulated_tokens = []
        self.current_text = ""
        self.start_time = time.time()
        self.first_token_time = None
        self.tokens_generated = 0
        self.metrics_callback = metrics_callback
        
        # Initialize Phi-3-specific filtering
        if ENHANCED_CONTEXT_AVAILABLE:
            self.use_phi3_filtering = True
            print("✅ Using enhanced Phi-3 token filtering")
        else:
            self.use_phi3_filtering = False
            print("⚠️ Using basic token filtering")
    
    def put(self, token_id: int) -> bool:
        """Process token with robust decoding and Phi-3-specific filtering"""
        self.accumulated_tokens.append(token_id)
        self.tokens_generated += 1

        # Record first token latency
        if self.first_token_time is None:
            self.first_token_time = time.time()

        try:
            # CORRECT: Decode the full sequence of tokens to handle multi-byte characters.
            full_text = self.tokenizer.decode(self.accumulated_tokens, skip_special_tokens=False)

            # Find the newly generated text by comparing with the previous state
            if len(full_text) > len(self.current_text):
                new_text = full_text[len(self.current_text):]

                # Now, check if the newly decoded chunk is a special token
                if self.use_phi3_filtering and is_phi3_special_token(new_text.strip()):
                    # It's a special token. Filter it by not sending it to the queue.
                    if self.metrics_callback:
                        self.metrics_callback("special_tokens_filtered", 1)
                else:
                    # It's a valid text chunk. Send it to the UI.
                    self.text_queue.put(new_text)

                # In either case, update the current full text state.
                self.current_text = full_text

        except Exception as e:
            print(f"❌ Token processing error: {e}")
            return False

        return False  # Continue generation
    
    def _is_special_token_text(self, text: str) -> bool:
        """Basic special token detection for fallback"""
        special_patterns = [
            '<|endoftext|>', '<|system|>', '<|user|>', '<|assistant|>', '<|end|>',
            '<think>', '</think>', '<tool_call>', '</tool_call>'
        ]
        
        for pattern in special_patterns:
            if pattern in text:
                if self.metrics_callback:
                    self.metrics_callback("special_tokens_filtered", 1)
                return True
        
        return False
    
    def end(self):
        """Finalize streaming and calculate performance metrics"""
        # Calculate performance metrics
        total_time = time.time() - self.start_time
        first_token_latency = (self.first_token_time - self.start_time) if self.first_token_time else 0
        tokens_per_second = self.tokens_generated / total_time if total_time > 0 else 0
        
        # Update global metrics via callback
        if self.metrics_callback:
            self.metrics_callback("first_token_latency", first_token_latency)
            self.metrics_callback("tokens_per_second", tokens_per_second)
            self.metrics_callback("total_tokens_generated", self.tokens_generated)
        
        # Log performance
        print(f"🚀 Generation complete: {self.tokens_generated} tokens in {total_time:.2f}s")
        print(f"   First token: {first_token_latency:.3f}s, Speed: {tokens_per_second:.1f} tok/s")
        
        
        # Signal end of generation
        self.text_queue.put(None)
    
    def __iter__(self):
        return self
    
    def __next__(self):
        item = self.text_queue.get()
        if item is None:
            raise StopIteration
        return item


class StreamingMetrics:
    """Simple metrics tracking for streaming performance"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """Reset all metrics"""
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.avg_response_time = 0.0
        self.avg_first_token_latency = 0.0
        self.avg_tokens_per_second = 0.0
        self.total_tokens_generated = 0
        self.special_tokens_filtered = 0
        self.session_start_time = time.time()
    
    def update_metric(self, metric_name: str, value):
        """Update a specific metric"""
        if metric_name == "special_tokens_filtered":
            self.special_tokens_filtered += value
        elif metric_name == "total_tokens_generated":
            self.total_tokens_generated += value
        elif metric_name == "first_token_latency":
            # Calculate running average
            if self.successful_requests > 0:
                self.avg_first_token_latency = (
                    (self.avg_first_token_latency * (self.successful_requests - 1) + value)
                    / self.successful_requests
                )
            else:
                self.avg_first_token_latency = value
        elif metric_name == "tokens_per_second":
            # Calculate running average
            if self.successful_requests > 0:
                self.avg_tokens_per_second = (
                    (self.avg_tokens_per_second * (self.successful_requests - 1) + value)
                    / self.successful_requests
                )
            else:
                self.avg_tokens_per_second = value
    
    def start_request(self):
        """Mark the start of a new request"""
        self.total_requests += 1
    
    def end_request(self, success: bool, response_time: float):
        """Mark the end of a request"""
        if success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
        
        # Update average response time
        self.avg_response_time = (
            (self.avg_response_time * (self.total_requests - 1) + response_time)
            / self.total_requests
        )
    
    def get_summary(self) -> dict:
        """Get metrics summary"""
        session_duration = time.time() - self.session_start_time
        success_rate = (self.successful_requests / max(self.total_requests, 1)) * 100
        
        return {
            "Session Duration": f"{session_duration:.1f}s",
            "Total Requests": self.total_requests,
            "Success Rate": f"{success_rate:.1f}%",
            "Avg Response Time": f"{self.avg_response_time:.2f}s",
            "Avg First Token": f"{self.avg_first_token_latency:.3f}s",
            "Avg Tokens/Second": f"{self.avg_tokens_per_second:.1f}",
            "Total Tokens Generated": self.total_tokens_generated,
            "Special Tokens Filtered": self.special_tokens_filtered,
        }


# Global metrics instance
streaming_metrics = StreamingMetrics()
────────────────────────────────────────────────────────────
END OF FILE: streamer.py
────────────────────────────────────────────────────────────


================================================================================
CATEGORY: SPECIALIZED MODULES
================================================================================

DESCRIPTION: Specialized modules for NPU optimization and agent capabilities

────────────────────────────────────────────────────────────
FILE: npu_patterns.py
PATH: app\npu_patterns.py
PURPOSE: NPU-specific optimization patterns for Phi-3
SIZE: 5,491 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

#!/usr/bin/env python3
"""
Phi-3 NPU Optimization Patterns
==============================

Generic NPU optimization patterns extracted from legacy Qwen3 context,
updated for Phi-3-mini-128k-instruct compatibility.

This replaces the incompatible Qwen3-specific context with model-agnostic
NPU patterns that actually work with Phi-3.
"""

import openvino_genai as ov_genai


# NPU Configuration Profiles
def get_npu_config_conservative():
    """Conservative NPU configuration for stable operation"""
    return {
        "NPU_USE_NPUW": "YES",
        "NPUW_LLM": "YES",
        "NPUW_LLM_BATCH_DIM": 0,
        "NPUW_LLM_SEQ_LEN_DIM": 1,
        "NPUW_LLM_MAX_PROMPT_LEN": 1024,
        "NPUW_LLM_MIN_RESPONSE_LEN": 128,
        "NPUW_LLM_PREFILL_HINT": "FAST_COMPILE",
        "NPUW_LLM_GENERATE_HINT": "BEST_PERF",
        "CACHE_MODE": "OPTIMIZE_SPEED"
    }


def get_npu_config_balanced():
    """Balanced NPU configuration for Phi-3"""
    return {
        "NPU_USE_NPUW": "YES",
        "NPUW_LLM": "YES",
        "NPUW_LLM_BATCH_DIM": 0,
        "NPUW_LLM_SEQ_LEN_DIM": 1,
        "NPUW_LLM_MAX_PROMPT_LEN": 2048,
        "NPUW_LLM_MIN_RESPONSE_LEN": 256,
        "NPUW_LLM_PREFILL_HINT": "FAST_COMPILE",
        "NPUW_LLM_GENERATE_HINT": "BEST_PERF",
        "CACHE_MODE": "OPTIMIZE_SPEED"
    }


def get_npu_config_aggressive():
    """Aggressive NPU configuration for maximum context"""
    return {
        "NPU_USE_NPUW": "YES",
        "NPUW_LLM": "YES",
        "NPUW_LLM_BATCH_DIM": 0,
        "NPUW_LLM_SEQ_LEN_DIM": 1,
        "NPUW_LLM_MAX_PROMPT_LEN": 8192,  # Use Phi-3's 128k context capability
        "NPUW_LLM_MIN_RESPONSE_LEN": 512,
        "NPUW_LLM_PREFILL_HINT": "FAST_COMPILE",
        "NPUW_LLM_GENERATE_HINT": "BEST_PERF",
        "CACHE_MODE": "OPTIMIZE_SPEED"
    }


def initialize_phi3_pipeline(model_path, device="NPU", profile="balanced", **kwargs):
    """
    Initialize Phi-3 pipeline with proper NPU configuration
    
    Args:
        model_path: Path to Phi-3 OpenVINO model
        device: Target device ("NPU", "CPU", "GPU")
        profile: NPU profile ("conservative", "balanced", "aggressive")
        **kwargs: Additional configuration overrides
    
    Returns:
        Initialized LLMPipeline optimized for Phi-3
    """
    
    # Get profile-specific configuration
    if profile == "conservative":
        base_config = get_npu_config_conservative()
    elif profile == "balanced":
        base_config = get_npu_config_balanced()
    elif profile == "aggressive":
        base_config = get_npu_config_aggressive()
    else:
        raise ValueError(f"Unknown profile: {profile}")
    
    # Add cache directory
    base_config["CACHE_DIR"] = f".ovcache_phi3_{device.lower()}"
    
    # Merge with user overrides
    final_config = {**base_config, **kwargs}
    
    # Three-tier initialization for maximum compatibility
    try:
        print(f"🚀 Initializing Phi-3 on {device} with {profile} profile...")
        pipe = ov_genai.LLMPipeline(model_path, device, **final_config)
        print(f"✅ Phi-3 initialization successful with {profile} config")
        return pipe
        
    except Exception as e1:
        print(f"⚠️ {profile} config failed: {e1}")
        
        try:
            # Fallback: Basic configuration
            basic_config = {
                "NPU_USE_NPUW": "YES",
                "NPUW_LLM": "YES",
                "CACHE_DIR": f".ovcache_phi3_{device.lower()}"
            }
            pipe = ov_genai.LLMPipeline(model_path, device, **basic_config)
            print(f"✅ Phi-3 initialization successful with basic config")
            return pipe
            
        except Exception as e2:
            print(f"⚠️ Basic config failed: {e2}")
            
            # Final fallback: Minimal configuration
            pipe = ov_genai.LLMPipeline(model_path, device)
            print(f"✅ Phi-3 initialization successful with minimal config")
            return pipe


# Phi-3 Special Tokens (correct tokens for this model)
PHI3_SPECIAL_TOKENS = {
    # Phi-3 uses different token format than Qwen3
    "<|endoftext|>": True,
    "<|system|>": True,
    "<|user|>": True,
    "<|assistant|>": True,
    "<|end|>": True
}


def is_phi3_special_token(token_text):
    """Check if a token is a Phi-3 special token that should be filtered"""
    if not token_text:
        return False
    
    token_text = token_text.strip()
    return token_text in PHI3_SPECIAL_TOKENS


# Performance profiles for Phi-3
PHI3_PERFORMANCE_PROFILES = {
    "NPU": {
        "typical_load_time": "60-90 seconds",
        "tokens_per_second": "10-20 tokens/sec",  # More conservative than Qwen3
        "max_concurrent_users": 1,
        "recommended_prompt_length": "< 2048 tokens",
        "memory_usage": "Low (NPU optimized)",
        "notes": "Phi-3 has different performance characteristics than Qwen3"
    },
    
    "CPU": {
        "typical_load_time": "10-25 seconds",
        "tokens_per_second": "5-12 tokens/sec",
        "max_concurrent_users": 2,
        "recommended_prompt_length": "< 8192 tokens (can use 128k context)",
        "memory_usage": "Moderate (3.8B parameters vs 8B in Qwen3)",
        "notes": "Phi-3 is smaller than Qwen3, uses less memory"
    }
}

if __name__ == "__main__":
    print("Phi-3 NPU Optimization Patterns Loaded")
    print("✅ Model-agnostic NPU configuration")
    print("✅ Correct Phi-3 special tokens")
    print("✅ Proper performance profiles")
────────────────────────────────────────────────────────────
END OF FILE: npu_patterns.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: agent.py
PATH: app\agent.py
PURPOSE: ReAct agent implementation with tool usage
SIZE: 16,542 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

"""
Agentic Architecture Module (Phase 3.3)
=======================================

Function-calling agent system that transforms the chat application from a passive 
chatbot into an active AI agent capable of using tools and performing complex tasks.

This implements the ReAct (Reasoning + Acting) pattern with OpenVINO GenAI as the 
reasoning engine and LangChain as the agent framework.
"""

import json
import re
import math
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass

# Agent framework imports with fallback
try:
    from langchain_core.tools import Tool
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    AGENT_AVAILABLE = True
    print("✅ Agent framework loaded successfully")
except ImportError as e:
    print(f"⚠️ Agent framework not available: {e}")
    print("📝 Install with: pip install langchain-core langchain-experimental")
    AGENT_AVAILABLE = False


@dataclass
class ToolResult:
    """Result from tool execution"""
    success: bool
    result: str
    error: Optional[str] = None


class AgentTools:
    """Collection of tools available to the AI agent"""
    
    @staticmethod
    def calculator(expression: str) -> ToolResult:
        """
        Perform mathematical calculations safely.
        
        Args:
            expression: Mathematical expression to evaluate (e.g., "2 + 2", "sqrt(16)")
            
        Returns:
            ToolResult with calculation result or error
        """
        try:
            # Sanitize input - only allow safe mathematical operations
            safe_chars = set('0123456789+-*/()., ')
            safe_functions = ['abs', 'ceil', 'floor', 'sqrt', 'pow', 'log', 'sin', 'cos', 'tan']
            
            # Replace safe function names
            sanitized = expression.lower()
            for func in safe_functions:
                sanitized = sanitized.replace(func, f'math.{func}')
            
            # Check for dangerous patterns
            dangerous = ['import', 'exec', 'eval', '__', 'open', 'file', 'input']
            if any(d in sanitized.lower() for d in dangerous):
                return ToolResult(False, "", "Expression contains unsafe operations")
            
            # Evaluate using math module in restricted environment
            import math
            allowed_names = {
                'math': math,
                'abs': abs, 'round': round, 'min': min, 'max': max,
                'sum': sum, 'len': len
            }
            
            result = eval(sanitized, {"__builtins__": {}}, allowed_names)
            return ToolResult(True, str(result))
            
        except Exception as e:
            return ToolResult(False, "", f"Calculation error: {str(e)}")
    
    @staticmethod
    def datetime_info(query: str = "") -> ToolResult:
        """
        Get current date, time, or calculate date differences.
        
        Args:
            query: Optional specific query like "tomorrow", "next week", etc.
            
        Returns:
            ToolResult with date/time information
        """
        try:
            now = datetime.now()
            
            if not query or query.lower() in ["now", "current", "today"]:
                result = f"Current date and time: {now.strftime('%Y-%m-%d %H:%M:%S')}"
            
            elif "tomorrow" in query.lower():
                tomorrow = now + timedelta(days=1)
                result = f"Tomorrow's date: {tomorrow.strftime('%Y-%m-%d')}"
            
            elif "yesterday" in query.lower():
                yesterday = now - timedelta(days=1)
                result = f"Yesterday's date: {yesterday.strftime('%Y-%m-%d')}"
            
            elif "next week" in query.lower():
                next_week = now + timedelta(weeks=1)
                result = f"Next week: {next_week.strftime('%Y-%m-%d')}"
            
            elif "week" in query.lower() and "ago" in query.lower():
                last_week = now - timedelta(weeks=1)
                result = f"One week ago: {last_week.strftime('%Y-%m-%d')}"
            
            else:
                result = f"Current date and time: {now.strftime('%Y-%m-%d %H:%M:%S')}\nDay of week: {now.strftime('%A')}"
            
            return ToolResult(True, result)
            
        except Exception as e:
            return ToolResult(False, "", f"DateTime error: {str(e)}")
    
    @staticmethod
    def web_search_mock(query: str) -> ToolResult:
        """
        Mock web search tool (placeholder for security).
        In production, this would connect to a search API.
        
        Args:
            query: Search query string
            
        Returns:
            ToolResult with mock search results
        """
        try:
            # This is a mock implementation for security reasons
            # In production, you would integrate with DuckDuckGo API, Google Custom Search, etc.
            
            mock_results = [
                f"Search results for: '{query}'",
                "• [Mock Result 1] This would be a real web search result",
                "• [Mock Result 2] Integration with search APIs available",
                "• [Mock Result 3] Enable by configuring search provider API keys",
                "",
                "Note: This is a mock tool. Configure real search API to enable live results."
            ]
            
            return ToolResult(True, "\n".join(mock_results))
            
        except Exception as e:
            return ToolResult(False, "", f"Search error: {str(e)}")
    
    @staticmethod
    def text_analysis(text: str, analysis_type: str = "summary") -> ToolResult:
        """
        Analyze text content (word count, character count, basic analysis).
        
        Args:
            text: Text to analyze
            analysis_type: Type of analysis ("summary", "count", "readability")
            
        Returns:
            ToolResult with text analysis
        """
        try:
            if not text:
                return ToolResult(False, "", "No text provided for analysis")
            
            # Basic analysis metrics
            word_count = len(text.split())
            char_count = len(text)
            char_count_no_spaces = len(text.replace(' ', ''))
            sentence_count = len([s for s in text.split('.') if s.strip()])
            paragraph_count = len([p for p in text.split('\n\n') if p.strip()])
            
            if analysis_type.lower() == "count":
                result = f"Word count: {word_count}\nCharacter count: {char_count}\nCharacters (no spaces): {char_count_no_spaces}\nSentences: {sentence_count}\nParagraphs: {paragraph_count}"
            
            elif analysis_type.lower() == "readability":
                # Simple readability metrics
                avg_words_per_sentence = word_count / max(sentence_count, 1)
                avg_chars_per_word = char_count_no_spaces / max(word_count, 1)
                
                result = f"Readability Analysis:\n• Average words per sentence: {avg_words_per_sentence:.1f}\n• Average characters per word: {avg_chars_per_word:.1f}\n• Text complexity: {'Simple' if avg_words_per_sentence < 15 else 'Moderate' if avg_words_per_sentence < 25 else 'Complex'}"
            
            else:  # summary
                result = f"Text Analysis Summary:\n• Words: {word_count}\n• Characters: {char_count}\n• Sentences: {sentence_count}\n• Paragraphs: {paragraph_count}\n• Average sentence length: {word_count / max(sentence_count, 1):.1f} words"
            
            return ToolResult(True, result)
            
        except Exception as e:
            return ToolResult(False, "", f"Analysis error: {str(e)}")


class ReActAgent:
    """ReAct (Reasoning + Acting) Agent using OpenVINO GenAI"""
    
    def __init__(self, llm_pipeline, tokenizer):
        """
        Initialize ReAct agent with LLM pipeline and available tools.
        
        Args:
            llm_pipeline: OpenVINO GenAI pipeline instance
            tokenizer: Tokenizer instance
        """
        self.llm = llm_pipeline
        self.tokenizer = tokenizer
        self.available = AGENT_AVAILABLE
        
        # Define available tools
        self.tools = {
            "calculator": {
                "function": AgentTools.calculator,
                "description": "Perform mathematical calculations. Input: mathematical expression (e.g., '2+2', 'sqrt(16)', 'sin(3.14/2)')",
                "parameters": "expression (string): Mathematical expression to evaluate"
            },
            "datetime": {
                "function": AgentTools.datetime_info,
                "description": "Get current date/time or calculate date differences. Input: query like 'now', 'tomorrow', 'next week'",
                "parameters": "query (string): Date/time query or leave empty for current datetime"
            },
            "web_search": {
                "function": AgentTools.web_search_mock,
                "description": "Search the web for information (mock implementation). Input: search query string",
                "parameters": "query (string): What to search for"
            },
            "text_analysis": {
                "function": AgentTools.text_analysis,
                "description": "Analyze text content (word count, readability, etc.). Input: text and analysis type",
                "parameters": "text (string): Text to analyze, analysis_type (string): 'summary', 'count', or 'readability'"
            }
        }
        
        # ReAct prompt template
        self.system_prompt = self._create_system_prompt()
        
        if self.available:
            print(f"✅ ReAct agent initialized with {len(self.tools)} tools")
        else:
            print("⚠️ Agent functionality limited - install langchain-core for full capabilities")
    
    def _create_system_prompt(self) -> str:
        """Create the ReAct system prompt with tool descriptions"""
        
        tools_description = "\n".join([
            f"- {name}: {info['description']}" 
            for name, info in self.tools.items()
        ])
        
        return f"""You are an AI assistant with access to tools. Use the ReAct (Reasoning + Acting) approach to solve problems.

Available Tools:
{tools_description}

When you need to use a tool, format your response EXACTLY like this:
```
Thought: I need to use a tool to help with this request.
Action: tool_name
Action Input: input_for_tool
```

After receiving tool results, continue reasoning and provide a final answer.

Example:
User: What's 25 * 4?
Assistant: I need to calculate this multiplication.

Thought: I need to calculate 25 * 4.
Action: calculator  
Action Input: 25 * 4

[Tool Result: 100]

The result of 25 * 4 is 100.

Always think step by step and use tools when they can help provide accurate information."""

    def _parse_action(self, text: str) -> Optional[Tuple[str, str]]:
        """
        Parse action from LLM response.
        
        Args:
            text: LLM response text
            
        Returns:
            Tuple of (tool_name, input) if action found, None otherwise
        """
        # Look for Action: tool_name pattern
        action_match = re.search(r'Action:\s*(\w+)', text, re.IGNORECASE)
        if not action_match:
            return None
        
        tool_name = action_match.group(1).lower()
        
        # Look for Action Input: input pattern
        input_match = re.search(r'Action Input:\s*(.+?)(?:\n|$)', text, re.IGNORECASE | re.DOTALL)
        if not input_match:
            return None
        
        tool_input = input_match.group(1).strip()
        
        return (tool_name, tool_input)
    
    def _execute_tool(self, tool_name: str, tool_input: str) -> ToolResult:
        """
        Execute a tool with given input.
        
        Args:
            tool_name: Name of tool to execute
            tool_input: Input parameters for the tool
            
        Returns:
            ToolResult with execution result
        """
        if tool_name not in self.tools:
            return ToolResult(False, "", f"Tool '{tool_name}' not found. Available: {list(self.tools.keys())}")
        
        try:
            tool_func = self.tools[tool_name]["function"]
            
            # Handle different tool signatures
            if tool_name == "text_analysis" and "," in tool_input:
                # Split text and analysis_type
                parts = [p.strip() for p in tool_input.split(",", 1)]
                if len(parts) == 2:
                    return tool_func(parts[0], parts[1])
                else:
                    return tool_func(tool_input)
            else:
                return tool_func(tool_input)
                
        except Exception as e:
            return ToolResult(False, "", f"Tool execution error: {str(e)}")
    
    def process_with_tools(self, user_message: str, generation_params: Dict[str, Any] = None) -> str:
        """
        Process user message using ReAct pattern with tools.
        
        Args:
            user_message: User's input message
            generation_params: Optional generation parameters
            
        Returns:
            Final response after tool usage (if needed)
        """
        if not self.available:
            return f"🤖 Basic Response: {user_message}\n\n⚠️ Agent tools not available. Install langchain-core for full capabilities."
        
        max_iterations = 5  # Prevent infinite loops
        conversation_history = [
            f"System: {self.system_prompt}",
            f"User: {user_message}"
        ]
        
        for iteration in range(max_iterations):
            # Generate response with current conversation context
            full_context = "\n\n".join(conversation_history)
            
            try:
                # Start a fresh chat session for agent reasoning
                self.llm.start_chat(self.system_prompt)
                
                # Generate response
                from .chat import create_phi3_generation_config
                gen_config = create_phi3_generation_config(generation_params)
                
                response = ""
                for chunk in self.llm.generate(full_context, gen_config):
                    response += chunk
                
                self.llm.finish_chat()
                
            except Exception as e:
                return f"❌ Agent processing error: {str(e)}"
            
            # Check if response contains an action
            action_result = self._parse_action(response)
            
            if action_result is None:
                # No action found, this is the final response
                return response
            
            tool_name, tool_input = action_result
            
            # Execute the tool
            print(f"🔧 Executing tool: {tool_name} with input: {tool_input}")
            tool_result = self._execute_tool(tool_name, tool_input)
            
            # Add tool result to conversation
            if tool_result.success:
                conversation_history.append(f"Assistant: {response}")
                conversation_history.append(f"Tool Result: {tool_result.result}")
            else:
                error_msg = tool_result.error or "Tool execution failed"
                conversation_history.append(f"Assistant: {response}")
                conversation_history.append(f"Tool Error: {error_msg}")
        
        # If we've exceeded max iterations, return what we have
        return f"⚠️ Reasoning process exceeded maximum iterations. Last response:\n\n{response}"
    
    def get_available_tools(self) -> Dict[str, str]:
        """Get list of available tools with descriptions"""
        return {
            name: info["description"] 
            for name, info in self.tools.items()
        }


# Global agent instance (initialized by main.py)
react_agent: Optional[ReActAgent] = None


def initialize_agent(llm_pipeline, tokenizer):
    """Initialize the global agent instance"""
    global react_agent
    react_agent = ReActAgent(llm_pipeline, tokenizer)
    return react_agent


def get_agent() -> Optional[ReActAgent]:
    """Get the global agent instance"""
    return react_agent
────────────────────────────────────────────────────────────
END OF FILE: agent.py
────────────────────────────────────────────────────────────


================================================================================
CATEGORY: UTILITIES AND TOOLS
================================================================================

DESCRIPTION: Utility scripts and development tools

────────────────────────────────────────────────────────────
FILE: check_model_config.py
PATH: check_model_config.py
PURPOSE: Model configuration validation utility
SIZE: 4,009 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

#!/usr/bin/env python3
"""
Check current model configuration and identify export settings
"""

import json
from pathlib import Path

def check_model_config(model_path):
    """Check the configuration of an existing OpenVINO model"""
    
    model_dir = Path(model_path)
    config_file = model_dir / "config.json"
    
    print(f"🔍 Checking model: {model_path}")
    print("=" * 50)
    
    if not model_dir.exists():
        print(f"❌ Model directory not found: {model_path}")
        return
    
    if not config_file.exists():
        print(f"❌ config.json not found in {model_path}")
        return
    
    try:
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        print("📋 Model Configuration:")
        print(f"   Model type: {config.get('model_type', 'Unknown')}")
        print(f"   Architecture: {config.get('architectures', 'Unknown')}")
        
        if 'max_position_embeddings' in config:
            print(f"   Max position embeddings: {config['max_position_embeddings']}")
        
        if 'hidden_size' in config:
            print(f"   Hidden size: {config['hidden_size']}")
            
        if 'num_attention_heads' in config:
            print(f"   Attention heads: {config['num_attention_heads']}")
            
        if 'vocab_size' in config:
            print(f"   Vocabulary size: {config['vocab_size']}")
        
        # Check for quantization info
        if 'quantization_config' in config:
            quant_config = config['quantization_config']
            print(f"\n🔧 Quantization Configuration:")
            for key, value in quant_config.items():
                print(f"   {key}: {value}")
        else:
            print("\n❓ No quantization configuration found")
        
        # Check OpenVINO files
        print(f"\n📁 OpenVINO Files:")
        xml_file = model_dir / "openvino_model.xml"
        bin_file = model_dir / "openvino_model.bin"
        
        if xml_file.exists():
            print(f"   ✓ Model XML: {xml_file.name} ({xml_file.stat().st_size / (1024*1024):.1f} MB)")
        else:
            print(f"   ❌ Missing: openvino_model.xml")
            
        if bin_file.exists():
            print(f"   ✓ Model weights: {bin_file.name} ({bin_file.stat().st_size / (1024*1024):.1f} MB)")
        else:
            print(f"   ❌ Missing: openvino_model.bin")
        
        # Check for tokenizer
        tokenizer_files = ["tokenizer.json", "tokenizer_config.json"]
        print(f"\n🔤 Tokenizer Files:")
        for tok_file in tokenizer_files:
            tok_path = model_dir / tok_file
            if tok_path.exists():
                print(f"   ✓ {tok_file}")
            else:
                print(f"   ❌ Missing: {tok_file}")
        
        # Detect likely source model
        print(f"\n🎯 Likely Source Model:")
        model_type = config.get('model_type', '').lower()
        if 'phi3' in model_type or 'phi-3' in model_type:
            print(f"   This appears to be a Phi-3 model")
            if config.get('hidden_size') == 3072:
                print(f"   Likely: microsoft/Phi-3-mini-128k-instruct")
        elif 'qwen' in model_type:
            print(f"   This appears to be a Qwen model")
            if config.get('hidden_size') == 4096:
                print(f"   Likely: Qwen2.5-7B or similar")
            elif config.get('hidden_size') == 3584:
                print(f"   Likely: Qwen2.5-3B or similar") 
        
        # Check if model supports NPU
        print(f"\n🖥️  NPU Compatibility Analysis:")
        print(f"   Current export likely has dynamic shapes (causing NPU compilation failure)")
        print(f"   Recommendation: Re-export with static shapes using export_model_for_npu.py")
        
        return config
        
    except Exception as e:
        print(f"❌ Error reading config: {e}")

if __name__ == "__main__":
    model_path = r"C:\OpenVinoModels\phi3-128k-npu"
    check_model_config(model_path)
────────────────────────────────────────────────────────────
END OF FILE: check_model_config.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: export_model_for_npu.py
PATH: export_model_for_npu.py
PURPOSE: NPU model export and optimization tool
SIZE: 7,432 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

#!/usr/bin/env python3
"""
NPU-Compatible Model Export Script
==================================

This script exports language models (Phi-3, Qwen, etc.) with NPU-specific optimizations:
- Static shapes for KV-cache (required for NPU)
- Symmetric INT4 quantization
- Stateful KV-cache implementation
- Channel-wise quantization for >1B parameter models

Usage:
    python export_model_for_npu.py --model microsoft/Phi-3-mini-128k-instruct --output phi3-128k-npu
    python export_model_for_npu.py --model Qwen/Qwen2.5-7B-Instruct --output qwen2.5-7b-npu
"""

import argparse
import os
import sys
from pathlib import Path

def check_requirements():
    """Check if required packages are installed"""
    try:
        import optimum.intel
        import openvino
        import transformers
        print("✓ All required packages found")
    except ImportError as e:
        print(f"❌ Missing required package: {e}")
        print("Install with: pip install optimum-intel transformers")
        return False
    return True

def export_model_for_npu(model_name, output_dir, max_seq_len=2048, min_response_len=256):
    """
    Export language model optimized for NPU inference
    
    Args:
        model_name: Hugging Face model name (e.g., "microsoft/Phi-3-mini-128k-instruct") 
        output_dir: Directory to save the exported model
        max_seq_len: Maximum sequence length for static shapes
        min_response_len: Minimum response length for NPU optimization
    """
    
    print(f"🚀 Exporting {model_name} for NPU...")
    print(f"📁 Output directory: {output_dir}")
    print(f"⚙️  Max sequence length: {max_seq_len}")
    print(f"⚙️  Min response length: {min_response_len}")
    
    # Ensure output directory exists
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # NPU Export Command - Critical parameters for NPU compatibility
    cmd = [
        "optimum-cli", "export", "openvino",
        "--model", model_name,
        "--task", "text-generation-with-past",  # Enable KV-cache
        "--weight-format", "int4",              # NPU-optimized quantization
        "--sym",                                # Symmetric quantization (NPU preferred)
        "--group-size", "-1",                   # Channel-wise for >1B models
        "--ratio", "1.0",                       # Full model quantization
        "--trust-remote-code",                  # For models requiring custom code
        output_dir
    ]
    
    print("\n🔄 Running export command:")
    print(" ".join(cmd))
    print("\n" + "="*60)
    
    # Execute the export
    import subprocess
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print("✓ Export completed successfully!")
        print(result.stdout)
        
        # Create NPU configuration file
        create_npu_config_file(output_dir, max_seq_len, min_response_len)
        
        print(f"\n🎉 NPU-compatible model exported to: {output_dir}")
        print(f"📋 Configuration saved to: {output_dir}/npu_config.py")
        
    except subprocess.CalledProcessError as e:
        print(f"❌ Export failed: {e}")
        print("STDOUT:", e.stdout)
        print("STDERR:", e.stderr)
        return False
    
    return True

def create_npu_config_file(output_dir, max_seq_len, min_response_len):
    """Create a configuration file with NPU-specific settings"""
    
    config_content = f'''# NPU Configuration for Exported Model
# Generated automatically - use with your NPU pipeline

import openvino_genai as ov_genai
import openvino.properties as props
import openvino.properties.hint as hints

# NPU Pipeline-specific configuration (critical for compilation)
pipeline_config = {{
    "MAX_PROMPT_LEN": {max_seq_len},        # Must match export static shape
    "MIN_RESPONSE_LEN": {min_response_len}   # Minimum response tokens
}}

# NPUW (NPU Wrapper) configuration for compilation success
npuw_config = {{
    "NPU_USE_NPUW": "YES",                    # Enable NPU Wrapper
    "NPUW_LLM": "YES",                        # Enable LLM-specific optimizations
    "NPUW_LLM_BATCH_DIM": 0,                  # Batch dimension index
    "NPUW_LLM_SEQ_LEN_DIM": 1,               # Sequence dimension index
    "NPUW_LLM_MAX_PROMPT_LEN": {max_seq_len}, # Must match MAX_PROMPT_LEN
    "NPUW_LLM_MIN_RESPONSE_LEN": {min_response_len}, # Must match MIN_RESPONSE_LEN
}}

# Device configuration for performance
device_config = {{
    hints.performance_mode: hints.PerformanceMode.LATENCY,
    props.cache_dir: ".npu_cache"
}}

def create_pipeline(model_path, device="NPU"):
    """Create optimized NPU pipeline with this exported model"""
    
    # Combine all configurations
    all_config = {{**device_config, **pipeline_config, **npuw_config}}
    
    try:
        print("Loading NPU-optimized model...")
        pipe = ov_genai.LLMPipeline(model_path, device, **all_config)
        print("✓ Successfully loaded model on NPU")
        return pipe
    except Exception as e:
        print(f"❌ Failed to load on NPU: {{e}}")
        print("Trying fallback configurations...")
        
        # Fallback: Try minimal NPUW config
        try:
            minimal_config = {{**device_config, **npuw_config}}
            pipe = ov_genai.LLMPipeline(model_path, device, **minimal_config)
            print("✓ Loaded with minimal NPUW configuration")
            return pipe
        except Exception as e2:
            print(f"❌ All NPU configurations failed: {{e2}}")
            return None

# Usage example:
# pipe = create_pipeline(r"{output_dir}")
# response = pipe.generate("Hello, how are you?")
'''
    
    config_path = Path(output_dir) / "npu_config.py"
    with open(config_path, 'w') as f:
        f.write(config_content)
    
    print(f"📄 Created NPU configuration: {config_path}")

def main():
    parser = argparse.ArgumentParser(description="Export language models for NPU inference")
    parser.add_argument("--model", "-m", 
                       default="microsoft/Phi-3-mini-128k-instruct",
                       help="Hugging Face model name")
    parser.add_argument("--output", "-o", 
                       default="phi3-npu-model",
                       help="Output directory")
    parser.add_argument("--max-seq-len", 
                       type=int, default=2048,
                       help="Maximum sequence length (static shape)")
    parser.add_argument("--min-response", 
                       type=int, default=256,
                       help="Minimum response length for NPU")
    
    args = parser.parse_args()
    
    print("🔧 NPU Model Export Tool")
    print("=" * 50)
    
    # Check requirements
    if not check_requirements():
        sys.exit(1)
    
    # Export the model
    success = export_model_for_npu(
        args.model, 
        args.output,
        args.max_seq_len,
        args.min_response
    )
    
    if success:
        print("\n✅ Export completed successfully!")
        print(f"🚀 Model ready for NPU inference: {args.output}")
        print("\n📖 Next steps:")
        print(f"1. Test with: python -c \"from {args.output}.npu_config import create_pipeline; create_pipeline(r'{args.output}')\"")
        print(f"2. Update your Gradio script to use: {args.output}")
    else:
        print("\n❌ Export failed. Check error messages above.")
        sys.exit(1)

if __name__ == "__main__":
    main()
────────────────────────────────────────────────────────────
END OF FILE: export_model_for_npu.py
────────────────────────────────────────────────────────────


================================================================================
CATEGORY: MODEL CONFIGURATION
================================================================================

DESCRIPTION: Model-specific configuration files and examples

────────────────────────────────────────────────────────────
FILE: config.json
PATH: phi3-128k-npu-fixed\config.json
PURPOSE: Exported Phi-3 model configuration
SIZE: 3,434 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

{
  "architectures": [
    "Phi3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_phi3.Phi3Config",
    "AutoModelForCausalLM": "modeling_phi3.Phi3ForCausalLM"
  },
  "bos_token_id": 1,
  "embd_pdrop": 0.0,
  "eos_token_id": 32000,
  "hidden_act": "silu",
  "hidden_size": 3072,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "model_type": "phi3",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "original_max_position_embeddings": 4096,
  "pad_token_id": 32000,
  "resid_pdrop": 0.0,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "long_factor": [
      1.0700000524520874,
      1.1200000047683716,
      1.149999976158142,
      1.4199999570846558,
      1.5699999332427979,
      1.7999999523162842,
      2.129999876022339,
      2.129999876022339,
      3.009999990463257,
      5.910000324249268,
      6.950000286102295,
      9.070000648498535,
      9.930000305175781,
      10.710000038146973,
      11.130000114440918,
      14.609999656677246,
      15.409998893737793,
      19.809999465942383,
      37.279998779296875,
      38.279998779296875,
      38.599998474121094,
      40.12000274658203,
      46.20000457763672,
      50.940006256103516,
      53.66000747680664,
      54.9373893737793,
      56.89738845825195,
      57.28738784790039,
      59.98738479614258,
      60.86738586425781,
      60.887386322021484,
      61.71739196777344,
      62.91739273071289,
      62.957393646240234,
      63.41739273071289,
      63.8173942565918,
      63.83739471435547,
      63.897396087646484,
      63.93739700317383,
      64.06739807128906,
      64.11434936523438,
      64.12435150146484,
      64.15435028076172,
      64.19435119628906,
      64.24435424804688,
      64.57435607910156,
      64.69000244140625,
      64.76000213623047
    ],
    "short_factor": [
      1.1,
      1.1,
      1.1,
      1.3000000000000003,
      1.3500000000000003,
      1.3500000000000003,
      1.4000000000000004,
      1.5500000000000005,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.000000000000001,
      2.0500000000000007,
      2.0500000000000007,
      2.0500000000000007,
      2.0500000000000007,
      2.0500000000000007,
      2.0500000000000007,
      2.1000000000000005,
      2.1000000000000005,
      2.1500000000000004,
      2.25,
      2.25,
      2.25,
      2.25,
      2.25,
      2.3999999999999995,
      2.4499999999999993,
      2.499999999999999,
      2.6999999999999984,
      2.6999999999999984,
      2.7499999999999982,
      2.799999999999998,
      2.8999999999999977,
      3.049999999999997
    ],
    "type": "longrope"
  },
  "rope_theta": 10000.0,
  "sliding_window": 262144,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.53.3",
  "use_cache": true,
  "vocab_size": 32064
}

────────────────────────────────────────────────────────────
END OF FILE: config.json
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: config.example.json
PATH: _context_archive\config.example.json
PURPOSE: Configuration template with examples
SIZE: 2,712 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

{
  "_comment": "Example configuration for Enhanced Phi-3-mini-128k-instruct OpenVINO GenAI Chat",
  "_usage": "Copy to config.json and customize as needed",
  
  "model": {
    "path": "C:\\OpenVinoModels\\phi3-128k-npu",
    "name": "Phi-3-mini-128k-instruct",
    "type": "phi3",
    "_notes": "Model path should point to OpenVINO model directory with .xml/.bin files"
  },
  
  "deployment": {
    "target_device": "NPU",
    "npu_profile": "balanced",
    "fallback_device": "CPU",
    "cache_directory": "./cache/.ovcache_phi3",
    "_notes": {
      "target_device": "NPU, CPU, GPU, or AUTO",
      "npu_profile": "conservative, balanced, or aggressive",
      "cache_directory": "Directory for OpenVINO compilation cache"
    }
  },
  
  "generation": {
    "max_new_tokens": 2048,
    "temperature": 0.6,
    "top_p": 0.95,
    "top_k": 20,
    "repetition_penalty": 1.1,
    "do_sample": true,
    "_notes": {
      "temperature": "0.0-2.0, higher = more creative",
      "top_p": "0.0-1.0, nucleus sampling threshold",
      "top_k": "1-100, limits vocabulary per step",
      "max_new_tokens": "Maximum response length (1-2048), increased for Phi-3 128k context"
    }
  },
  
  "ui": {
    "max_message_length": 2000,
    "max_conversation_tokens": 8000,
    "emergency_limit": 16384,
    "show_performance_metrics": true,
    "theme": "soft",
    "_notes": {
      "max_message_length": "UI limit for single message, increased for Phi-3 128k context",
      "max_conversation_tokens": "NPU memory optimization, increased for larger context",
      "theme": "gradio theme: default, soft, monochrome"
    }
  },
  
  "performance": {
    "generation_timeout": 30.0,
    "truncation_warning_delay": 0.5,
    "ui_update_interval": 0.1,
    "thread_pool_size": 2,
    "_notes": {
      "generation_timeout": "Seconds before timeout",
      "truncation_warning_delay": "Pause to show warnings",
      "ui_update_interval": "UI refresh rate",
      "thread_pool_size": "Number of threads for processing"
    }
  },
  
  "logging": {
    "log_level": "INFO",
    "enable_performance_logging": true,
    "enable_token_filtering_logs": false,
    "_notes": {
      "log_level": "DEBUG, INFO, WARNING, ERROR",
      "enable_performance_logging": "Log timing and metrics",
      "enable_token_filtering_logs": "Detailed token processing logs"
    }
  },
  
  "security": {
    "validate_input": true,
    "max_file_uploads": 0,
    "allowed_file_types": [],
    "rate_limit_requests": false,
    "_notes": {
      "validate_input": "Enable input validation and sanitization",
      "max_file_uploads": "Maximum simultaneous file uploads",
      "rate_limit_requests": "Enable request rate limiting"
    }
  }
}
────────────────────────────────────────────────────────────
END OF FILE: config.example.json
────────────────────────────────────────────────────────────


================================================================================
CATEGORY: TEST SUITE
================================================================================

DESCRIPTION: Test files and testing configuration

────────────────────────────────────────────────────────────
FILE: test_streaming_format.py
PATH: tests\test_streaming_format.py
PURPOSE: Gradio streaming format compliance tests
SIZE: 5,756 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

#!/usr/bin/env python3
"""
Test Suite for Gradio Streaming Format Compliance
=================================================

Tests to verify that the chat streaming functions return properly formatted
data compatible with Gradio's ChatInterface (type='messages') format.

This addresses the critical requirement that streaming functions must yield
List[Dict[str, str]] objects with the correct message format.

Copyright (c) 2025 sbran
Licensed under the MIT License - see LICENSE file for details
"""

import pytest
from typing import List, Dict, Any, Union
import json


def test_message_format_structure():
    """Test that message format follows required structure."""
    # Example of correct message format
    correct_format = [
        {"role": "user", "content": "Hello"},
        {"role": "assistant", "content": "Hi there!"}
    ]
    
    # Validate structure
    assert isinstance(correct_format, list), "History must be a list"
    
    for message in correct_format:
        assert isinstance(message, dict), "Each message must be a dictionary"
        assert "role" in message, "Each message must have 'role' key"
        assert "content" in message, "Each message must have 'content' key"
        assert isinstance(message["role"], str), "Role must be string"
        assert isinstance(message["content"], str), "Content must be string"
        assert message["role"] in ["user", "assistant", "system"], "Role must be valid"


def test_streaming_output_format_validator():
    """Test validator function for streaming output format."""
    
    def is_valid_gradio_history(history: Any) -> bool:
        """
        Validate that history conforms to Gradio ChatInterface format.
        
        Args:
            history: History object to validate
            
        Returns:
            True if valid, False otherwise
        """
        try:
            # Must be a list
            if not isinstance(history, list):
                return False
            
            # Each item must be a dict with role and content
            for item in history:
                if not isinstance(item, dict):
                    return False
                if "role" not in item or "content" not in item:
                    return False
                if not isinstance(item["role"], str) or not isinstance(item["content"], str):
                    return False
                if item["role"] not in ["user", "assistant", "system"]:
                    return False
                    
            return True
        except Exception:
            return False
    
    # Test valid formats
    valid_cases = [
        [],  # Empty history
        [{"role": "user", "content": "Hello"}],
        [{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi"}],
        [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "Hello"}]
    ]
    
    for case in valid_cases:
        assert is_valid_gradio_history(case), f"Valid case failed: {case}"
    
    # Test invalid formats
    invalid_cases = [
        "not a list",
        [{"role": "user"}],  # Missing content
        [{"content": "Hello"}],  # Missing role
        [{"role": "invalid", "content": "Hello"}],  # Invalid role
        [{"role": 123, "content": "Hello"}],  # Non-string role
        [{"role": "user", "content": 123}],  # Non-string content
        [["user", "Hello"]],  # Wrong structure
        {"role": "user", "content": "Hello"}  # Not a list
    ]
    
    for case in invalid_cases:
        assert not is_valid_gradio_history(case), f"Invalid case passed: {case}"


def test_json_serialization():
    """Test that message format can be JSON serialized."""
    history = [
        {"role": "user", "content": "Test message"},
        {"role": "assistant", "content": "Test response"}
    ]
    
    # Should serialize without error
    json_str = json.dumps(history, indent=2, default=str)
    assert isinstance(json_str, str)
    
    # Should deserialize back to same structure
    deserialized = json.loads(json_str)
    assert deserialized == history


def test_empty_and_edge_cases():
    """Test edge cases for streaming format."""
    # Empty history should be valid
    empty_history = []
    assert isinstance(empty_history, list)
    
    # Single message should be valid
    single_message = [{"role": "user", "content": "Single message"}]
    assert len(single_message) == 1
    assert single_message[0]["role"] == "user"
    
    # Long content should be valid
    long_content = "x" * 10000
    long_message = [{"role": "user", "content": long_content}]
    assert len(long_message[0]["content"]) == 10000
    
    # Empty content should be valid
    empty_content = [{"role": "user", "content": ""}]
    assert empty_content[0]["content"] == ""


def test_conversation_flow():
    """Test typical conversation flow format."""
    conversation = []
    
    # Add user message
    conversation.append({"role": "user", "content": "What is Python?"})
    assert len(conversation) == 1
    
    # Add assistant response
    conversation.append({"role": "assistant", "content": "Python is a programming language."})
    assert len(conversation) == 2
    
    # Add follow-up
    conversation.append({"role": "user", "content": "Tell me more."})
    conversation.append({"role": "assistant", "content": "Python is known for simplicity."})
    assert len(conversation) == 4
    
    # Verify alternating pattern (common but not required)
    for i, msg in enumerate(conversation):
        if i % 2 == 0:
            assert msg["role"] == "user"
        else:
            assert msg["role"] == "assistant"


if __name__ == "__main__":
    # Run tests if executed directly
    pytest.main([__file__, "-v"])
────────────────────────────────────────────────────────────
END OF FILE: test_streaming_format.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: test_chat_format.py
PATH: test_chat_format.py
PURPOSE: Chat format validation tests
SIZE: 1,261 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

#!/usr/bin/env python3
"""
Quick test to verify Gradio chat format compatibility
"""

import gradio as gr

def simple_chat_test(message, history):
    """Simple test function that ensures correct format"""
    print(f"🧪 TEST - Message: {message}")
    print(f"🧪 TEST - History type: {type(history)}")
    print(f"🧪 TEST - History content: {history}")
    
    # Ensure history is correct format
    if not isinstance(history, list):
        history = []
    
    # Add user message and bot response
    new_history = history.copy()
    new_history.append({"role": "user", "content": message})
    new_history.append({"role": "assistant", "content": f"Echo: {message}"})
    
    print(f"🧪 TEST - Returning: {new_history}")
    return new_history

# Create test interface
with gr.Blocks() as demo:
    chatbot = gr.Chatbot(
        type='messages',
        height=400
    )
    msg_input = gr.Textbox(
        placeholder="Test message...",
        show_label=False
    )
    
    msg_input.submit(
        simple_chat_test, 
        [msg_input, chatbot], 
        chatbot
    ).then(
        lambda: gr.update(value=""), 
        None, 
        [msg_input]
    )

if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=7861)
────────────────────────────────────────────────────────────
END OF FILE: test_chat_format.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: test_minimal_gradio.py
PATH: test_minimal_gradio.py
PURPOSE: Minimal Gradio interface tests
SIZE: 1,646 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

#!/usr/bin/env python3
"""
Absolute minimal test to isolate Gradio format issue
"""

import gradio as gr

def minimal_test(message, history):
    """Minimal test with hardcoded correct format"""
    print(f"🧪 Input: message='{message}', history={history}")
    
    # Return the absolute minimal correct format
    result = [
        {"role": "user", "content": "test"},
        {"role": "assistant", "content": "response"}
    ]
    
    print(f"🧪 Returning: {repr(result)}")
    return result

def generator_test(message, history):
    """Test with generator (like our main app)"""
    print(f"🧪 Generator Input: message='{message}', history={history}")
    
    def gen():
        result = [
            {"role": "user", "content": "test"},
            {"role": "assistant", "content": "response"}
        ]
        print(f"🧪 Generator Yielding: {repr(result)}")
        yield result
    
    return gen()

# Test both approaches
with gr.Blocks() as demo:
    with gr.Tab("Direct Return"):
        chatbot1 = gr.Chatbot(type='messages', height=300)
        msg1 = gr.Textbox(placeholder="Test direct return...")
        msg1.submit(minimal_test, [msg1, chatbot1], chatbot1).then(
            lambda: gr.update(value=""), None, [msg1]
        )
    
    with gr.Tab("Generator Return"):
        chatbot2 = gr.Chatbot(type='messages', height=300)
        msg2 = gr.Textbox(placeholder="Test generator return...")
        msg2.submit(generator_test, [msg2, chatbot2], chatbot2).then(
            lambda: gr.update(value=""), None, [msg2]
        )

if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=7862)
────────────────────────────────────────────────────────────
END OF FILE: test_minimal_gradio.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: test_simple_chat.py
PATH: test_simple_chat.py
PURPOSE: Basic chat functionality tests
SIZE: 1,238 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

#!/usr/bin/env python3
"""
Ultra-simple Gradio chat test to isolate the format issue
"""

import gradio as gr

def simple_chat_test(message, history):
    """Minimal streaming test that yields correct format"""
    print(f"Input - message: {message}")
    print(f"Input - history: {history}")
    
    # Add new user message to history
    if not isinstance(history, list):
        history = []
    
    # Create new history with user message and empty assistant response
    new_history = history + [
        {"role": "user", "content": message},
        {"role": "assistant", "content": ""}
    ]
    
    # Stream response word by word
    words = ["This", "is", "a", "test", "response"]
    for word in words:
        new_history[-1]["content"] += word + " "
        print(f"Yielding: {new_history}")
        yield new_history

# Simple interface
with gr.Blocks() as demo:
    gr.Markdown("# Simple Chat Test")
    
    chatbot = gr.Chatbot(type='messages')
    msg = gr.Textbox(placeholder="Type a message...")
    
    msg.submit(simple_chat_test, [msg, chatbot], chatbot).then(
        lambda: gr.update(value=""), None, [msg]
    )

if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=7863, debug=True)
────────────────────────────────────────────────────────────
END OF FILE: test_simple_chat.py
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: pytest.ini
PATH: _context_archive\pytest.ini
PURPOSE: Test configuration and settings
SIZE: 537 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --strict-markers
    --disable-warnings
    --cov=app
    --cov-branch
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
────────────────────────────────────────────────────────────
END OF FILE: pytest.ini
────────────────────────────────────────────────────────────

────────────────────────────────────────────────────────────
FILE: test_config.py
PATH: _context_archive\tests\test_config.py
PURPOSE: Configuration testing patterns
SIZE: 5,975 bytes
STATUS: ✅ Available
────────────────────────────────────────────────────────────

"""
Unit tests for ConfigurationLoader class
========================================

Tests configuration loading and priority system.
"""

import pytest
import tempfile
import json
import os
from unittest.mock import patch, mock_open
from app.config import ConfigurationLoader


class TestConfigurationLoader:
    """Test cases for configuration management"""
    
    def test_default_config_loading(self):
        """Test loading of default configuration values"""
        with patch('os.path.exists', return_value=False):
            config = ConfigurationLoader()
            
            # Test default model configuration
            model_path = config.get("model", "path")
            assert "phi3-128k-npu" in model_path
            assert config.get("model", "type") == "phi3"
            
            # Test default deployment settings
            assert config.get("deployment", "target_device") == "NPU"
            assert config.get("deployment", "npu_profile") == "balanced"
    
    def test_json_config_loading(self):
        """Test loading configuration from JSON file"""
        test_config = {
            "model": {
                "path": "/test/model/path",
                "name": "Test-Model",
                "type": "test"
            },
            "deployment": {
                "target_device": "CPU",
                "npu_profile": "conservative"
            }
        }
        
        config_content = json.dumps(test_config)
        
        with patch('os.path.exists', return_value=True):
            with patch('builtins.open', mock_open(read_data=config_content)):
                config = ConfigurationLoader()
                
                assert config.get("model", "path") == "/test/model/path"
                assert config.get("model", "type") == "test"
                assert config.get("deployment", "target_device") == "CPU"
    
    @patch.dict(os.environ, {
        'MODEL_PATH': '/env/model/path',
        'TARGET_DEVICE': 'GPU',
        'NPU_PROFILE': 'aggressive'
    })
    def test_environment_override(self):
        """Test environment variable override"""
        with patch('os.path.exists', return_value=False):
            config = ConfigurationLoader()
            
            # Environment variables should override defaults
            assert config.get("model", "path") == "/env/model/path"
            assert config.get("deployment", "target_device") == "GPU"
            assert config.get("deployment", "npu_profile") == "aggressive"
    
    @patch.dict(os.environ, {
        'QWEN3_MODEL_PATH': '/legacy/model/path'
    })
    def test_legacy_env_variable_support(self):
        """Test backward compatibility with legacy environment variables"""
        with patch('os.path.exists', return_value=False):
            config = ConfigurationLoader()
            
            # Legacy QWEN3_MODEL_PATH should still work
            assert config.get("model", "path") == "/legacy/model/path"
    
    @patch.dict(os.environ, {
        'MODEL_PATH': '/new/model/path',
        'QWEN3_MODEL_PATH': '/legacy/model/path'
    })
    def test_env_variable_priority(self):
        """Test that new environment variables take precedence over legacy ones"""
        with patch('os.path.exists', return_value=False):
            config = ConfigurationLoader()
            
            # New MODEL_PATH should override legacy QWEN3_MODEL_PATH
            assert config.get("model", "path") == "/new/model/path"
    
    def test_nested_config_access(self):
        """Test nested configuration value access"""
        test_config = {
            "ui": {
                "performance": {
                    "timeout": 45.0
                }
            }
        }
        
        config_content = json.dumps(test_config)
        
        with patch('os.path.exists', return_value=True):
            with patch('builtins.open', mock_open(read_data=config_content)):
                config = ConfigurationLoader()
                
                # Test nested access
                section = config.get_section("ui")
                assert section["performance"]["timeout"] == 45.0
    
    def test_missing_config_fallback(self):
        """Test fallback when configuration keys are missing"""
        with patch('os.path.exists', return_value=False):
            config = ConfigurationLoader()
            
            # Should return default for missing keys
            missing_value = config.get("nonexistent", "key", "default_value")
            assert missing_value == "default_value"
    
    def test_invalid_json_handling(self):
        """Test handling of invalid JSON configuration"""
        invalid_json = "{ invalid json content"
        
        with patch('os.path.exists', return_value=True):
            with patch('builtins.open', mock_open(read_data=invalid_json)):
                # Should fall back to defaults without crashing
                config = ConfigurationLoader()
                
                # Should still have default values
                assert config.get("model", "type") == "phi3"
    
    def test_configuration_priority_order(self):
        """Test the 4-tier configuration priority system"""
        # Setup: JSON config with base values
        json_config = {
            "model": {"path": "/json/path"},
            "deployment": {"target_device": "CPU"}
        }
        
        config_content = json.dumps(json_config)
        
        with patch('os.path.exists', return_value=True):
            with patch('builtins.open', mock_open(read_data=config_content)):
                with patch.dict(os.environ, {'MODEL_PATH': '/env/path'}):
                    config = ConfigurationLoader()
                    
                    # ENV should override JSON
                    assert config.get("model", "path") == "/env/path"
                    # JSON should override defaults
                    assert config.get("deployment", "target_device") == "CPU"
────────────────────────────────────────────────────────────
END OF FILE: test_config.py
────────────────────────────────────────────────────────────


================================================================================
CONTEXT FILE GENERATION SUMMARY
================================================================================

Generated: 2025-08-15 11:01:12
Total Files Processed: 24
Total Content Size: 192,742 bytes (188.2 KB)

FILE TYPE DISTRIBUTION:
  .ini: 1 files
  .json: 3 files
  .md: 2 files
  .py: 16 files
  .txt: 2 files

LARGEST FILES:
  app\chat.py: 31,498 bytes
  app\ui.py: 28,974 bytes
  README.md: 16,635 bytes
  app\agent.py: 16,542 bytes
  app\model.py: 16,280 bytes

================================================================================
USAGE INSTRUCTIONS FOR EXTERNAL LLM
================================================================================

This context file is optimized for external LLM analysis. Key points:

1. ARCHITECTURE UNDERSTANDING:
   - Review the architecture diagram and critical patterns section
   - Understand the 4-tier configuration priority system
   - Note the stateful OpenVINO API usage pattern

2. CRITICAL CONSTRAINTS:
   - NPU context limit: ~8k tokens (hardware constraint)
   - NPUW hints must use FAST_COMPILE/BEST_PERF (not generic hints)
   - Gradio streaming requires List[Dict[str, str]] format

3. DEBUGGING INSIGHTS:
   - Recent fixes section contains solutions to common issues
   - Legacy naming (qwen3) is intentionally preserved for compatibility
   - Device fallback (NPU → CPU) is automatic and critical

4. DEVELOPMENT PATTERNS:
   - Follow modular architecture with clear separation of concerns
   - Use configuration management for all settings
   - Implement comprehensive error handling with user-friendly messages
   - Maintain backward compatibility while guiding toward modern APIs

5. TESTING AND VALIDATION:
   - Use the test suite to validate streaming format compliance
   - Validate NPU compilation before CPU fallback
   - Test both short (NPU) and long (CPU) context scenarios

================================================================================
END OF ENHANCED CONTEXT FILE
================================================================================
