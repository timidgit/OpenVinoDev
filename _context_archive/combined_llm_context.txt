================================================================================
ENHANCED PHI-3 CHAT SYSTEM - COMPLETE PROJECT CONTEXT
================================================================================
Generated: 2025-08-12 15:31:16
Project Root: C:\Users\sbran\OneDrive\Desktop\OpenVinoDev
Total Files: 57
================================================================================

STRATEGIC ROADMAP IMPLEMENTATION STATUS:
✅ Phase 1: Foundational Refactoring & Consolidation
✅ Phase 2: Productionization & Scalability
✅ Phase 3: Advanced AI Capabilities
   ✅ Phase 3.1: Advanced Document Parsing (unstructured)
   ✅ Phase 3.2: Cross-Encoder Reranking (BAAI/bge-reranker-base)
   ✅ Phase 3.3: Agentic Architecture (ReAct + function-calling)

The application has been successfully transformed from prototype to
production-ready AI system with state-of-the-art capabilities.

================================================================================

FILE: .claude\settings.local.json
----------------------------------------
{
  "permissions": {
    "allow": [
      "WebFetch(domain:github.com)",
      "WebFetch(domain:docs.openvino.ai)",
      "Bash(python:*)",
      "Bash(rm:*)",
      "Bash(cp:*)",
      "Bash(find:*)",
      "Bash(tree:*)",
      "Bash(mkdir:*)",
      "Bash(move gradio_qwen_debug.py gradio_qwen_hybrid.py gradio_qwen_optimized.py gradio_qwen_refined.py archive)",
      "Bash(mv:*)",
      "Bash(timeout:*)",
      "Bash(set QWEN3_MODEL_PATH=C:OpenVinoModelsphi3-128k-npu)",
      "Bash(set MODEL_PATH=C:OpenVinoModelsphi3-128k-npu)",
      "Bash(set QWEN3_MODEL_PATH=)",
      "Bash(git add:*)",
      "Bash(dir:*)",
      "Bash(pip install:*)"
    ],
    "deny": []
  }
}

================================================================================

FILE: ACKNOWLEDGMENTS.md
----------------------------------------
# Acknowledgments

This project builds upon and integrates with several outstanding open-source projects and models. We gratefully acknowledge the following contributors to the AI and open-source communities:

## OpenVINO GenAI
- **Source**: Intel Corporation
- **License**: Apache License 2.0
- **Repository**: https://github.com/openvinotoolkit/openvino.genai
- **Website**: https://docs.openvino.ai/
- **Usage**: This project uses configuration patterns, API examples, and best practices derived from OpenVINO GenAI official samples and documentation
- **Specific Attribution**: 
  - Pipeline initialization patterns from `samples/python/` directory
  - Stateful chat API usage following `chat_sample.py` patterns
  - Generation configuration examples and parameter handling
  - NPU optimization techniques and NPUW configuration patterns

## Qwen Models
- **Source**: Alibaba Cloud / Qwen Team
- **License**: Apache License 2.0 (with additional terms for model usage)
- **Repository**: https://github.com/QwenLM/Qwen
- **Website**: https://qwenlm.github.io/
- **Usage**: This project is designed to work with Qwen3-8B models and incorporates Qwen-specific optimizations
- **Specific Attribution**:
  - Special token definitions and handling for Qwen3 architecture
  - Chat template formats following Qwen3 specifications
  - Model-specific configuration parameters and constraints
  - Token filtering patterns for Qwen3's 26+ special tokens
- **Note**: Users must obtain Qwen3 models through official channels and comply with model license terms

## Gradio
- **Source**: HuggingFace Inc.
- **License**: Apache License 2.0
- **Repository**: https://github.com/gradio-app/gradio
- **Website**: https://gradio.app/
- **Usage**: User interface patterns and streaming implementations inspired by official Gradio examples
- **Specific Attribution**:
  - ChatInterface patterns and streaming response handling
  - Component layout and theming approaches
  - Event handling patterns for chat applications
  - Performance dashboard and metrics display techniques

## Transformers Library
- **Source**: HuggingFace Inc.
- **License**: Apache License 2.0
- **Repository**: https://github.com/huggingface/transformers
- **Website**: https://huggingface.co/transformers/
- **Usage**: Tokenizer integration and text processing utilities
- **Specific Attribution**:
  - AutoTokenizer usage patterns and configuration
  - Token handling and decoding techniques
  - Integration patterns with custom model pipelines

## OpenVINO Toolkit
- **Source**: Intel Corporation
- **License**: Apache License 2.0
- **Repository**: https://github.com/openvinotoolkit/openvino
- **Website**: https://openvino.ai/
- **Usage**: Core inference engine and optimization techniques
- **Specific Attribution**:
  - Property configuration patterns and device handling
  - Performance optimization techniques for Intel hardware
  - NPU-specific configuration and deployment patterns

## Python Dependencies
This project also uses several Python libraries, each with their respective licenses:
- **numpy**: BSD License
- **queue, threading, time**: Python Standard Library (PSF License)
- **dataclasses, typing**: Python Standard Library (PSF License)
- **pathlib, os, sys**: Python Standard Library (PSF License)

## Documentation and Learning Resources
We also acknowledge the broader community resources that influenced the development of this project:
- OpenVINO GenAI documentation and tutorials
- Intel Developer Zone articles on NPU optimization
- HuggingFace model hub documentation
- Gradio community examples and tutorials
- GitHub community discussions and issue threads

## Community Contributions
Special thanks to the open-source AI community for:
- Sharing optimization techniques and best practices
- Providing detailed documentation and examples
- Contributing to forums and discussions that helped solve implementation challenges
- Creating comprehensive guides for AI model deployment

---

## How to Cite

If you use this project in your research or applications, please consider citing the original sources:

### OpenVINO GenAI
```
@software{openvino_genai,
  title={OpenVINO GenAI},
  author={Intel Corporation},
  url={https://github.com/openvinotoolkit/openvino.genai},
  year={2024}
}
```

### Qwen Models
```
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
```

### Gradio
```
@software{gradio,
  title={Gradio: Hassle-free sharing and testing of ML models in the wild},
  author={Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},
  url={https://gradio.app/},
  year={2019}
}
```

---

*This project is a community contribution and is not officially affiliated with Intel, Alibaba Cloud, HuggingFace, or any of the above-mentioned organizations.*

================================================================================

FILE: CLAUDE.md
----------------------------------------
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## **Project Overview**

This is a **production-ready, modular Phi-3-mini-128k-instruct chat application** using OpenVINO GenAI with Intel NPU optimization. The project has evolved from a monolithic implementation to a professional modular architecture with comprehensive features including RAG document processing, dynamic system prompts, and advanced performance monitoring. **Recently migrated from Qwen3-8B to Phi-3 for massive 128k context support**.

## **Architecture Overview**

### **Unified Architecture**
- **`main.py`**: Modular architecture with CLI interface (unified entry point)
- **`archive/gradio_qwen_enhanced.py`**: Archived legacy implementation

**CRITICAL**: All development targets the **modular architecture** (`app/` directory) accessed via `main.py`.

### **Core Module Structure**
```
app/
├── config.py      # Multi-source configuration (CLI → env → JSON → defaults)
├── model.py       # NPU pipeline deployment with comprehensive fallbacks  
├── streamer.py    # Token filtering & performance metrics (legacy Qwen3 naming)
├── chat.py        # Chat processing with RAG integration & Gradio compatibility
└── ui.py          # Professional Gradio interface with advanced features
```

### **Context System**
```
context/
├── qwen3_model_context/     # Legacy optimizations (still used for NPU patterns)
│   ├── npu_optimization.py  # NPU NPUW configuration profiles
│   ├── model_architecture.py # Model-specific settings
│   └── special_tokens.py    # Token filtering definitions
├── core_cpp/               # OpenVINO GenAI C++ insights
├── python_samples/         # Reference implementations
└── gradio_patterns/        # UI component patterns
```

## **Common Commands**

### **Primary Application Launch**
```bash
# Modular entry point (unified architecture)
python main.py
```

### **CLI Configuration & Testing**
```bash
# System validation
python main.py --validate-only

# Device-specific testing
python main.py --device CPU
python main.py --device NPU --npu-profile conservative

# Debug mode with verbose logging  
python main.py --debug

# Custom model path (useful for model switching)
python main.py --model-path "C:\OpenVinoModels\phi3-128k-npu"

# Full CLI help
python main.py --help
```

### **Development & Utilities**
```bash
# Export NPU-compatible model
python export_model_for_npu.py --model microsoft/Phi-3-mini-128k-instruct --output phi3-128k-npu

# Analyze model compatibility  
python check_model_config.py

# Create consolidated context
create_llm_context.bat
```

### **Installation**
```bash
# Install dependencies
pip install -r requirements.txt

# Key dependencies
pip install openvino-genai>=2024.4.0 gradio>=4.0.0 transformers>=4.30.0

# RAG dependencies (optional)
pip install langchain faiss-cpu sentence-transformers
```

## **Current Model Configuration**

### **Phi-3-mini-128k-instruct (Current)**
The application now uses **microsoft/Phi-3-mini-128k-instruct** with massive context improvements:

```json
{
  "model": {
    "path": "C:\\OpenVinoModels\\phi3-128k-npu",
    "name": "Phi-3-mini-128k-instruct", 
    "type": "phi3"
  },
  "ui": {
    "max_message_length": 2000,      # Was 400 (5x increase)
    "max_conversation_tokens": 8000, # Was 1800 (4.4x increase)
    "emergency_limit": 16384         # Was 2048 (8x increase)
  }
}
```

### **Environment Variable Configuration**
```bash
# Primary (recommended)
set MODEL_PATH=C:\OpenVinoModels\phi3-128k-npu

# Backward compatibility  
set QWEN3_MODEL_PATH=C:\OpenVinoModels\phi3-128k-npu

# Device and profile
set TARGET_DEVICE=NPU
set NPU_PROFILE=balanced
```

## **Critical NPU Configuration Knowledge**

### **NPUW Configuration (ESSENTIAL)**
NPU requires specific NPUW hint values. **Current configuration uses supported values**:

```python
# CORRECT (Currently in codebase after fixes):
"NPUW_LLM_PREFILL_HINT": "LATENCY",
"NPUW_LLM_GENERATE_HINT": "LATENCY",
"NPUW_LLM_MAX_PROMPT_LEN": 8192,  # Increased for Phi-3 128k context

# INCORRECT (Causes compilation errors):
"NPUW_LLM_PREFILL_HINT": "FAST_COMPILE",  # Not supported by current drivers
"NPUW_LLM_GENERATE_HINT": "BEST_PERF"     # Not supported

# CRITICAL: Do NOT use generic PERFORMANCE_HINT with NPUW hints - they conflict
```

### **Configuration Priority System**
The modular architecture implements **4-tier configuration priority**:
1. **CLI Arguments** (highest): `--device CPU --model-path path/to/model`
2. **Environment Variables**: `MODEL_PATH=/path/to/model`
3. **JSON Configuration**: `config.json` settings
4. **Built-in Defaults** (lowest): Hardcoded fallbacks

### **NPU Deployment Strategy**
`deploy_qwen3_pipeline()` (function name retained for compatibility) implements multi-tier fallback:
1. **Enhanced Context**: Uses `context/qwen3_model_context/` for NPU optimization patterns
2. **Manual Configuration**: Fallback NPUW settings optimized for Phi-3
3. **Basic Configuration**: Minimal OpenVINO properties  
4. **CPU Fallback**: Automatic device switching

## **Gradio ChatInterface Compatibility (CRITICAL)**

### **Data Format Requirements**
The chat system uses **Gradio ChatInterface** which requires specific data formats:

```python
# CORRECT format for Gradio ChatInterface:
ChatHistory = List[List[str]]  # [["user_msg", "bot_response"], ...]

# Functions return/yield this format:
history = [["Hello", "Hi there!"], ["How are you?", "I'm doing well!"]]

# WRONG format (causes "Data incompatible with messages format" errors):
history = [{"role": "user", "content": "Hello"}, {"role": "assistant", "content": "Hi!"}]
```

### **Streaming Implementation**
```python
# Correct streaming pattern for Gradio:
def stream_response_to_history(streamer, history):
    for chunk in streamer:
        if chunk:
            history[-1][1] += chunk  # Update bot response part (index [1])
            yield history
```

## **Stateful API Usage (ESSENTIAL)**

OpenVINO GenAI pipelines are **stateful** - they manage conversation history internally:

```python
# CORRECT Pattern:
pipe.start_chat(SYSTEM_PROMPT)        # Initialize session
pipe.generate(new_message, config)    # Send only new message  
pipe.finish_chat()                    # Clear session

# WRONG Pattern (causes token limit errors):
full_conversation = build_history(history + [new_message])
prompt = tokenizer.apply_chat_template(full_conversation)
pipe.generate(prompt)  # Re-processes entire conversation each time
```

## **Advanced Features Integration**

### **RAG Document Processing**
- **Dependencies**: `langchain`, `faiss-cpu`, `sentence-transformers`
- **Supported formats**: `.txt`, `.md`, `.py`, `.js`, `.html`, `.css`, `.json`
- **Security**: File type validation, size limits, content sanitization
- **Integration**: Automatic context retrieval and prompt augmentation

### **Dynamic System Prompts**
- **Real-time customization** through Gradio interface
- **Session management**: `pipe.start_chat(SYSTEM_PROMPT)`
- **Persistence**: Maintained across conversation until explicitly changed

### **Performance Monitoring**
- **Real-time metrics**: Token generation rates, response times, device utilization
- **NPU-specific diagnostics**: Compilation status, memory usage, fallback triggers
- **Streaming metrics**: Token filtering statistics, special token handling

## **Development Guidelines**

### **Model Migration Patterns**
When switching models:
1. **Update config.json**: Model path, cache directory, token limits
2. **Update app/model.py**: NPU configuration limits, cache paths, print statements
3. **Update app/ui.py**: UI labels, system prompts, performance targets
4. **Update main.py**: Application descriptions and startup messages
5. **Test thoroughly**: Validate system requirements and full startup

### **NPU Development Constraints**
- **Token Limits**: NPU has hard-coded prompt length limits (now 8192 for Phi-3)
- **Defensive Programming**: Always validate input length before NPU processing
- **Configuration Validation**: Ensure NPUW settings use supported values
- **Conflict Avoidance**: Do not use generic PERFORMANCE_HINT with NPUW-specific hints

### **Testing Strategy**
```bash
# Validate system requirements
python main.py --validate-only

# Test NPU functionality with debug output
python main.py --device NPU --debug

# Test CPU fallback behavior  
python main.py --device CPU --debug

# Test model path override
python main.py --model-path "C:\OpenVinoModels\phi3-128k-npu" --debug
```

## **Legacy Context System**

**NOTE**: The `context/qwen3_model_context/` directory retains its Qwen3 naming but contains NPU optimization patterns that work effectively with Phi-3. The enhanced context system:
- Provides NPU NPUW configuration profiles with correct hint values
- Contains C++ reference implementations
- Includes Gradio integration patterns
- Should be preserved even when migrating to other models

## **Quality Gates for NPU Features**

A feature is "production ready" when:
- ✅ NPU compilation succeeds with target configuration
- ✅ CPU fallback works when NPU fails
- ✅ User receives clear feedback about active device
- ✅ Performance metrics are reasonable (>10 tokens/sec for NPU)
- ✅ Memory usage stays within NPU constraints
- ✅ Conversations leverage full context without crashes
- ✅ Gradio data format compatibility maintained

## **Critical Implementation Patterns**

### **Configuration Composition**
- Use dependency injection, not global variables
- `ConfigurationLoader` manages all settings with priority system
- Environment-agnostic code for deployment flexibility
- Every feature has CPU/basic fallback option

### **Error Handling Strategy**
- Fail fast with descriptive messages
- Specific try/except blocks with clear fallback paths
- User-friendly error messages without technical details
- Device fallback triggers with transparent communication

### **Performance Optimization**
- Token-level streaming with model-agnostic filtering
- Real-time metrics integration throughout pipeline
- Model compilation caching for faster subsequent loads
- Proper resource cleanup and session management
- Leverage Phi-3's 128k context for complex conversations

================================================================================

FILE: DISCLAIMER.md
----------------------------------------
# Disclaimer and Usage Guidelines

## ⚠️ Important Notice

This software is provided **"AS IS"** without warranty of any kind. This is a **hobby/educational project** created to demonstrate OpenVINO GenAI capabilities and is not intended for commercial use.

## 🔒 User Responsibilities

### Model Acquisition and Licensing
Users are fully responsible for:
1. **Obtaining Qwen3-8B models** through official and authorized channels:
   - HuggingFace: https://huggingface.co/Qwen
   - ModelScope: https://modelscope.cn/organization/qwen
   - Official Qwen repositories
2. **Reading and complying** with all model license terms and conditions
3. **Following attribution requirements** as specified by model providers
4. **Ensuring proper usage** according to model-specific guidelines

### Hardware and Software Requirements
Users must ensure:
1. **Compatible hardware** (Intel NPU-enabled systems or compatible CPUs)
2. **Proper driver installation** for NPU acceleration
3. **OpenVINO environment setup** with all required dependencies
4. **Sufficient system resources** (memory, storage, compute)

### Legal Compliance
Users are responsible for:
1. **Compliance with all applicable licenses** (see ACKNOWLEDGMENTS.md)
2. **Following local laws and regulations** regarding AI model usage
3. **Respecting intellectual property rights** of all third-party components
4. **Obtaining necessary permissions** for specific use cases

## 🚫 Limitations and Exclusions

### No Warranties
The authors make **NO WARRANTIES** regarding:
- **Functionality**: Software may not work as expected
- **Performance**: No guarantees about speed, accuracy, or reliability  
- **Compatibility**: May not work with all hardware/software configurations
- **Security**: Users responsible for security considerations
- **Data Safety**: No guarantees regarding data handling or privacy

### Liability Limitations
The authors are **NOT LIABLE** for:
- **Direct damages** from software use or malfunction
- **Indirect or consequential damages** including but not limited to:
  - Data loss or corruption
  - System failures or crashes
  - Business interruption or loss of profits
  - Legal issues arising from improper usage
- **Third-party component issues** or license violations
- **Model-generated content** or its appropriateness

### Usage Restrictions
This software should **NOT** be used for:
- **Critical systems** where failure could cause harm
- **Production environments** without thorough testing
- **Commercial purposes** without proper licensing compliance
- **Illegal activities** or violation of terms of service
- **Generating harmful content** or violating platform policies

## 🏢 Organizational Disclaimers

### No Affiliation
This project and its author(s) are **NOT affiliated** with:
- **Intel Corporation** (OpenVINO, NPU technologies)
- **Alibaba Cloud / Qwen Team** (Qwen models)
- **HuggingFace Inc.** (Gradio, Transformers)
- Any other mentioned organizations or projects

### No Endorsement
This project does **NOT** constitute:
- **Official endorsement** by any of the above organizations
- **Official support** or maintenance guarantees
- **Recommended usage patterns** by component creators
- **Best practices** as defined by original authors

## 🔧 Technical Disclaimers

### Experimental Nature
This software:
- Is **experimental** and may contain bugs or errors
- Uses **complex AI/ML technologies** that are rapidly evolving
- Integrates **multiple third-party components** with their own limitations
- May produce **unexpected results** or behaviors

### Performance Variability
Performance may vary significantly based on:
- **Hardware configuration** (NPU availability, CPU capabilities)
- **Model availability** and format compatibility
- **System load** and resource availability
- **Network conditions** (for model downloads)
- **Environmental factors** (temperature, power management)

### Security Considerations
Users should be aware that:
- **AI models** may generate unpredictable content
- **Network communication** may involve data transmission
- **Local model storage** requires appropriate security measures
- **User inputs** are processed by the AI model
- **Generated outputs** should be reviewed before use

## 📚 Educational Purpose

### Learning and Research
This project is intended for:
- **Educational purposes** - learning OpenVINO GenAI integration
- **Research and experimentation** - exploring NPU acceleration
- **Community contribution** - sharing implementation patterns
- **Technical demonstration** - showcasing optimization techniques

### Not for Production
This project is **NOT** intended for:
- **Production deployments** without substantial testing and hardening
- **Commercial applications** without proper licensing and compliance
- **Mission-critical systems** where reliability is paramount
- **Large-scale deployments** without proper architecture review

## 🔄 Updates and Changes

### No Maintenance Guarantee
The authors provide **NO GUARANTEE** of:
- **Ongoing maintenance** or updates
- **Bug fixes** or security patches
- **Feature additions** or improvements
- **Compatibility** with future versions of dependencies

### Community-Driven
This project relies on:
- **Community contributions** for improvements
- **User feedback** for issue identification
- **Volunteer effort** for maintenance
- **Best-effort support** through community channels

## 📞 Support and Contact

### No Official Support
Please understand that:
- **No official support** is provided for this software
- **Community forums** and issue trackers are best-effort only
- **Response times** may vary significantly
- **Support quality** depends on community volunteer availability

### Community Guidelines
When seeking help:
1. **Read documentation** thoroughly first
2. **Search existing issues** before creating new ones
3. **Provide complete information** about your setup and problem
4. **Be respectful** of volunteer contributors' time
5. **Contribute back** when possible to help others

## ⚖️ Legal Information

### Jurisdiction
This disclaimer is governed by applicable laws in the jurisdiction where the software is used. Users are responsible for ensuring compliance with local laws.

### Severability
If any portion of this disclaimer is found to be unenforceable, the remainder shall remain in full force and effect.

### Entire Agreement
This disclaimer, together with the LICENSE and ACKNOWLEDGMENTS files, constitutes the entire agreement regarding the use of this software.

---

## 📝 Acceptance

**By using this software, you acknowledge that you have read, understood, and agree to be bound by all terms in this disclaimer, the LICENSE file, and all third-party license terms referenced in ACKNOWLEDGMENTS.md.**

**If you do not agree with any of these terms, you should not use this software.**

---

*Last updated: January 2025*

*For the most current version of this disclaimer, please refer to the repository's main branch.*

================================================================================

FILE: Dockerfile
----------------------------------------
# Enhanced OpenVINO GenAI Chat Application
# Production-ready Docker container with NPU support
# Based on strategic roadmap Phase 2.1

FROM openvino/ubuntu22_dev:latest

# Set working directory
WORKDIR /app

# Install system dependencies for NPU and Python packages
RUN apt-get update && apt-get install -y \
    python3-pip \
    python3-venv \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set up Python environment
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# Copy requirements first for better Docker layer caching
COPY requirements.txt .

# Install Python dependencies
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy application source code
COPY app/ ./app/
COPY context/ ./context/
COPY tests/ ./tests/
COPY main.py .
COPY config.json .
COPY config.example.json .
COPY *.md ./
COPY *.ini .
COPY .flake8 .

# Create cache directory
RUN mkdir -p /app/cache

# Expose Gradio port
EXPOSE 7860

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:7860/healthcheck || exit 1

# Configure Gradio to listen on all interfaces
ENV GRADIO_SERVER_NAME=0.0.0.0
ENV GRADIO_SERVER_PORT=7860

# Default command with production-ready settings
CMD ["python3", "main.py", "--device", "AUTO", "--debug"]

# Metadata
LABEL maintainer="sbran"
LABEL description="Enhanced Phi-3-mini-128k-instruct Chat Application with OpenVINO GenAI"
LABEL version="2.0.0"

================================================================================

FILE: ENHANCEMENT_GUIDE.md
----------------------------------------
# Enhanced Qwen3 OpenVINO GenAI - New Features

## 🎯 Dynamic System Prompt Configuration

The application now includes an interactive system prompt editor that allows you to:

- **Modify AI behavior in real-time**: Change the assistant's persona, expertise, and response style
- **Reset to defaults**: Quickly restore the original optimized prompt
- **Apply changes instantly**: Changes take effect immediately when you clear the chat

### How to Use:
1. Expand the "🎯 System Prompt Configuration" accordion
2. Edit the system prompt in the text area
3. Click "✅ Apply & Clear Chat" to activate your changes
4. Use "🔄 Reset to Default" to restore original settings

## 📚 Retrieval-Augmented Generation (RAG)

Upload your own documents to provide the AI with specific context for more accurate, grounded responses.

### Supported File Types:
- Text files (`.txt`)
- Markdown (`.md`) 
- Code files (`.py`, `.js`, `.html`, `.css`, `.json`)

### How to Use:
1. Expand the "📚 Document Upload (RAG)" accordion
2. Upload one or more text-based files
3. Wait for processing confirmation
4. Ask questions about your uploaded content
5. The AI will reference your documents when relevant

### RAG Features:
- **Automatic context retrieval**: Finds relevant sections from your documents
- **Source attribution**: Shows which document information came from
- **Multiple document support**: Upload and query multiple files simultaneously
- **Smart chunking**: Optimally splits documents for better retrieval
- **Fallback behavior**: Works normally if no relevant context is found

## 📊 Enhanced Interface

### New Components:
- **System Prompt Editor**: Configure AI behavior
- **File Upload Panel**: Add documents for RAG
- **Upload Status Display**: Monitor document processing
- **RAG Status Button**: Check document processing statistics
- **Clear Documents**: Remove all uploaded content

### Professional Improvements:
- **Organized accordions**: Collapsible sections for clean interface
- **Contextual help**: Tooltips and guidance text
- **Error handling**: Graceful fallbacks when dependencies are missing
- **Status feedback**: Real-time updates on system operations

## 🔧 Installation Requirements

### Core Dependencies (Already Included):
```bash
pip install openvino-genai>=2024.4.0 gradio>=4.0.0 transformers>=4.30.0
```

### RAG Dependencies (New):
```bash
pip install langchain faiss-cpu sentence-transformers
```

If RAG dependencies aren't installed, the system will work normally but without document processing capabilities.

## 🚀 Usage Examples

### System Prompt Customization:
```
You are a specialized Python coding assistant with expertise in machine learning and data science. 

Key behaviors:
- Provide complete, working code examples
- Explain complex algorithms step-by-step
- Focus on best practices and optimization
- Include relevant imports and dependencies
```

### RAG-Enhanced Queries:
After uploading technical documentation:
- "What does the uploaded document say about model optimization?"
- "Summarize the key configuration options from the files"
- "How should I implement the feature described in the documentation?"

## 🏗️ Project Structure Changes

### New Archive Structure:
```
OpenVinoDev/
├── gradio_qwen_enhanced.py  # ← Main application (enhanced)
├── archive/                 # ← Legacy files moved here
│   ├── gradio_qwen_debug.py
│   ├── gradio_qwen_hybrid.py
│   ├── gradio_qwen_optimized.py
│   └── gradio_qwen_refined.py
└── requirements.txt         # ← Updated with RAG dependencies
```

## 🔒 Security & Performance

### Security Features:
- **Input validation**: Sanitizes uploaded files and user input
- **File type restrictions**: Only allows safe text-based formats
- **Memory management**: Efficient document chunking and storage
- **Graceful fallbacks**: System works even if RAG fails

### Performance Optimizations:
- **Lightweight embeddings**: Fast sentence-transformers model
- **Smart chunking**: Optimized for retrieval accuracy
- **Context caching**: FAISS vector store for fast similarity search
- **NPU compatibility**: RAG processing runs on CPU while inference uses NPU

## 🎊 Ready to Use

Your enhanced system is now ready with:
✅ Legacy files archived  
✅ Dynamic system prompts  
✅ Document-aware conversations  
✅ Professional interface  
✅ Enhanced error handling  
✅ Improved user experience

Run `python gradio_qwen_enhanced.py` to start the enhanced application!

================================================================================

FILE: PUBLIC_REPO_CHECKLIST.md
----------------------------------------
# Public Repository Preparation Checklist ✅

This document tracks the changes made to prepare the repository for public release.

## ✅ Completed Tasks

### 📝 **Code Review Improvements (Based on Community Feedback)**
- [x] **Enhanced error handling** with comprehensive system validation
- [x] **Refactored core chat logic** into modular, testable functions
- [x] **Added comprehensive type hints** and professional docstrings
- [x] **Implemented configuration file system** (config.json) with environment overrides
- [x] **Added security improvements** with input validation and sanitization
- [x] **Professional code organization** with clear separation of concerns

## ✅ Completed Tasks

### 📜 Legal and Licensing
- [x] **Created MIT LICENSE** - Standard open-source license for the original code
- [x] **Created comprehensive ACKNOWLEDGMENTS.md** - Detailed third-party attributions
- [x] **Created DISCLAIMER.md** - Usage guidelines and liability limitations
- [x] **Added copyright headers** to main source files
- [x] **Updated CLAUDE.md** with public repository guidelines

### 📚 Documentation
- [x] **Created comprehensive README.md** with:
  - Features overview and installation guide
  - Usage examples and configuration options
  - Legal section with license compliance
  - Performance targets and troubleshooting
  - Community contribution guidelines
- [x] **Created SETUP.md** - Quick start guide for new users
- [x] **Created requirements.txt** - Python dependencies with notes

### 🔧 Code Sanitization
- [x] **Updated gradio_qwen_enhanced.py**:
  - Added professional copyright header
  - Replaced hardcoded paths with environment variables
  - Added third-party attribution comments
- [x] **Updated context files** with copyright headers
- [x] **Created .gitignore** to prevent sensitive data commits

### 🛡️ Security and Privacy
- [x] **Removed hardcoded local paths**:
  - `MODEL_PATH` now uses `QWEN3_MODEL_PATH` env var
  - `CACHE_DIR` now uses `CACHE_DIR` env var  
  - `DEVICE` now uses `TARGET_DEVICE` env var
- [x] **Added .gitignore rules** for:
  - Model files and caches
  - Personal configuration
  - Logs and temporary files
  - Sensitive information patterns

## 📝 File Structure After Changes

```
OpenVinoDev/
├── LICENSE                         # MIT License
├── README.md                      # Main documentation  
├── SETUP.md                       # Quick setup guide
├── DISCLAIMER.md                  # Usage disclaimers
├── ACKNOWLEDGMENTS.md             # Third-party attributions
├── requirements.txt               # Python dependencies
├── PUBLIC_REPO_CHECKLIST.md      # This checklist
├── .gitignore                     # Git ignore rules
├── CLAUDE.md                      # Updated with public guidelines
├── gradio_qwen_enhanced.py        # Updated with copyright & env vars
├── context/                       # Enhanced context system
│   ├── README.md                  
│   ├── qwen3_model_context/       # Updated with copyright
│   │   ├── npu_optimization.py
│   │   ├── special_tokens.py
│   │   ├── model_architecture.py
│   │   └── README.md
│   ├── gradio_patterns/           # Gradio integration patterns
│   ├── gradio_testing/            # Testing utilities
│   ├── python_samples/            # OpenVINO samples
│   └── [other context directories]
└── [legacy files]                 # Previous versions (unchanged)
```

## 🔍 Key Changes Made

### 1. Environment Variables (Security)
```python
# BEFORE (hardcoded)
MODEL_PATH = r"C:\OpenVinoModels\qwen3-8b-int4-cw-ov"
CACHE_DIR = r"C:\temp\.ovcache_qwen3_enhanced"

# AFTER (environment-based)
MODEL_PATH = os.getenv("QWEN3_MODEL_PATH", "./models/qwen3-8b-int4-cw-ov")
CACHE_DIR = os.getenv("CACHE_DIR", "./cache/.ovcache_qwen3_enhanced")
```

### 2. Copyright Attribution
All major files now include:
```python
"""
Copyright (c) 2025 sbran
Licensed under the MIT License - see LICENSE file for details

This application integrates with third-party components:
- OpenVINO GenAI (Apache 2.0, Intel Corporation)
- Qwen3 models (Apache 2.0 with additional terms, Alibaba Cloud)
[...]
"""
```

### 3. Professional Documentation
- Clear installation and setup instructions
- Comprehensive legal compliance guidance
- Detailed troubleshooting and performance information
- Community contribution guidelines

## ⚖️ Legal Compliance Status

### ✅ Compliant Components
- **Original Architecture**: MIT Licensed
- **Configuration Management**: MIT Licensed  
- **UI Design**: MIT Licensed
- **Integration Logic**: MIT Licensed

### ✅ Properly Attributed Third-Party
- **OpenVINO GenAI**: Apache 2.0 - Properly attributed
- **Qwen3 Models**: Apache 2.0 + terms - User responsibility
- **Gradio**: Apache 2.0 - Properly attributed
- **Transformers**: Apache 2.0 - Properly attributed

### ✅ Risk Mitigation
- **No hardcoded proprietary paths**
- **Clear third-party attributions**  
- **User responsibility for model licensing**
- **Comprehensive disclaimers**

## 🚀 Ready for Public Release

The repository is now prepared for public release with:

1. **✅ Legal Compliance**: All licenses properly attributed and respected
2. **✅ Security**: No sensitive information or hardcoded paths
3. **✅ Documentation**: Professional documentation for community use
4. **✅ User Experience**: Clear setup and usage instructions
5. **✅ Community Ready**: Contributing guidelines and professional presentation

## 🎯 Next Steps for Repository Owner

1. **Review all changes** to ensure they meet your requirements
2. **Test the setup process** using only environment variables
3. **Create GitHub repository** and push the code
4. **Verify .gitignore** is working properly
5. **Consider adding GitHub Actions** for automated testing (optional)
6. **Add repository URL** to README.md clone instructions

## 📞 User Support Strategy

The repository is designed for **community self-service**:
- **Comprehensive documentation** reduces support burden
- **Clear error messages** help users troubleshoot independently
- **Professional disclaimers** set appropriate expectations
- **Issue template suggestions** (optional) can help organize community support

---

**Repository Status: ✅ READY FOR PUBLIC RELEASE**

All copyright, licensing, and security requirements have been addressed for a hobby/educational project with third-party integrations.

================================================================================

FILE: README.md
----------------------------------------
# Enhanced Qwen3 OpenVINO GenAI Chat Application

A production-ready, modular implementation of Qwen3-8B chat interface using OpenVINO GenAI with Intel NPU optimization, RAG document processing, and comprehensive performance monitoring. Built with professional software engineering practices and a modular architecture for maximum maintainability and scalability.

![License](https://img.shields.io/badge/License-MIT-blue.svg)
![Python](https://img.shields.io/badge/Python-3.8+-green.svg)
![OpenVINO](https://img.shields.io/badge/OpenVINO-2024.4+-orange.svg)

## ✨ Features

### 🚀 **Performance Optimization**
- **Complete Intel NPU optimization** with NPUW (NPU Wrapper) configuration
- **Qwen3-specific tuning** with model architecture awareness  
- **Intelligent device fallback** (NPU → CPU) with appropriate configurations
- **Professional performance monitoring** with real-time metrics
- **Fixed NPUW configuration issues** for stable NPU compilation

### 🎯 **Advanced Capabilities**
- **🎯 Dynamic System Prompts**: Real-time AI behavior customization
- **📚 RAG Document Processing**: Upload and query your own documents
- **🔍 26+ Special Token Filtering** for clean Qwen3 output
- **📊 Real-time Performance Monitoring** with comprehensive metrics
- **⚙️ Multi-source Configuration** (CLI, env vars, JSON)

### 🖥️ **Professional User Experience**
- **Modern Gradio interface** with professional theming and accordions
- **Real-time streaming responses** with token-level filtering
- **Smart message processing** with intelligent truncation
- **Document upload and processing** for context-aware conversations
- **Security-focused input validation** and sanitization

### 🏗️ **Enterprise-Ready Architecture**
- **Modular codebase** with clear separation of concerns
- **Command-line interface** with comprehensive argument support
- **Configuration management** with environment override support
- **Professional error handling** with detailed diagnostics
- **Comprehensive logging** and debugging capabilities

## 🛠️ Installation

### Prerequisites
- **Python 3.8+**
- **OpenVINO 2024.4+** with GenAI support
- **Intel NPU drivers** (for NPU acceleration)
- **Qwen3-8B INT4 model** in OpenVINO format

### Quick Setup

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd OpenVinoDev
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up OpenVINO environment**
```bash
# Source OpenVINO environment variables
source /opt/intel/openvino/setupvars.sh  # Linux
# or
"C:\Program Files (x86)\Intel\openvino\setupvars.bat"  # Windows
```

4. **Configure the application**

**Option A: Environment Variables**
```bash
export QWEN3_MODEL_PATH="/path/to/your/qwen3-8b-int4-ov"
export TARGET_DEVICE="NPU"  # or "CPU"
export NPU_PROFILE="balanced"  # conservative, balanced, aggressive
```

**Option B: Configuration File (Recommended)**
```bash
# Copy example configuration
cp config.example.json config.json

# Edit config.json with your settings
nano config.json
```

**Option C: Command-Line Arguments**
```bash
# Use CLI arguments for quick configuration changes
python main.py --device CPU --npu-profile conservative
```

5. **Run the application**
```bash
# Modular entry point (unified architecture)
python main.py
```

## 📋 Requirements

### Hardware
- **Recommended**: Intel NPU-enabled system (Meteor Lake, Arrow Lake, or later)
- **Alternative**: Intel CPU with AVX-512 support
- **Memory**: 8GB+ RAM for optimal performance

### Software
**Core Dependencies:**
```
openvino-genai>=2024.4
gradio>=4.0.0
transformers>=4.30.0
numpy>=1.21.0
typing-extensions>=4.0.0
```

**RAG Dependencies (Optional):**
```
langchain>=0.1.0
faiss-cpu>=1.7.4
sentence-transformers>=2.2.0
```

See `requirements.txt` for complete dependencies.

## 🚀 Usage

### Basic Usage
```bash
# Start with default configuration
python main.py

# Use specific device
python main.py --device CPU

# Custom model path and profile
python main.py --model-path ./models/qwen3 --npu-profile balanced

# Enable public sharing (use with caution)
python main.py --share

# Show all available options
python main.py --help
```

The application will:
1. **Auto-detect** available hardware (NPU/CPU)
2. **Deploy** Qwen3 model with optimal configuration
3. **Launch** web interface at `http://127.0.0.1:7860`

### Configuration Options

**Environment Variables:**
```bash
export QWEN3_MODEL_PATH="/path/to/model"      # Model location
export CACHE_DIR="/path/to/cache"             # Cache directory
export NPU_PROFILE="balanced"                 # balanced|conservative|aggressive
export MAX_MESSAGE_LENGTH="400"               # Max input length
```

**NPU Profiles:**
- **Conservative**: Lower memory usage, smaller contexts
- **Balanced**: Optimal for most use cases (default)
- **Aggressive**: Maximum performance, higher memory usage

## 🏗️ Architecture

### Core Components

```
OpenVinoDev/
├── app/                        # Modular application architecture
│   ├── __init__.py            # Package initialization & exports
│   ├── config.py              # Configuration management
│   ├── model.py               # Model deployment & system initialization
│   ├── streamer.py            # Token streaming & filtering
│   ├── chat.py                # Core chat processing & RAG
│   └── ui.py                  # Gradio interface & event handling
├── context/                   # Enhanced context system
│   ├── qwen3_model_context/   # Qwen3-specific optimizations
│   ├── gradio_patterns/       # UI patterns
│   └── gradio_testing/        # Testing utilities
├── archive/                   # Legacy implementations
│   ├── gradio_qwen_refined.py # Earlier implementation
│   └── gradio_qwen_debug.py   # Debug version
├── main.py                    # Application entry point with CLI
├── archive/                   # Legacy implementations
├── config.json               # Configuration file
└── requirements.txt          # Dependencies
```

### Key Components

**Configuration Management (`app/config.py`)**
- **`ConfigurationLoader`**: Multi-source configuration (JSON, env vars, CLI)
- **JSON/Environment/CLI priority system** for flexible configuration

**Model Management (`app/model.py`)**
- **`deploy_qwen3_pipeline()`**: NPU-optimized model deployment
- **`initialize_system_with_validation()`**: Comprehensive system initialization
- **`Qwen3ConfigurationManager`**: Device and profile-specific configurations

**Streaming & Performance (`app/streamer.py`)**
- **`EnhancedQwen3Streamer`**: Real-time response streaming with token filtering
- **`StreamingMetrics`**: Performance monitoring and diagnostics

**Chat Processing (`app/chat.py`)**
- **`enhanced_qwen3_chat()`**: Main chat processing with RAG integration
- **`DocumentRAGSystem`**: Document upload and context retrieval
- **`InputValidator`**: Security-focused input validation

**User Interface (`app/ui.py`)**
- **`create_enhanced_interface()`**: Professional Gradio interface
- **Dynamic system prompts, RAG uploads, performance monitoring**

## 🔧 Advanced Configuration

### NPU Optimization

The application includes comprehensive NPUW configuration for Intel NPU:

```python
npu_config = {
    "NPU_USE_NPUW": "YES",
    "NPUW_LLM": "YES", 
    "NPUW_LLM_MAX_PROMPT_LEN": 2048,
    "NPUW_LLM_MIN_RESPONSE_LEN": 256,
    "NPUW_LLM_PREFILL_HINT": "LATENCY",
    "NPUW_LLM_GENERATE_HINT": "LATENCY"
}
```

### Custom Profiles

Create custom optimization profiles:

```python
custom_profile = NPUPerformanceProfile(
    max_prompt_len=4096,
    min_response_len=512, 
    cache_mode="OPTIMIZE_SPEED",
    performance_hint="LATENCY",
    compilation_strategy="OPTIMAL"
)
```

## 📊 Performance Monitoring

Access real-time performance metrics through the UI:
- **Response times** and token generation rates
- **Device utilization** and memory usage  
- **Error rates** and compilation diagnostics
- **Token filtering** statistics

## 🛠️ Troubleshooting

### Common Issues

**NPU Compilation Errors:**
```bash
# Check NPUW configuration
export NPUW_LLM=YES
export NPU_USE_NPUW=YES

# Verify driver installation
ov-device-list
```

**Memory Issues:**
- Reduce `MAX_PROMPT_LEN` in configuration
- Use "conservative" NPU profile
- Enable `NPU_LOW_MEMORY_MODE`

**Slow Performance:**
- Verify NPU drivers are installed
- Check cache directory permissions
- Use "aggressive" profile for maximum speed

### Debug Mode

Enable detailed logging:
```bash
# Use debug flag with modular version
python main.py --debug

```

### Command-Line Diagnostics

The new modular version includes comprehensive CLI diagnostics:

```bash
# Validate system requirements only
python main.py --validate-only

# Debug mode with verbose logging
python main.py --debug

# Test specific configurations
python main.py --device CPU --debug
python main.py --npu-profile conservative --debug
```

## 🎯 New Features Guide

### Dynamic System Prompts

Customize AI behavior in real-time:

1. **Access**: Click "🎯 System Prompt Configuration" in the interface
2. **Edit**: Modify the prompt to change AI behavior, expertise, and style
3. **Apply**: Click "✅ Apply & Clear Chat" to activate changes
4. **Reset**: Use "🔄 Reset to Default" to restore original prompt

**Example Custom Prompt:**
```
You are a Python coding expert specializing in data science and machine learning.

Key behaviors:
- Provide complete, working code examples
- Explain complex algorithms step-by-step
- Include relevant imports and dependencies
- Focus on best practices and optimization

You excel at: pandas, numpy, scikit-learn, and deep learning frameworks.
```

### RAG Document Processing

Upload and query your own documents:

1. **Upload**: Click "📚 Document Upload (RAG)" and select files
2. **Supported**: `.txt`, `.md`, `.py`, `.js`, `.html`, `.css`, `.json` files
3. **Process**: Wait for "✅ Successfully processed" confirmation
4. **Query**: Ask questions about your uploaded content
5. **Context**: AI will reference relevant document sections in responses

**Example Queries:**
- "What does the uploaded document say about error handling?"
- "Summarize the key points from the technical specification"
- "How should I implement the feature described in the documentation?"

### Performance Monitoring

Access real-time metrics:

1. **View**: Click "📊 Performance Metrics" 
2. **Monitor**: Real-time response times, tokens/second, device utilization
3. **Analyze**: Special tokens filtered, compilation errors, cache hits
4. **Reset**: Use "🔄 Reset Metrics" to clear historical data

## 🤝 Contributing

Contributions are welcome! This is a hobby project aimed at showcasing OpenVINO GenAI capabilities.

### Areas for Enhancement
- Additional model support (Llama, ChatGLM, etc.)
- Multi-modal capabilities (vision, audio)
- Distributed inference support
- Advanced UI features

### Development Setup
1. Fork the repository
2. Create feature branch
3. Follow existing code patterns
4. Add tests for new functionality
5. Submit pull request

## 📚 Documentation

- **[ACKNOWLEDGMENTS.md](ACKNOWLEDGMENTS.md)**: Third-party attributions
- **[DISCLAIMER.md](DISCLAIMER.md)**: Usage guidelines and disclaimers
- **[CLAUDE.md](CLAUDE.md)**: Development guidelines and context
- **`context/README.md`**: Enhanced context system documentation

## 🔍 Examples

### Simple Chat
```python
# Basic usage
message = "Explain quantum computing"
response = enhanced_qwen3_chat(message, [])
```

### Performance Monitoring
```python
# Access metrics
metrics = system_metrics.to_display_dict()
print(f"Avg response time: {metrics['Avg Response Time']}")
```

### Custom Configuration  
```python
# Deploy with custom config
deployment = Qwen3NPUDeployment(model_path, "aggressive")
pipeline = deployment.deploy()
```

## 📈 Performance Targets

### Intel NPU (Typical)
- **Load Time**: 60-90 seconds (first run), 30-60 seconds (cached)
- **First Token**: <2 seconds latency
- **Generation**: 15-25 tokens/second
- **Memory**: Optimized for NPU constraints

### CPU Fallback
- **Load Time**: 10-30 seconds
- **Generation**: 8-15 tokens/second  
- **Memory**: 8GB+ recommended

## 🔄 Version History

- **v1.0 (Enhanced)**: Production-ready with complete optimization
- **v0.9 (Refined)**: Hybrid architecture with consultant insights
- **v0.8 (Debug)**: NPU-optimized with debugging features
- **v0.7**: Basic Gradio integration

## ⚖️ Legal & Licensing

### License
This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

### Third-Party Components
This project integrates with several third-party components, each with their own licenses:
- **OpenVINO GenAI**: Apache 2.0 License (Intel Corporation)
- **Qwen3 Models**: Apache 2.0 with additional terms (Alibaba Cloud)  
- **Gradio**: Apache 2.0 License (HuggingFace)
- **Transformers**: Apache 2.0 License (HuggingFace)

See [ACKNOWLEDGMENTS.md](ACKNOWLEDGMENTS.md) for detailed attributions.

### Model Usage
This software is designed to work with Qwen3-8B models. Users must:
1. **Obtain Qwen3 models** through official channels (HuggingFace, ModelScope, etc.)
2. **Comply with Qwen model license terms** including attribution requirements
3. **Follow responsible AI usage guidelines**

### Disclaimer
This is a **hobby/educational project** and is not intended for commercial use. Users are responsible for ensuring compliance with all applicable licenses and terms of service.

See [DISCLAIMER.md](DISCLAIMER.md) for complete usage guidelines.

## 🙏 Acknowledgments

This project builds upon the excellent work of many open-source contributors:

- **Intel Corporation** for OpenVINO GenAI and NPU optimization techniques
- **Alibaba Cloud/Qwen Team** for the Qwen3 model architecture and specifications
- **HuggingFace** for Gradio, Transformers, and the broader AI ecosystem
- **The open-source AI community** for sharing knowledge and best practices

See [ACKNOWLEDGMENTS.md](ACKNOWLEDGMENTS.md) for complete attributions.

---

## 🔗 Links

- **OpenVINO GenAI**: https://github.com/openvinotoolkit/openvino.genai
- **Qwen Models**: https://github.com/QwenLM/Qwen  
- **Gradio**: https://gradio.app/
- **Intel NPU**: https://www.intel.com/content/www/us/en/products/docs/processors/core/core-processors-with-intel-npu.html

---

*⭐ If this project helps you, please consider giving it a star!*

*🐛 Found a bug? Please open an issue!*

*💡 Have an idea? Contributions are welcome!*

================================================================================

FILE: SETUP.md
----------------------------------------
# Setup Guide for Enhanced Qwen3 OpenVINO GenAI Chat

Quick setup guide for new users to get the application running.

## 🚀 Quick Start (5 minutes)

### 1. Prerequisites Check
```bash
# Check Python version (3.8+ required)
python --version

# Check if you have git
git --version
```

### 2. Clone and Install
```bash
# Clone the repository
git clone <your-repo-url>
cd OpenVinoDev

# Install Python dependencies
pip install -r requirements.txt
```

### 3. Install OpenVINO
Follow official installation guide: https://docs.openvino.ai/latest/openvino_docs_install_guides_overview.html

**Quick install (most systems):**
```bash
pip install openvino
```

### 4. Get Qwen3 Model
Download Qwen3-8B INT4 model from official sources:
- **HuggingFace**: https://huggingface.co/Qwen/Qwen2.5-8B-Instruct
- **ModelScope**: https://modelscope.cn/models/qwen/Qwen2.5-8B-Instruct

Convert to OpenVINO format using `optimum-intel` if needed.

### 5. Configure Environment
```bash
# Set model path (adjust to your location)
export QWEN3_MODEL_PATH="/path/to/your/qwen3-8b-int4-ov"

# Optional: Set cache directory
export CACHE_DIR="./cache"
```

### 6. Run the Application
```bash
python gradio_qwen_enhanced.py
```

Open http://127.0.0.1:7860 in your browser.

## 🔧 Detailed Setup

### For Intel NPU Users
1. **Install NPU drivers** from Intel's official site
2. **Verify NPU availability**:
   ```bash
   python -c "import openvino as ov; print(ov.Core().available_devices)"
   ```
3. **Set NPU as target device**:
   ```bash
   export TARGET_DEVICE="NPU"
   ```

### For CPU-Only Users
```bash
export TARGET_DEVICE="CPU"
```

### Troubleshooting
- **"Model not found"**: Check `QWEN3_MODEL_PATH` environment variable
- **"NPU not available"**: Install NPU drivers or use CPU fallback
- **Import errors**: Ensure all dependencies from `requirements.txt` are installed

## 📚 Next Steps
- Read [README.md](README.md) for detailed features
- Check [DISCLAIMER.md](DISCLAIMER.md) for usage guidelines  
- See [ACKNOWLEDGMENTS.md](ACKNOWLEDGMENTS.md) for third-party attributions

## 🆘 Getting Help
- Check existing GitHub issues
- Review troubleshooting section in README.md
- Ensure all dependencies are correctly installed

================================================================================

FILE: app\__init__.py
----------------------------------------
"""
Enhanced OpenVINO GenAI Chat Application
========================================

A modular, production-ready implementation of Phi-3-mini-128k-instruct chat interface
using OpenVINO GenAI with Intel NPU optimization and RAG capabilities.

Copyright (c) 2025 sbran
Licensed under the MIT License - see LICENSE file for details

Modules:
--------
- config: Configuration management and environment handling
- model: Pipeline deployment and system initialization  
- streamer: Token streaming and filtering for LLMs
- chat: Core chat processing with RAG integration
- ui: Gradio interface creation and event handling
"""

__version__ = "2.0.0"
__author__ = "sbran"
__license__ = "MIT"

# Import key components for easy access
from .config import ConfigurationLoader
from .model import deploy_llm_pipeline, initialize_system_with_validation
from .streamer import EnhancedLLMStreamer
from .chat import enhanced_llm_chat
from .ui import create_enhanced_interface

__all__ = [
    "ConfigurationLoader",
    "deploy_llm_pipeline", 
    "initialize_system_with_validation",
    "EnhancedLLMStreamer",
    "enhanced_llm_chat",
    "create_enhanced_interface"
]

================================================================================

FILE: app\agent.py
----------------------------------------
"""
Agentic Architecture Module (Phase 3.3)
=======================================

Function-calling agent system that transforms the chat application from a passive 
chatbot into an active AI agent capable of using tools and performing complex tasks.

This implements the ReAct (Reasoning + Acting) pattern with OpenVINO GenAI as the 
reasoning engine and LangChain as the agent framework.
"""

import json
import re
import math
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass

# Agent framework imports with fallback
try:
    from langchain_core.tools import Tool
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    AGENT_AVAILABLE = True
    print("✅ Agent framework loaded successfully")
except ImportError as e:
    print(f"⚠️ Agent framework not available: {e}")
    print("📝 Install with: pip install langchain-core langchain-experimental")
    AGENT_AVAILABLE = False


@dataclass
class ToolResult:
    """Result from tool execution"""
    success: bool
    result: str
    error: Optional[str] = None


class AgentTools:
    """Collection of tools available to the AI agent"""
    
    @staticmethod
    def calculator(expression: str) -> ToolResult:
        """
        Perform mathematical calculations safely.
        
        Args:
            expression: Mathematical expression to evaluate (e.g., "2 + 2", "sqrt(16)")
            
        Returns:
            ToolResult with calculation result or error
        """
        try:
            # Sanitize input - only allow safe mathematical operations
            safe_chars = set('0123456789+-*/()., ')
            safe_functions = ['abs', 'ceil', 'floor', 'sqrt', 'pow', 'log', 'sin', 'cos', 'tan']
            
            # Replace safe function names
            sanitized = expression.lower()
            for func in safe_functions:
                sanitized = sanitized.replace(func, f'math.{func}')
            
            # Check for dangerous patterns
            dangerous = ['import', 'exec', 'eval', '__', 'open', 'file', 'input']
            if any(d in sanitized.lower() for d in dangerous):
                return ToolResult(False, "", "Expression contains unsafe operations")
            
            # Evaluate using math module in restricted environment
            import math
            allowed_names = {
                'math': math,
                'abs': abs, 'round': round, 'min': min, 'max': max,
                'sum': sum, 'len': len
            }
            
            result = eval(sanitized, {"__builtins__": {}}, allowed_names)
            return ToolResult(True, str(result))
            
        except Exception as e:
            return ToolResult(False, "", f"Calculation error: {str(e)}")
    
    @staticmethod
    def datetime_info(query: str = "") -> ToolResult:
        """
        Get current date, time, or calculate date differences.
        
        Args:
            query: Optional specific query like "tomorrow", "next week", etc.
            
        Returns:
            ToolResult with date/time information
        """
        try:
            now = datetime.now()
            
            if not query or query.lower() in ["now", "current", "today"]:
                result = f"Current date and time: {now.strftime('%Y-%m-%d %H:%M:%S')}"
            
            elif "tomorrow" in query.lower():
                tomorrow = now + timedelta(days=1)
                result = f"Tomorrow's date: {tomorrow.strftime('%Y-%m-%d')}"
            
            elif "yesterday" in query.lower():
                yesterday = now - timedelta(days=1)
                result = f"Yesterday's date: {yesterday.strftime('%Y-%m-%d')}"
            
            elif "next week" in query.lower():
                next_week = now + timedelta(weeks=1)
                result = f"Next week: {next_week.strftime('%Y-%m-%d')}"
            
            elif "week" in query.lower() and "ago" in query.lower():
                last_week = now - timedelta(weeks=1)
                result = f"One week ago: {last_week.strftime('%Y-%m-%d')}"
            
            else:
                result = f"Current date and time: {now.strftime('%Y-%m-%d %H:%M:%S')}\nDay of week: {now.strftime('%A')}"
            
            return ToolResult(True, result)
            
        except Exception as e:
            return ToolResult(False, "", f"DateTime error: {str(e)}")
    
    @staticmethod
    def web_search_mock(query: str) -> ToolResult:
        """
        Mock web search tool (placeholder for security).
        In production, this would connect to a search API.
        
        Args:
            query: Search query string
            
        Returns:
            ToolResult with mock search results
        """
        try:
            # This is a mock implementation for security reasons
            # In production, you would integrate with DuckDuckGo API, Google Custom Search, etc.
            
            mock_results = [
                f"Search results for: '{query}'",
                "• [Mock Result 1] This would be a real web search result",
                "• [Mock Result 2] Integration with search APIs available",
                "• [Mock Result 3] Enable by configuring search provider API keys",
                "",
                "Note: This is a mock tool. Configure real search API to enable live results."
            ]
            
            return ToolResult(True, "\n".join(mock_results))
            
        except Exception as e:
            return ToolResult(False, "", f"Search error: {str(e)}")
    
    @staticmethod
    def text_analysis(text: str, analysis_type: str = "summary") -> ToolResult:
        """
        Analyze text content (word count, character count, basic analysis).
        
        Args:
            text: Text to analyze
            analysis_type: Type of analysis ("summary", "count", "readability")
            
        Returns:
            ToolResult with text analysis
        """
        try:
            if not text:
                return ToolResult(False, "", "No text provided for analysis")
            
            # Basic analysis metrics
            word_count = len(text.split())
            char_count = len(text)
            char_count_no_spaces = len(text.replace(' ', ''))
            sentence_count = len([s for s in text.split('.') if s.strip()])
            paragraph_count = len([p for p in text.split('\n\n') if p.strip()])
            
            if analysis_type.lower() == "count":
                result = f"Word count: {word_count}\nCharacter count: {char_count}\nCharacters (no spaces): {char_count_no_spaces}\nSentences: {sentence_count}\nParagraphs: {paragraph_count}"
            
            elif analysis_type.lower() == "readability":
                # Simple readability metrics
                avg_words_per_sentence = word_count / max(sentence_count, 1)
                avg_chars_per_word = char_count_no_spaces / max(word_count, 1)
                
                result = f"Readability Analysis:\n• Average words per sentence: {avg_words_per_sentence:.1f}\n• Average characters per word: {avg_chars_per_word:.1f}\n• Text complexity: {'Simple' if avg_words_per_sentence < 15 else 'Moderate' if avg_words_per_sentence < 25 else 'Complex'}"
            
            else:  # summary
                result = f"Text Analysis Summary:\n• Words: {word_count}\n• Characters: {char_count}\n• Sentences: {sentence_count}\n• Paragraphs: {paragraph_count}\n• Average sentence length: {word_count / max(sentence_count, 1):.1f} words"
            
            return ToolResult(True, result)
            
        except Exception as e:
            return ToolResult(False, "", f"Analysis error: {str(e)}")


class ReActAgent:
    """ReAct (Reasoning + Acting) Agent using OpenVINO GenAI"""
    
    def __init__(self, llm_pipeline, tokenizer):
        """
        Initialize ReAct agent with LLM pipeline and available tools.
        
        Args:
            llm_pipeline: OpenVINO GenAI pipeline instance
            tokenizer: Tokenizer instance
        """
        self.llm = llm_pipeline
        self.tokenizer = tokenizer
        self.available = AGENT_AVAILABLE
        
        # Define available tools
        self.tools = {
            "calculator": {
                "function": AgentTools.calculator,
                "description": "Perform mathematical calculations. Input: mathematical expression (e.g., '2+2', 'sqrt(16)', 'sin(3.14/2)')",
                "parameters": "expression (string): Mathematical expression to evaluate"
            },
            "datetime": {
                "function": AgentTools.datetime_info,
                "description": "Get current date/time or calculate date differences. Input: query like 'now', 'tomorrow', 'next week'",
                "parameters": "query (string): Date/time query or leave empty for current datetime"
            },
            "web_search": {
                "function": AgentTools.web_search_mock,
                "description": "Search the web for information (mock implementation). Input: search query string",
                "parameters": "query (string): What to search for"
            },
            "text_analysis": {
                "function": AgentTools.text_analysis,
                "description": "Analyze text content (word count, readability, etc.). Input: text and analysis type",
                "parameters": "text (string): Text to analyze, analysis_type (string): 'summary', 'count', or 'readability'"
            }
        }
        
        # ReAct prompt template
        self.system_prompt = self._create_system_prompt()
        
        if self.available:
            print(f"✅ ReAct agent initialized with {len(self.tools)} tools")
        else:
            print("⚠️ Agent functionality limited - install langchain-core for full capabilities")
    
    def _create_system_prompt(self) -> str:
        """Create the ReAct system prompt with tool descriptions"""
        
        tools_description = "\n".join([
            f"- {name}: {info['description']}" 
            for name, info in self.tools.items()
        ])
        
        return f"""You are an AI assistant with access to tools. Use the ReAct (Reasoning + Acting) approach to solve problems.

Available Tools:
{tools_description}

When you need to use a tool, format your response EXACTLY like this:
```
Thought: I need to use a tool to help with this request.
Action: tool_name
Action Input: input_for_tool
```

After receiving tool results, continue reasoning and provide a final answer.

Example:
User: What's 25 * 4?
Assistant: I need to calculate this multiplication.

Thought: I need to calculate 25 * 4.
Action: calculator  
Action Input: 25 * 4

[Tool Result: 100]

The result of 25 * 4 is 100.

Always think step by step and use tools when they can help provide accurate information."""

    def _parse_action(self, text: str) -> Optional[Tuple[str, str]]:
        """
        Parse action from LLM response.
        
        Args:
            text: LLM response text
            
        Returns:
            Tuple of (tool_name, input) if action found, None otherwise
        """
        # Look for Action: tool_name pattern
        action_match = re.search(r'Action:\s*(\w+)', text, re.IGNORECASE)
        if not action_match:
            return None
        
        tool_name = action_match.group(1).lower()
        
        # Look for Action Input: input pattern
        input_match = re.search(r'Action Input:\s*(.+?)(?:\n|$)', text, re.IGNORECASE | re.DOTALL)
        if not input_match:
            return None
        
        tool_input = input_match.group(1).strip()
        
        return (tool_name, tool_input)
    
    def _execute_tool(self, tool_name: str, tool_input: str) -> ToolResult:
        """
        Execute a tool with given input.
        
        Args:
            tool_name: Name of tool to execute
            tool_input: Input parameters for the tool
            
        Returns:
            ToolResult with execution result
        """
        if tool_name not in self.tools:
            return ToolResult(False, "", f"Tool '{tool_name}' not found. Available: {list(self.tools.keys())}")
        
        try:
            tool_func = self.tools[tool_name]["function"]
            
            # Handle different tool signatures
            if tool_name == "text_analysis" and "," in tool_input:
                # Split text and analysis_type
                parts = [p.strip() for p in tool_input.split(",", 1)]
                if len(parts) == 2:
                    return tool_func(parts[0], parts[1])
                else:
                    return tool_func(tool_input)
            else:
                return tool_func(tool_input)
                
        except Exception as e:
            return ToolResult(False, "", f"Tool execution error: {str(e)}")
    
    def process_with_tools(self, user_message: str, generation_params: Dict[str, Any] = None) -> str:
        """
        Process user message using ReAct pattern with tools.
        
        Args:
            user_message: User's input message
            generation_params: Optional generation parameters
            
        Returns:
            Final response after tool usage (if needed)
        """
        if not self.available:
            return f"🤖 Basic Response: {user_message}\n\n⚠️ Agent tools not available. Install langchain-core for full capabilities."
        
        max_iterations = 5  # Prevent infinite loops
        conversation_history = [
            f"System: {self.system_prompt}",
            f"User: {user_message}"
        ]
        
        for iteration in range(max_iterations):
            # Generate response with current conversation context
            full_context = "\n\n".join(conversation_history)
            
            try:
                # Start a fresh chat session for agent reasoning
                self.llm.start_chat(self.system_prompt)
                
                # Generate response
                from .chat import create_qwen3_generation_config
                gen_config = create_qwen3_generation_config(generation_params)
                
                response = ""
                for chunk in self.llm.generate(full_context, gen_config):
                    response += chunk
                
                self.llm.finish_chat()
                
            except Exception as e:
                return f"❌ Agent processing error: {str(e)}"
            
            # Check if response contains an action
            action_result = self._parse_action(response)
            
            if action_result is None:
                # No action found, this is the final response
                return response
            
            tool_name, tool_input = action_result
            
            # Execute the tool
            print(f"🔧 Executing tool: {tool_name} with input: {tool_input}")
            tool_result = self._execute_tool(tool_name, tool_input)
            
            # Add tool result to conversation
            if tool_result.success:
                conversation_history.append(f"Assistant: {response}")
                conversation_history.append(f"Tool Result: {tool_result.result}")
            else:
                error_msg = tool_result.error or "Tool execution failed"
                conversation_history.append(f"Assistant: {response}")
                conversation_history.append(f"Tool Error: {error_msg}")
        
        # If we've exceeded max iterations, return what we have
        return f"⚠️ Reasoning process exceeded maximum iterations. Last response:\n\n{response}"
    
    def get_available_tools(self) -> Dict[str, str]:
        """Get list of available tools with descriptions"""
        return {
            name: info["description"] 
            for name, info in self.tools.items()
        }


# Global agent instance (initialized by main.py)
react_agent: Optional[ReActAgent] = None


def initialize_agent(llm_pipeline, tokenizer):
    """Initialize the global agent instance"""
    global react_agent
    react_agent = ReActAgent(llm_pipeline, tokenizer)
    return react_agent


def get_agent() -> Optional[ReActAgent]:
    """Get the global agent instance"""
    return react_agent

================================================================================

FILE: app\chat.py
----------------------------------------
"""
Chat Processing Module
=====================

Core chat processing logic with RAG integration, input validation,
security features, and comprehensive error handling.
"""

import time
import threading
import re
from typing import Iterator, List, Dict, Tuple, Any
from typing_extensions import TypedDict

import openvino_genai as ov_genai

from .config import get_config
from .streamer import EnhancedLLMStreamer, streaming_metrics

# Agent system import (Phase 3.3)
try:
    from .agent import get_agent, AGENT_AVAILABLE
    print("✅ Agent system loaded")
except ImportError as e:
    print(f"⚠️ Agent system not available: {e}")
    AGENT_AVAILABLE = False
    get_agent = lambda: None

# Type definitions for Gradio ChatInterface compatibility
# ChatHistory is a list of message dictionaries with role and content
ChatHistory = List[Dict[str, str]]

# RAG system imports with fallback
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    try:
        # Try modern import first
        from langchain_huggingface import HuggingFaceEmbeddings
    except ImportError:
        # Fallback to legacy import
        from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    
    # Advanced document parsing (Phase 3.1)
    try:
        from langchain_unstructured import UnstructuredLoader
        from unstructured.partition.auto import partition
        ADVANCED_PARSING_AVAILABLE = True
        print("✅ Advanced document parsing (unstructured) loaded successfully")
    except ImportError:
        ADVANCED_PARSING_AVAILABLE = False
        print("📝 Advanced parsing not available - install with: pip install unstructured[local-inference] langchain-unstructured")
    
    # Cross-encoder reranking (Phase 3.2)
    try:
        from sentence_transformers import CrossEncoder
        RERANKING_AVAILABLE = True
        print("✅ Cross-encoder reranking loaded successfully")
    except ImportError:
        RERANKING_AVAILABLE = False
        print("📝 Reranking not available - already have sentence-transformers but need torch")
    
    RAG_AVAILABLE = True
    print("✅ RAG dependencies loaded successfully")
except ImportError as e:
    print(f"⚠️ RAG dependencies not available: {e}")
    print("📝 Install with: pip install langchain faiss-cpu sentence-transformers")
    RAG_AVAILABLE = False
    ADVANCED_PARSING_AVAILABLE = False
    RERANKING_AVAILABLE = False


class DocumentRAGSystem:
    """Retrieval-Augmented Generation system for document processing"""
    
    def __init__(self):
        """Initialize RAG system with fallback handling"""
        self.vector_store = None
        self.embeddings = None
        self.text_splitter = None
        self.processed_docs_count = 0
        self.available = RAG_AVAILABLE
        
        # Advanced features
        self.advanced_parsing = ADVANCED_PARSING_AVAILABLE
        self.reranking = RERANKING_AVAILABLE
        self.cross_encoder = None
        
        if RAG_AVAILABLE:
            try:
                # Initialize embeddings model (lightweight for fast loading)
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2",
                    model_kwargs={'device': 'cpu'}
                )
                
                # Initialize text splitter with optimized settings
                self.text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800,  # Smaller chunks for better retrieval
                    chunk_overlap=100,  # Overlap for context preservation
                    separators=["\n\n", "\n", ". ", " ", ""]
                )
                
                # Initialize cross-encoder for reranking (Phase 3.2)
                if RERANKING_AVAILABLE:
                    try:
                        self.cross_encoder = CrossEncoder('BAAI/bge-reranker-base')
                        print("✅ Cross-encoder reranker initialized")
                    except Exception as e:
                        print(f"⚠️ Cross-encoder initialization failed: {e}")
                        self.reranking = False
                
                print("✅ RAG system initialized successfully")
                print(f"📊 Features: Advanced parsing={self.advanced_parsing}, Reranking={self.reranking}")
                
            except Exception as e:
                print(f"⚠️ RAG initialization failed: {e}")
                self.available = False
        else:
            print("📝 RAG system not available - install dependencies to enable")
    
    def process_uploaded_file(self, file_path: str, file_name: str) -> str:
        """
        Process uploaded file for RAG retrieval.
        
        Args:
            file_path: Path to the uploaded file
            file_name: Original name of the file
            
        Returns:
            Status message about processing result
        """
        if not self.available:
            return "❌ RAG system not available. Install langchain and faiss-cpu to enable document processing."
        
        try:
            # Use advanced parsing if available (Phase 3.1)
            if self.advanced_parsing and file_name.lower().endswith(('.pdf', '.docx', '.pptx', '.html')):
                try:
                    # Use unstructured for advanced document parsing
                    elements = partition(filename=file_path, strategy="hi_res")
                    text = "\n\n".join([str(element) for element in elements])
                    parsing_method = "Advanced (unstructured)"
                    print(f"📚 Using advanced parsing for {file_name}")
                except Exception as e:
                    print(f"⚠️ Advanced parsing failed for {file_name}, falling back to basic: {e}")
                    # Fallback to basic parsing
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            text = f.read()
                    except UnicodeDecodeError:
                        with open(file_path, 'r', encoding='latin-1') as f:
                            text = f.read()
                    parsing_method = "Basic (fallback)"
            else:
                # Basic text parsing for supported formats
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                except UnicodeDecodeError:
                    # Try with different encoding
                    with open(file_path, 'r', encoding='latin-1') as f:
                        text = f.read()
                parsing_method = "Basic (text)"
            
            if not text.strip():
                return f"⚠️ File '{file_name}' appears to be empty."
            
            # Split text into chunks
            chunks = self.text_splitter.split_text(text)
            
            if not chunks:
                return f"⚠️ No processable content found in '{file_name}'."
            
            # Create or update vector store
            if self.vector_store is None:
                self.vector_store = FAISS.from_texts(
                    texts=chunks, 
                    embedding=self.embeddings,
                    metadatas=[{"source": file_name, "chunk": i} for i in range(len(chunks))]
                )
            else:
                # Add new documents to existing store
                new_store = FAISS.from_texts(
                    texts=chunks, 
                    embedding=self.embeddings,
                    metadatas=[{"source": file_name, "chunk": i} for i in range(len(chunks))]
                )
                self.vector_store.merge_from(new_store)
            
            self.processed_docs_count += 1
            
            return f"✅ Successfully processed '{file_name}' using {parsing_method}: {len(chunks)} chunks created from {len(text):,} characters. Ready to answer questions about this document."
            
        except Exception as e:
            return f"❌ Error processing '{file_name}': {str(e)}"
    
    def retrieve_context(self, query: str, k: int = 3) -> str:
        """
        Retrieve relevant context for a query with optional cross-encoder reranking.
        
        Args:
            query: User question to find relevant context for
            k: Number of top chunks to retrieve
            
        Returns:
            Concatenated context from relevant document chunks
        """
        if not self.available or self.vector_store is None:
            return ""
        
        try:
            # Use two-stage retrieval with reranking if available (Phase 3.2)
            if self.reranking and self.cross_encoder is not None:
                # Stage 1: Retrieve larger candidate set (e.g., top 20)
                candidate_docs = self.vector_store.similarity_search(query, k=min(20, k*6))
                
                if not candidate_docs:
                    return ""
                
                # Stage 2: Rerank with cross-encoder
                try:
                    query_doc_pairs = [(query, doc.page_content) for doc in candidate_docs]
                    scores = self.cross_encoder.predict(query_doc_pairs)
                    
                    # Sort by reranking scores and take top k
                    scored_docs = list(zip(candidate_docs, scores))
                    scored_docs.sort(key=lambda x: x[1], reverse=True)
                    docs = [doc for doc, score in scored_docs[:k]]
                    
                    print(f"🔄 Reranked {len(candidate_docs)} candidates → {k} results")
                    
                except Exception as e:
                    print(f"⚠️ Reranking failed, using vector search results: {e}")
                    docs = candidate_docs[:k]
            else:
                # Standard single-stage retrieval
                docs = self.vector_store.similarity_search(query, k=k)
            
            if not docs:
                return ""
            
            # Format context with source attribution
            context_parts = []
            for doc in docs:
                source = doc.metadata.get("source", "Unknown")
                content = doc.page_content.strip()
                context_parts.append(f"[From {source}]\n{content}")
            
            return "\n\n---\n\n".join(context_parts)
            
        except Exception as e:
            print(f"⚠️ Context retrieval error: {e}")
            return ""
    
    def clear_documents(self) -> str:
        """Clear all processed documents"""
        self.vector_store = None
        self.processed_docs_count = 0
        return "✅ All documents cleared from memory."
    
    def get_status(self) -> dict:
        """Get current RAG system status with advanced features"""
        return {
            "Available": self.available,
            "Documents Processed": self.processed_docs_count,
            "Vector Store": "Loaded" if self.vector_store is not None else "Empty",
            "Embedding Model": "all-MiniLM-L6-v2" if self.available else "None",
            "Advanced Parsing": "✅ unstructured" if self.advanced_parsing else "❌ basic text only",
            "Cross-Encoder Reranking": "✅ BAAI/bge-reranker-base" if self.reranking else "❌ vector search only",
            "Supported Formats": "PDF, DOCX, PPTX, HTML, TXT, MD, PY, JS, CSS, JSON" if self.advanced_parsing else "TXT, MD, PY, JS, CSS, JSON"
        }


class InputValidator:
    """Security-focused input validation and sanitization"""
    
    @staticmethod
    def validate_message(message: str) -> Tuple[bool, str]:
        """
        Validate user message for security and content policy compliance.
        
        Args:
            message: User input to validate
            
        Returns:
            Tuple of (is_valid, reason_if_invalid)
        """
        if not message or not isinstance(message, str):
            return False, "Empty or invalid message"
        
        # Check for excessively long messages (security)
        if len(message) > 10000:  # Much higher than UI limit
            return False, "Message exceeds maximum length"
        
        # Check for potential injection patterns
        suspicious_patterns = [
            r'<script[^>]*>',  # Script injection
            r'javascript:',     # JavaScript URLs
            r'data:.*base64',   # Data URLs
            r'eval\s*\(',      # Eval calls
            r'exec\s*\(',      # Exec calls
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return False, "Message contains potentially unsafe content"
        
        # Check for excessive special characters (potential encoding attacks)
        special_char_ratio = len([c for c in message if not c.isalnum() and not c.isspace()]) / len(message)
        if special_char_ratio > 0.5:  # More than 50% special characters
            return False, "Message contains excessive special characters"
        
        return True, ""
    
    @staticmethod
    def sanitize_message(message: str) -> str:
        """
        Sanitize user message while preserving readability.
        
        Args:
            message: Raw user input
            
        Returns:
            Sanitized message safe for processing
        """
        # Remove null bytes and control characters (except newlines and tabs)
        sanitized = ''.join(char for char in message if ord(char) >= 32 or char in '\n\t')
        
        # Normalize whitespace
        sanitized = ' '.join(sanitized.split())
        
        # Limit consecutive repeated characters (potential DoS protection)
        sanitized = re.sub(r'(.)\1{10,}', r'\1\1\1', sanitized)
        
        return sanitized.strip()


# Global RAG system instance
rag_system = DocumentRAGSystem()

# Global instances (will be set by main.py)
pipe = None
tokenizer = None


def create_qwen3_generation_config(generation_params: Dict[str, Any] = None) -> ov_genai.GenerationConfig:
    """
    Create optimized generation configuration for Phi-3 with dynamic parameters.
    
    Args:
        generation_params: Optional dict with temperature, top_p, max_new_tokens
    
    Returns:
        Configured GenerationConfig with security-conscious defaults
    """
    gen_config = ov_genai.GenerationConfig()
    config = get_config()
    
    # Load generation settings from configuration (defaults)
    gen_settings = config.get_section("generation")
    
    # Use dynamic parameters if provided, otherwise fall back to config defaults
    if generation_params:
        temperature = generation_params.get('temperature', gen_settings.get("temperature", 0.6))
        top_p = generation_params.get('top_p', gen_settings.get("top_p", 0.95))
        max_new_tokens = generation_params.get('max_new_tokens', gen_settings.get("max_new_tokens", 1024))
    else:
        temperature = gen_settings.get("temperature", 0.6)
        top_p = gen_settings.get("top_p", 0.95)
        max_new_tokens = gen_settings.get("max_new_tokens", 1024)
    
    gen_config.do_sample = gen_settings.get("do_sample", True)
    gen_config.temperature = min(float(temperature), 2.0)  # Security: cap temperature
    gen_config.top_p = min(float(top_p), 1.0)  # Security: cap top_p
    gen_config.top_k = min(gen_settings.get("top_k", 20), 100)  # Security: reasonable top_k
    gen_config.max_new_tokens = min(int(max_new_tokens), 2048)  # Security: limit tokens
    gen_config.repetition_penalty = max(1.0, min(gen_settings.get("repetition_penalty", 1.1), 2.0))  # Security: reasonable range
    
    return gen_config


def process_user_message(message: str, history: ChatHistory) -> Tuple[str, bool]:
    """
    Process user message with smart truncation handling.
    
    Args:
        message: Raw user input message
        history: Current chat conversation history
        
    Returns:
        Tuple of (processed_message, was_truncated)
    """
    config = get_config()
    max_message_length = config.get("ui", "max_message_length", 400)
    original_length = len(message)
    
    # Handle overly long messages
    if original_length > max_message_length:
        # Smart truncation
        if '.' in message:
            sentences = message.split('.')
            truncated = []
            current_length = 0
            
            for sentence in sentences:
                if current_length + len(sentence) + 1 <= max_message_length * 0.85:
                    truncated.append(sentence)
                    current_length += len(sentence) + 1
                else:
                    break
            
            if truncated:
                processed = '. '.join(truncated) + '.'
                if len(processed) < original_length * 0.5:
                    processed = message[:max_message_length-50] + "..."
            else:
                processed = message[:max_message_length-50] + "..."
        else:
            processed = message[:max_message_length-50] + "..."
        
        print(f"📏 Message truncated: {original_length} → {len(processed)} chars")
        return processed, True
    
    return message, False


def prepare_chat_input(message: str, history: ChatHistory) -> Tuple[str, bool, ChatHistory]:
    """
    Prepare and validate chat input with smart message handling and security validation.
    
    Args:
        message: Raw user input
        history: Current chat history
        
    Returns:
        Tuple of (processed_message, was_truncated, updated_history)
        
    Raises:
        ValueError: If message fails security validation
    """
    # Input validation
    if not message.strip():
        return message, False, history
    
    # Security validation
    is_valid, reason = InputValidator.validate_message(message)
    if not is_valid:
        error_history = history.copy()
        error_history.extend([
            {"role": "user", "content": message},
            {"role": "assistant", "content": f"🚫 Message rejected: {reason}. Please try a different message."}
        ])
        raise ValueError(f"Security validation failed: {reason}")
    
    # Sanitize input
    sanitized_message = InputValidator.sanitize_message(message)
    
    # Process message with smart handling
    processed_message, was_truncated = process_user_message(sanitized_message, history)
    
    # Update history with user message and truncation warning if needed
    updated_history = history.copy()
    
    if was_truncated:
        # Add truncation warning as a separate exchange
        updated_history.extend([
            {"role": "user", "content": message},
            {"role": "assistant", "content": f"⚠️ Your message was truncated from {len(message):,} to {len(processed_message)} characters due to NPU memory limits. Processing the truncated version..."}
        ])
    
    # Add current user message with empty bot response placeholder
    updated_history.extend([
        {"role": "user", "content": processed_message},
        {"role": "assistant", "content": ""}
    ])
    
    return processed_message, was_truncated, updated_history


def execute_generation(processed_message: str, streamer: EnhancedLLMStreamer, generation_params: Dict[str, Any] = None) -> bool:
    """
    Execute model generation in a controlled manner.
    
    Args:
        processed_message: Message to generate response for
        streamer: Configured streamer for token processing
        generation_params: Optional dict with temperature, top_p, max_new_tokens
        
    Returns:
        True if generation succeeded, False otherwise
    """
    try:
        generation_config = create_qwen3_generation_config(generation_params)
        pipe.generate(processed_message, generation_config, streamer)
        return True
    except Exception as e:
        print(f"❌ Generation error: {e}")
        # Send error through streamer
        error_msg = f"❌ Generation error: {str(e)[:100]}..."
        streamer.text_queue.put(error_msg)
        streamer.text_queue.put(None)
        return False


def stream_response_to_history(streamer: EnhancedLLMStreamer, history: ChatHistory) -> Iterator[ChatHistory]:
    """
    Stream model response tokens to chat history.
    
    Args:
        streamer: Active streamer with generation in progress
        history: Chat history to update (list of [user_msg, bot_response] pairs)
        
    Yields:
        Updated history with streaming response
    """
    try:
        for chunk in streamer:
            if chunk:  # Only add non-empty chunks
                history[-1]["content"] += chunk  # Update the bot response content
                yield history
    except Exception as e:
        print(f"❌ Streaming error: {e}")
        history[-1]["content"] = f"❌ Streaming error: {str(e)[:100]}..."
        yield history


def handle_chat_error(error: Exception, history: ChatHistory) -> ChatHistory:
    """
    Handle chat errors with user-friendly messages.
    
    Args:
        error: Exception that occurred
        history: Current chat history
        
    Returns:
        Updated history with error message
    """
    print(f"❌ Chat function error: {error}")
    
    # Determine error type and provide helpful message
    error_message = "❌ An error occurred. "
    error_str = str(error).lower()
    
    if "memory" in error_str:
        error_message += "Memory limit reached. Try starting a new conversation."
    elif "token" in error_str or "length" in error_str:
        error_message += "Message too long. Please try a shorter message."
    elif "compile" in error_str:
        error_message += "NPU compilation issue. Check NPUW configuration."
    elif "timeout" in error_str:
        error_message += "Generation timed out. Try a simpler request."
    elif "device" in error_str:
        error_message += "Device error. NPU may not be available."
    else:
        error_message += f"Details: {str(error)[:100]}..."
    
    # Add error to history in the correct format
    updated_history = history.copy()
    if not updated_history or updated_history[-1]["role"] != "assistant" or updated_history[-1]["content"]:
        # If no history or last message has a bot response, add new exchange
        updated_history.append({"role": "assistant", "content": error_message})
    else:
        # Update the empty bot response
        updated_history[-1]["content"] = error_message
    
    return updated_history


def should_use_agent(message: str) -> bool:
    """
    Determine if the user message should be processed by the agent system.
    
    Args:
        message: User input message
        
    Returns:
        True if agent should be used, False for regular chat
    """
    # Keywords that suggest tool usage
    agent_keywords = [
        'calculate', 'compute', 'math', 'equation', 'solve',
        'what time', 'what date', 'today', 'tomorrow', 'yesterday', 
        'search', 'look up', 'find information',
        'analyze text', 'word count', 'character count',
        'tool', 'function', 'help me with'
    ]
    
    message_lower = message.lower()
    return any(keyword in message_lower for keyword in agent_keywords)


def enhanced_llm_chat(message: str, history: ChatHistory, generation_params: Dict[str, Any] = None) -> Iterator[ChatHistory]:
    """
    Enhanced chat function with comprehensive Phi-3 optimization and RAG support.
    
    This is the main chat processing function that handles user input,
    processes it through the Phi-3 model with optional document context,
    and streams back the response with comprehensive error handling and performance monitoring.
    
    Args:
        message: User input message to process
        history: Current chat conversation history
        generation_params: Optional dict with temperature, top_p, max_new_tokens
        
    Yields:
        Updated chat history with streaming response as it's generated
    """
    request_start_time = time.time()
    streaming_metrics.start_request()
    
    try:
        # Step 1: Prepare and validate input
        processed_message, was_truncated, updated_history = prepare_chat_input(message, history)
        
        # Early return for empty messages
        if not processed_message.strip():
            yield updated_history
            return
        
        # Show truncation warning with brief pause
        if was_truncated:
            yield updated_history
            time.sleep(0.5)  # Brief pause for user to see warning
        
        # Step 1.5: Agent vs Regular Chat Decision
        agent = get_agent()
        use_agent = AGENT_AVAILABLE and agent and should_use_agent(processed_message)
        
        if use_agent:
            # Use agent processing for tool-based responses
            print(f"🤖 Using agent processing for: {processed_message[:50]}...")
            try:
                agent_response = agent.process_with_tools(processed_message, generation_params)
                
                # Stream the agent response
                updated_history[-1]["content"] = agent_response
                yield updated_history
                
                print(f"📊 Agent request complete: {time.time() - request_start_time:.2f}s total")
                return
                
            except Exception as e:
                print(f"⚠️ Agent processing failed, falling back to regular chat: {e}")
                # Continue with regular processing
        
        # Step 1.6: RAG Context Retrieval (for regular chat)
        rag_context = ""
        if rag_system.available and rag_system.vector_store is not None:
            rag_context = rag_system.retrieve_context(processed_message)
            if rag_context:
                # Augment the message with context
                augmented_message = f"""Based on the following context from uploaded documents, please answer the user's question. If the context doesn't contain relevant information, please indicate that and provide a general response.

Context:
{rag_context}

Question: {processed_message}"""
                processed_message = augmented_message
                print(f"📚 Using RAG context: {len(rag_context)} characters from documents")
        
        # Step 2: Initialize streaming components
        def metrics_callback(metric_name: str, value):
            streaming_metrics.update_metric(metric_name, value)
        
        streamer = EnhancedLLMStreamer(tokenizer, metrics_callback)
        
        # Step 3: Execute generation in separate thread
        def generation_worker():
            success = execute_generation(processed_message, streamer, generation_params)
            elapsed_time = time.time() - request_start_time
            streaming_metrics.end_request(success, elapsed_time)
        
        generation_thread = threading.Thread(target=generation_worker, daemon=True)
        generation_thread.start()
        
        # Step 4: Stream response to UI
        yield from stream_response_to_history(streamer, updated_history)
        
        # Step 5: Wait for generation completion with timeout
        generation_thread.join(timeout=30.0)
        if generation_thread.is_alive():
            print("⚠️ Generation timeout - thread still running")
        
        print(f"📊 Request complete: {time.time() - request_start_time:.2f}s total")
        
    except Exception as e:
        elapsed_time = time.time() - request_start_time
        streaming_metrics.end_request(False, elapsed_time)
        error_history = handle_chat_error(e, history)
        yield error_history


def initialize_globals(pipeline, tokenizer_instance):
    """Initialize global pipeline and tokenizer instances"""
    global pipe, tokenizer
    pipe = pipeline
    tokenizer = tokenizer_instance
    
    # Initialize agent system if available
    if AGENT_AVAILABLE:
        try:
            from .agent import initialize_agent
            initialize_agent(pipeline, tokenizer_instance)
            print("✅ Agent system initialized with LLM pipeline")
        except Exception as e:
            print(f"⚠️ Agent initialization failed: {e}")

================================================================================

FILE: app\config.py
----------------------------------------
"""
Configuration Management Module
=============================

Handles configuration loading from multiple sources including JSON files,
environment variables, and command-line arguments.
"""

import json
import os
from typing import Any, Dict
from typing_extensions import TypedDict


# Type definitions
ConfigDict = Dict[str, Any]


class ConfigurationLoader:
    """Load and manage application configuration from multiple sources"""
    
    def __init__(self, config_file: str = "config.json") -> None:
        """
        Initialize configuration loader.
        
        Args:
            config_file: Path to JSON configuration file
        """
        self.config_file = config_file
        self._config: ConfigDict = {}
        self.load_configuration()
    
    def load_configuration(self) -> None:
        """Load configuration from file and environment variables"""
        # Load default configuration
        self._config = self._get_default_config()
        
        # Try to load from file
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    file_config = json.load(f)
                self._merge_config(file_config)
                print(f"✅ Loaded configuration from {self.config_file}")
            else:
                print(f"📝 Using default configuration (no {self.config_file} found)")
        except Exception as e:
            print(f"⚠️ Failed to load {self.config_file}: {e}")
            print("📝 Using default configuration")
        
        # Override with environment variables
        self._apply_env_overrides()
    
    def _get_default_config(self) -> ConfigDict:
        """Get default configuration values"""
        return {
            "model": {
                "path": "C:\\OpenVinoModels\\phi3-128k-npu",
                "name": "Phi-3-mini-128k-instruct",
                "type": "phi3"
            },
            "deployment": {
                "target_device": "NPU",
                "npu_profile": "balanced",
                "fallback_device": "CPU",
                "cache_directory": "./cache/.ovcache_qwen3"
            },
            "generation": {
                "max_new_tokens": 1024,
                "temperature": 0.6,
                "top_p": 0.95,
                "top_k": 20,
                "repetition_penalty": 1.1,
                "do_sample": True
            },
            "ui": {
                "max_message_length": 400,
                "max_conversation_tokens": 1800,
                "emergency_limit": 2048,
                "show_performance_metrics": True,
                "theme": "soft"
            },
            "performance": {
                "generation_timeout": 30.0,
                "truncation_warning_delay": 0.5,
                "ui_update_interval": 0.1
            }
        }
    
    def _merge_config(self, new_config: ConfigDict) -> None:
        """Merge new configuration with existing configuration"""
        def merge_dict(base: Dict[str, Any], update: Dict[str, Any]) -> None:
            for key, value in update.items():
                if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                    merge_dict(base[key], value)
                else:
                    base[key] = value
        
        merge_dict(self._config, new_config)
    
    def _apply_env_overrides(self) -> None:
        """Apply environment variable overrides"""
        env_mappings = {
            "QWEN3_MODEL_PATH": ("model", "path"),  # Backward compatibility
            "MODEL_PATH": ("model", "path"),        # Generic name for any model
            "TARGET_DEVICE": ("deployment", "target_device"),
            "NPU_PROFILE": ("deployment", "npu_profile"),
            "CACHE_DIR": ("deployment", "cache_directory"),
            "MAX_MESSAGE_LENGTH": ("ui", "max_message_length"),
            "GENERATION_TIMEOUT": ("performance", "generation_timeout")
        }
        
        for env_var, (section, key) in env_mappings.items():
            value = os.getenv(env_var)
            if value is not None:
                # Type conversion based on key
                if key in ["max_message_length"]:
                    value = int(value)
                elif key in ["generation_timeout"]:
                    value = float(value)
                elif key in ["show_performance_metrics", "do_sample"]:
                    value = value.lower() in ('true', '1', 'yes', 'on')
                
                self._config[section][key] = value
                print(f"🔧 Environment override: {env_var} = {value}")
    
    def update_from_args(self, args) -> None:
        """
        Update configuration from command-line arguments.
        
        Args:
            args: Parsed arguments from argparse
        """
        if hasattr(args, 'model_path') and args.model_path:
            self._config['model']['path'] = args.model_path
            
        if hasattr(args, 'device') and args.device:
            self._config['deployment']['target_device'] = args.device
            
        if hasattr(args, 'npu_profile') and args.npu_profile:
            self._config['deployment']['npu_profile'] = args.npu_profile
            
        if hasattr(args, 'cache_dir') and args.cache_dir:
            self._config['deployment']['cache_directory'] = args.cache_dir
    
    def get(self, section: str, key: str, default: Any = None) -> Any:
        """Get configuration value"""
        return self._config.get(section, {}).get(key, default)
    
    def get_section(self, section: str) -> Dict[str, Any]:
        """Get entire configuration section"""
        return self._config.get(section, {})
    
    @property
    def config(self) -> ConfigDict:
        """Get full configuration"""
        return self._config.copy()


# Global configuration instance - will be initialized by main.py
config: ConfigurationLoader = None


def initialize_config(config_file: str = "config.json", args=None) -> ConfigurationLoader:
    """
    Initialize global configuration instance.
    
    Args:
        config_file: Path to configuration file
        args: Command-line arguments from argparse
        
    Returns:
        Initialized ConfigurationLoader instance
    """
    global config
    config = ConfigurationLoader(config_file)
    
    if args:
        config.update_from_args(args)
    
    return config


def get_config() -> ConfigurationLoader:
    """Get the global configuration instance"""
    if config is None:
        raise RuntimeError("Configuration not initialized. Call initialize_config() first.")
    return config

================================================================================

FILE: app\model.py
----------------------------------------
"""
Model Deployment and System Initialization
=========================================

Handles OpenVINO GenAI pipeline deployment with comprehensive error handling,
NPU optimization, and system validation.
"""

import os
import time
from typing import Any, Tuple, List
from typing_extensions import Literal

import openvino_genai as ov_genai
from transformers import AutoTokenizer

from .config import get_config

# Try to import OpenVINO properties with fallback
try:
    import openvino.properties as props
    import openvino.properties.hint as hints
    OPENVINO_PROPERTIES_AVAILABLE = True
    print("✅ OpenVINO properties imported successfully")
except ImportError as e:
    print(f"⚠️ OpenVINO properties not available: {e}")
    print("🔄 Using fallback configuration...")
    OPENVINO_PROPERTIES_AVAILABLE = False
    
    # Create mock objects for compatibility
    class MockHints:
        class PerformanceMode:
            LATENCY = "LATENCY"
            THROUGHPUT = "THROUGHPUT"
    
    class MockProps:
        class cache_dir:
            pass
        class streams:
            class num:
                pass
        class inference_num_threads:
            pass
    
    hints = MockHints()
    props = MockProps()

# Import enhanced context patterns
import sys
context_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "context")
sys.path.insert(0, context_path)

# Import Qwen3-specific optimizations
try:
    from qwen3_model_context.npu_optimization import (
        Qwen3NPUConfigBuilder, 
        Qwen3NPUDeployment,
        QWEN3_NPU_PROFILES
    )
    from qwen3_model_context.model_architecture import (
        QWEN3_8B_ARCHITECTURE,
        initialize_qwen3_pipeline
    )
    ENHANCED_CONTEXT_AVAILABLE = True
    print("✅ Enhanced NPU context loaded successfully")
except ImportError as e:
    print(f"⚠️ Enhanced context not available: {e}")
    print("📝 Using fallback patterns - consider updating context path")
    ENHANCED_CONTEXT_AVAILABLE = False


# Type definitions
DeviceType = Literal["NPU", "CPU", "GPU", "AUTO"]
ProfileType = Literal["conservative", "balanced", "aggressive"]
ConfigDict = dict[str, Any]


class LLMConfigurationManager:
    """Advanced configuration management with model-specific optimization (Phi-3 compatible)"""
    
    def __init__(self, profile: ProfileType = "balanced") -> None:
        """
        Initialize configuration manager with specified profile.
        
        Args:
            profile: NPU optimization profile (conservative, balanced, aggressive)
        """
        self.profile = profile
        self.config_builder: Any = None
        
        if ENHANCED_CONTEXT_AVAILABLE:
            self.config_builder = Qwen3NPUConfigBuilder(profile)
    
    def get_npu_config(self) -> ConfigDict:
        """
        Get complete NPU configuration with NPUW optimization.
        
        Returns:
            Dictionary containing NPU-specific configuration parameters
        """
        if ENHANCED_CONTEXT_AVAILABLE and self.config_builder:
            # Use enhanced Qwen3-specific configuration
            return self.config_builder.build_complete_config()
        else:
            # Fallback configuration optimized for Phi-3 128k context
            config = {
                "NPU_USE_NPUW": "YES",
                "NPUW_LLM": "YES", 
                "NPUW_LLM_BATCH_DIM": 0,
                "NPUW_LLM_SEQ_LEN_DIM": 1,
                "NPUW_LLM_MAX_PROMPT_LEN": 8192,  # Increased for Phi-3 128k context
                "NPUW_LLM_MIN_RESPONSE_LEN": 512,  # Increased for better responses
                "CACHE_MODE": "OPTIMIZE_SPEED",
                "NPUW_LLM_PREFILL_HINT": "BEST_PERF",
                "NPUW_LLM_GENERATE_HINT": "BEST_PERF"
            }
            
            # Add OpenVINO properties if available (no generic PERFORMANCE_HINT for NPU)
            if OPENVINO_PROPERTIES_AVAILABLE:
                cache_dir = get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3")
                config.update({
                    props.cache_dir: cache_dir
                })
            else:
                config.update({
                    "CACHE_DIR": get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3")
                })
            
            return config
    
    def get_cpu_config(self) -> ConfigDict:
        """
        Get optimized CPU configuration.
        
        Returns:
            Dictionary containing CPU-specific configuration parameters
        """
        if ENHANCED_CONTEXT_AVAILABLE and self.config_builder:
            return self.config_builder.build_complete_config()
        else:
            config = {
                "MAX_PROMPT_LEN": 16384,  # Much larger context for Phi-3 on CPU
                "MIN_RESPONSE_LEN": 512
            }
            
            # Add OpenVINO properties if available
            if OPENVINO_PROPERTIES_AVAILABLE:
                cache_dir = get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3") + "_cpu"
                config.update({
                    hints.performance_mode: hints.PerformanceMode.THROUGHPUT,
                    props.cache_dir: cache_dir,
                    props.streams.num: 2,
                    props.inference_num_threads: 0  # Auto-detect
                })
            else:
                config.update({
                    "PERFORMANCE_HINT": "THROUGHPUT",
                    "CACHE_DIR": get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3") + "_cpu",
                    "NUM_STREAMS": 2,
                    "INFERENCE_NUM_THREADS": 0  # Auto-detect
                })
            
            return config


def deploy_llm_pipeline(
    model_path: str, 
    target_device: DeviceType, 
    profile: ProfileType = "balanced"
) -> Tuple[Any, str, str, float]:
    """
    Deploy language model pipeline (Phi-3) with comprehensive error handling and optimization.
    
    Args:
        model_path: Path to the Phi-3 OpenVINO model directory
        target_device: Target device for deployment (NPU, CPU, GPU, AUTO)
        profile: NPU optimization profile
        
    Returns:
        Tuple of (pipeline, device_used, config_used, load_time)
        
    Raises:
        RuntimeError: If all deployment configurations fail
    """
    load_start_time = time.time()
    
    if ENHANCED_CONTEXT_AVAILABLE:
        print(f"🚀 Deploying Phi-3 with enhanced NPU context (profile: {profile})")
        
        # Use enhanced deployment (NPU optimization patterns from context)
        deployment = Qwen3NPUDeployment(model_path, profile)
        pipeline = deployment.deploy()
        
        if pipeline:
            load_time = time.time() - load_start_time
            return pipeline, target_device, f"enhanced_{profile}", load_time
        else:
            print("⚠️ Enhanced deployment failed, falling back to manual configuration")
    
    # Fallback to manual configuration
    print(f"🔄 Using manual pipeline deployment (target: {target_device})")
    
    config_manager = LLMConfigurationManager(profile)
    
    configurations = []
    
    # Create basic configurations with compatibility handling
    cache_dir = get_config().get("deployment", "cache_directory", "./cache/.ovcache_phi3")
    
    if OPENVINO_PROPERTIES_AVAILABLE:
        basic_npu_config = {hints.performance_mode: hints.PerformanceMode.LATENCY, props.cache_dir: cache_dir}
        basic_cpu_config = {hints.performance_mode: hints.PerformanceMode.THROUGHPUT, props.cache_dir: cache_dir}
    else:
        basic_npu_config = {"PERFORMANCE_HINT": "LATENCY", "CACHE_DIR": cache_dir}
        basic_cpu_config = {"PERFORMANCE_HINT": "THROUGHPUT", "CACHE_DIR": cache_dir}
    
    if target_device == "NPU":
        configurations = [
            ("enhanced_npu_phi3", target_device, config_manager.get_npu_config()),
            ("basic_npu", target_device, basic_npu_config),
            ("minimal_npu", target_device, {}),
            ("cpu_fallback", "CPU", config_manager.get_cpu_config())
        ]
    else:
        configurations = [
            ("optimized_cpu_phi3", target_device, config_manager.get_cpu_config()),
            ("basic_cpu", target_device, basic_cpu_config),
            ("minimal_cpu", target_device, {})
        ]
    
    for config_name, device, config in configurations:
        try:
            print(f"🔄 Trying {device} with {config_name} configuration...")
            
            if ENHANCED_CONTEXT_AVAILABLE:
                # Use enhanced initialization if available
                pipeline = initialize_qwen3_pipeline(model_path, device, **config)
            else:
                # Fallback initialization
                if config:
                    pipeline = ov_genai.LLMPipeline(model_path, device, **config)
                else:
                    pipeline = ov_genai.LLMPipeline(model_path, device)
                
            load_time = time.time() - load_start_time
            print(f"✅ Success: {device} with {config_name} ({load_time:.1f}s)")
            return pipeline, device, config_name, load_time
            
        except Exception as e:
            print(f"⚠️ {config_name} failed: {e}")
            continue
    
    raise RuntimeError("All configurations failed. Check model path, device drivers, and NPUW configuration.")


def validate_system_requirements() -> List[str]:
    """Validate system requirements and return list of issues."""
    issues = []
    
    config = get_config()
    model_path = config.get("model", "path")
    target_device = config.get("deployment", "target_device")
    cache_dir = config.get("deployment", "cache_directory")
    
    # Check model path
    if not os.path.exists(model_path):
        issues.append(f"Model path does not exist: {model_path}")
    elif not os.path.isdir(model_path):
        issues.append(f"Model path is not a directory: {model_path}")
    else:
        # Check for required OpenVINO files
        required_files = ['openvino_model.xml', 'openvino_model.bin']
        for file_name in required_files:
            if not os.path.exists(os.path.join(model_path, file_name)):
                issues.append(f"Missing OpenVINO model file: {file_name}")
    
    # Check cache directory
    cache_parent = os.path.dirname(cache_dir)
    if not os.path.exists(cache_parent):
        try:
            os.makedirs(cache_parent, exist_ok=True)
        except PermissionError:
            issues.append(f"Cannot create cache directory: {cache_parent} (permission denied)")
        except Exception as e:
            issues.append(f"Cannot create cache directory: {cache_parent} ({str(e)})")
    
    # Check OpenVINO installation
    try:
        import openvino as ov
        core = ov.Core()
        available_devices = core.available_devices
        if target_device not in available_devices and target_device != "AUTO":
            issues.append(f"Target device '{target_device}' not available. Available: {available_devices}")
    except Exception as e:
        issues.append(f"OpenVINO not properly installed: {str(e)}")
    
    return issues


def initialize_system_with_validation():
    """Initialize system with comprehensive validation and error handling."""
    config = get_config()
    
    print("🔍 Validating system requirements...")
    issues = validate_system_requirements()
    
    if issues:
        print("❌ System validation failed:")
        for i, issue in enumerate(issues, 1):
            print(f"   {i}. {issue}")
        print("\n🔧 Suggested fixes:")
        print("   • Set MODEL_PATH environment variable to correct model location")
        print("   • Install OpenVINO with: pip install openvino")
        print("   • For NPU: Install Intel NPU drivers from official site")
        print("   • Ensure model is in OpenVINO format (.xml/.bin files)")
        raise SystemExit(1)
    
    try:
        print("🚀 Initializing Enhanced Phi-3 Chat System...")
        
        # Get configuration values
        model_path = config.get("model", "path")
        target_device = config.get("deployment", "target_device", "NPU")
        npu_profile = config.get("deployment", "npu_profile", "balanced")
        
        print(f"📂 Model: {model_path}")
        print(f"🎯 Target Device: {target_device}")
        print(f"📊 Optimization Profile: {npu_profile}")
        print(f"🔧 Enhanced Context: {'Available' if ENHANCED_CONTEXT_AVAILABLE else 'Fallback Mode'}")
        
        # Deploy pipeline with comprehensive error handling
        pipeline, device_used, config_used, load_time = deploy_llm_pipeline(
            model_path, target_device, npu_profile
        )
        
        # Initialize tokenizer with error handling
        print("📚 Loading Phi-3 tokenizer...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            
            # Configure tokenizer for Phi-3
            if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
                tokenizer.pad_token_id = tokenizer.eos_token_id
                
        except Exception as tokenizer_error:
            print(f"⚠️ Tokenizer loading failed: {tokenizer_error}")
            print("🔄 Attempting fallback tokenizer initialization...")
            try:
                # Fallback: try without trust_remote_code
                tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=False)
                if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
                    tokenizer.pad_token_id = tokenizer.eos_token_id
                print("✅ Fallback tokenizer loaded successfully")
            except Exception as fallback_error:
                print(f"❌ Fallback tokenizer also failed: {fallback_error}")
                raise RuntimeError("Unable to initialize tokenizer with any method") from fallback_error
        
        print(f"✅ System Ready!")
        print(f"   Device: {device_used}")
        print(f"   Config: {config_used}")
        print(f"   Load Time: {load_time:.1f}s")
        print(f"   Model Path: {model_path}")
        print(f"   Tokenizer: {tokenizer.__class__.__name__}")
        if ENHANCED_CONTEXT_AVAILABLE:
            from qwen3_model_context.special_tokens import QWEN3_SPECIAL_TOKENS
            print(f"   Special Tokens: {len(QWEN3_SPECIAL_TOKENS)} special tokens available (NPU context)")
        print("=" * 60)
        
        return pipeline, tokenizer, device_used, config_used, load_time
        
    except Exception as e:
        print(f"❌ System initialization failed: {e}")
        print("\n🔧 Detailed diagnostics:")
        print(f"   Model Path: {config.get('model', 'path')}")
        print(f"   Target Device: {config.get('deployment', 'target_device')}")
        print(f"   Cache Directory: {config.get('deployment', 'cache_directory')}")
        print(f"   Enhanced Context: {ENHANCED_CONTEXT_AVAILABLE}")
        
        # Provide specific guidance based on error type
        error_str = str(e).lower()
        if "compile" in error_str:
            print("\n💡 NPU Compilation Error - Try:")
            print("   • Verify NPU drivers are installed")
            print("   • Check NPUW configuration compatibility")
            print("   • Try CPU fallback with: --device CPU")
        elif "file" in error_str or "path" in error_str:
            print("\n💡 File/Path Error - Try:")
            print("   • Verify model path contains .xml and .bin files")
            print("   • Check file permissions and access rights")
        elif "memory" in error_str:
            print("\n💡 Memory Error - Try:")
            print("   • Use conservative NPU profile")
            print("   • Ensure sufficient system RAM")
            print("   • Close other applications")
        
        raise SystemExit(1)

================================================================================

FILE: app\streamer.py
----------------------------------------
"""
Token Streaming and Filtering
============================

Enhanced streaming implementation with Phi-3-specific token filtering,
performance monitoring, and robust error handling.
"""

import time
import queue
from typing import Optional
import openvino_genai as ov_genai
from transformers import AutoTokenizer

# Import enhanced context patterns
import os
import sys
context_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "context")
sys.path.insert(0, context_path)

# Import Qwen3-specific filtering
try:
    from qwen3_model_context.special_tokens import (
        Qwen3StreamingFilter,
        QWEN3_SPECIAL_TOKENS
    )
    ENHANCED_CONTEXT_AVAILABLE = True
    print("✅ Enhanced Phi-3 token filtering loaded")
except ImportError:
    print("⚠️ Enhanced token filtering not available - using fallback")
    ENHANCED_CONTEXT_AVAILABLE = False


class EnhancedLLMStreamer(ov_genai.StreamerBase):
    """
    Production-ready streamer with Phi-3-specific optimizations:
    - Proper special token filtering (26+ tokens)
    - Performance monitoring
    - Robust error handling
    - Token-level streaming control
    """
    
    def __init__(self, tokenizer: AutoTokenizer, metrics_callback=None):
        """
        Initialize enhanced streamer.
        
        Args:
            tokenizer: HuggingFace tokenizer for decoding
            metrics_callback: Optional callback to update global metrics
        """
        super().__init__()
        self.tokenizer = tokenizer
        self.text_queue = queue.Queue()
        self.accumulated_tokens = []
        self.current_text = ""
        self.start_time = time.time()
        self.first_token_time = None
        self.tokens_generated = 0
        self.metrics_callback = metrics_callback
        
        # Initialize Qwen3-specific filtering
        if ENHANCED_CONTEXT_AVAILABLE:
            self.token_filter = Qwen3StreamingFilter()
            print("✅ Using enhanced Qwen3 token filtering")
        else:
            self.token_filter = None
            print("⚠️ Using basic token filtering")
    
    def put(self, token_id: int) -> bool:
        """Process token with Qwen3-specific filtering"""
        self.accumulated_tokens.append(token_id)
        self.tokens_generated += 1
        
        # Record first token latency
        if self.first_token_time is None:
            self.first_token_time = time.time()
        
        try:
            # Decode with special token handling
            if ENHANCED_CONTEXT_AVAILABLE and self.token_filter:
                # Use enhanced filtering
                try:
                    token_text = self.tokenizer.decode([token_id], skip_special_tokens=False)
                except:
                    token_text = f"[UNK_{token_id}]"
                
                # Process through Qwen3 filter
                display_text = self.token_filter.process_token(token_id, token_text)
                
                if display_text is not None:
                    self.current_text += display_text
                    self.text_queue.put(display_text)
                else:
                    # Token was filtered (special token)
                    if self.metrics_callback:
                        self.metrics_callback("special_tokens_filtered", 1)
                    
            else:
                # Fallback filtering
                try:
                    # Decode incrementally
                    full_text = self.tokenizer.decode(self.accumulated_tokens, skip_special_tokens=True)
                    
                    if len(full_text) > len(self.current_text):
                        new_text = full_text[len(self.current_text):]
                        self.current_text = full_text
                        
                        # Basic special token filtering
                        if not self._is_special_token_text(new_text):
                            self.text_queue.put(new_text)
                except Exception as e:
                    print(f"⚠️ Decoding error: {e}")
                    return False
                    
        except Exception as e:
            print(f"❌ Token processing error: {e}")
            return False
        
        return False  # Continue generation
    
    def _is_special_token_text(self, text: str) -> bool:
        """Basic special token detection for fallback"""
        special_patterns = [
            '<|im_start|>', '<|im_end|>', '<|endoftext|>',
            '<think>', '</think>', '<tool_call>', '</tool_call>',
            '<|system|>', '<|user|>', '<|assistant|>'
        ]
        
        for pattern in special_patterns:
            if pattern in text:
                if self.metrics_callback:
                    self.metrics_callback("special_tokens_filtered", 1)
                return True
        
        return False
    
    def end(self):
        """Finalize streaming and calculate performance metrics"""
        # Calculate performance metrics
        total_time = time.time() - self.start_time
        first_token_latency = (self.first_token_time - self.start_time) if self.first_token_time else 0
        tokens_per_second = self.tokens_generated / total_time if total_time > 0 else 0
        
        # Update global metrics via callback
        if self.metrics_callback:
            self.metrics_callback("first_token_latency", first_token_latency)
            self.metrics_callback("tokens_per_second", tokens_per_second)
            self.metrics_callback("total_tokens_generated", self.tokens_generated)
        
        # Log performance
        print(f"🚀 Generation complete: {self.tokens_generated} tokens in {total_time:.2f}s")
        print(f"   First token: {first_token_latency:.3f}s, Speed: {tokens_per_second:.1f} tok/s")
        
        if ENHANCED_CONTEXT_AVAILABLE and self.token_filter:
            thinking_content = self.token_filter.get_thinking_content()
            if thinking_content.strip():
                print(f"🧠 Model thinking: {thinking_content[:100]}...")
        
        # Signal end of generation
        self.text_queue.put(None)
    
    def __iter__(self):
        return self
    
    def __next__(self):
        item = self.text_queue.get()
        if item is None:
            raise StopIteration
        return item


class StreamingMetrics:
    """Simple metrics tracking for streaming performance"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """Reset all metrics"""
        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.avg_response_time = 0.0
        self.avg_first_token_latency = 0.0
        self.avg_tokens_per_second = 0.0
        self.total_tokens_generated = 0
        self.special_tokens_filtered = 0
        self.session_start_time = time.time()
    
    def update_metric(self, metric_name: str, value):
        """Update a specific metric"""
        if metric_name == "special_tokens_filtered":
            self.special_tokens_filtered += value
        elif metric_name == "total_tokens_generated":
            self.total_tokens_generated += value
        elif metric_name == "first_token_latency":
            # Calculate running average
            if self.successful_requests > 0:
                self.avg_first_token_latency = (
                    (self.avg_first_token_latency * (self.successful_requests - 1) + value)
                    / self.successful_requests
                )
            else:
                self.avg_first_token_latency = value
        elif metric_name == "tokens_per_second":
            # Calculate running average
            if self.successful_requests > 0:
                self.avg_tokens_per_second = (
                    (self.avg_tokens_per_second * (self.successful_requests - 1) + value)
                    / self.successful_requests
                )
            else:
                self.avg_tokens_per_second = value
    
    def start_request(self):
        """Mark the start of a new request"""
        self.total_requests += 1
    
    def end_request(self, success: bool, response_time: float):
        """Mark the end of a request"""
        if success:
            self.successful_requests += 1
        else:
            self.failed_requests += 1
        
        # Update average response time
        self.avg_response_time = (
            (self.avg_response_time * (self.total_requests - 1) + response_time)
            / self.total_requests
        )
    
    def get_summary(self) -> dict:
        """Get metrics summary"""
        session_duration = time.time() - self.session_start_time
        success_rate = (self.successful_requests / max(self.total_requests, 1)) * 100
        
        return {
            "Session Duration": f"{session_duration:.1f}s",
            "Total Requests": self.total_requests,
            "Success Rate": f"{success_rate:.1f}%",
            "Avg Response Time": f"{self.avg_response_time:.2f}s",
            "Avg First Token": f"{self.avg_first_token_latency:.3f}s",
            "Avg Tokens/Second": f"{self.avg_tokens_per_second:.1f}",
            "Total Tokens Generated": self.total_tokens_generated,
            "Special Tokens Filtered": self.special_tokens_filtered,
        }


# Global metrics instance
streaming_metrics = StreamingMetrics()

================================================================================

FILE: app\ui.py
----------------------------------------
"""
User Interface Module  
====================

Gradio interface creation with advanced features including dynamic system prompts,
RAG document upload, performance monitoring, and professional styling.
"""

import os
import gradio as gr
from typing import Dict, Any

from .config import get_config
from .chat import enhanced_llm_chat, rag_system, RAG_AVAILABLE

# Agent system import
try:
    from .agent import AGENT_AVAILABLE, get_agent
    print("✅ Agent UI components loaded")
except ImportError:
    AGENT_AVAILABLE = False
    get_agent = lambda: None
    print("⚠️ Agent UI components not available")
from .streamer import streaming_metrics

# Import enhanced context patterns
import sys
context_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "context")
sys.path.insert(0, context_path)

# Import Qwen3-specific optimizations
try:
    from qwen3_model_context.npu_optimization import QWEN3_NPU_PROFILES
    from qwen3_model_context.model_architecture import QWEN3_8B_ARCHITECTURE
    from qwen3_model_context.special_tokens import QWEN3_SPECIAL_TOKENS
    ENHANCED_CONTEXT_AVAILABLE = True
except ImportError:
    ENHANCED_CONTEXT_AVAILABLE = False
    QWEN3_NPU_PROFILES = {}
    QWEN3_8B_ARCHITECTURE = {}


# System prompt management
DEFAULT_SYSTEM_PROMPT = """You are a helpful, concise AI assistant powered by Phi-3-mini-128k-instruct running on Intel NPU via OpenVINO GenAI. 

Key behaviors:
- Provide accurate, well-structured responses
- Be concise but comprehensive 
- Use clear formatting when helpful
- Acknowledge when you're uncertain
- Optimize for NPU constraints (prefer shorter, focused responses)

You excel at: reasoning, coding, analysis, creative writing, and technical explanations."""

# Current system prompt (can be modified by user)
SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT

# Global references (will be set by main.py)
pipe = None
device_used = "NPU"
config_used = "enhanced"
load_time = 0.0


def create_enhanced_interface():
    """Create production-ready Gradio interface with advanced features"""
    
    config = get_config()
    
    # Custom CSS for professional appearance
    custom_css = """
    .gradio-container { max-width: 1400px; margin: auto; }
    .chatbot { height: 650px; }
    .metrics-panel { 
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        padding: 15px; 
        border-radius: 8px; 
        border: 1px solid #dee2e6;
    }
    .system-info {
        background: #f8f9fa;
        padding: 10px;
        border-radius: 6px;
        border-left: 4px solid #28a745;
        margin: 10px 0;
    }
    .warning-banner {
        background: #fff3cd;
        padding: 8px;
        border-radius: 4px;
        border-left: 4px solid #ffc107;
        margin: 5px 0;
    }
    """
    
    with gr.Blocks(
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="gray",
            neutral_hue="slate"
        ),
        title="Enhanced Phi-3 Chat",
        css=custom_css,
    ) as demo:
        
        # Header with system status
        with gr.Row():
            with gr.Column(scale=3):
                gr.Markdown(f"""
                # 🤖 Enhanced Phi-3 Chat System
                
                **Production-Ready Implementation with Complete Optimization**
                """)
            
            with gr.Column(scale=2, elem_classes=["system-info"]):
                system_status = gr.Markdown(f"""
                **Device**: {device_used} | **Config**: {config_used}  
                **Model**: Phi-3-mini-128k-instruct INT4 | **Load Time**: {load_time:.1f}s  
                **Enhanced Context**: {'✅ Active' if ENHANCED_CONTEXT_AVAILABLE else '⚠️ Fallback'}  
                **Profile**: {config.get('deployment', 'npu_profile', 'balanced').title()}
                """)
        
        # Warning banner if fallback mode
        if not ENHANCED_CONTEXT_AVAILABLE:
            gr.Markdown("""
            <div class="warning-banner">
            ⚠️ <strong>Fallback Mode</strong>: Enhanced context not loaded. Some optimizations may be limited.
            </div>
            """)
        
        # Main chat interface using official ChatInterface pattern
        chatbot = gr.Chatbot(
            label=f"Conversation (Phi-3-mini-128k on {device_used})",
            height=650,
            type='messages',
            avatar_images=(None, "🤖"),
            show_copy_button=True,
            show_share_button=False,
            bubble_full_width=False,
            render_markdown=True
        )
        
        # System prompt control
        with gr.Accordion("🎯 System Prompt Configuration", open=False):
            system_prompt_input = gr.Textbox(
                value=SYSTEM_PROMPT,
                lines=6,
                label="System Prompt",
                placeholder="Configure the AI's behavior and persona...",
                interactive=True,
                info="This prompt sets the AI's behavior, expertise, and response style. Changes take effect after clearing the chat."
            )
            
            with gr.Row():
                reset_prompt_btn = gr.Button("🔄 Reset to Default", size="sm")
                apply_prompt_btn = gr.Button("✅ Apply & Clear Chat", variant="primary", size="sm")
        
        # Document upload for RAG
        with gr.Accordion("📚 Document Upload (RAG)", open=False):
            with gr.Row():
                with gr.Column(scale=3):
                    # Dynamic file types based on parsing capabilities
                    if rag_system.advanced_parsing:
                        supported_types = [".txt", ".md", ".py", ".js", ".html", ".css", ".json", ".pdf", ".docx", ".pptx"]
                        upload_label = "Upload Documents (Advanced Parsing Enabled)"
                    else:
                        supported_types = [".txt", ".md", ".py", ".js", ".html", ".css", ".json"]
                        upload_label = "Upload Documents (Basic Text Processing)"
                    
                    file_upload = gr.File(
                        label=upload_label,
                        file_types=supported_types,
                        file_count="multiple",
                        interactive=True
                    )
                    
                with gr.Column(scale=2):
                    upload_status = gr.Textbox(
                        label="Upload Status",
                        interactive=False,
                        placeholder="No documents uploaded"
                    )
            
            with gr.Row():
                clear_docs_btn = gr.Button("🗑️ Clear Documents", variant="secondary", size="sm")
                rag_status_btn = gr.Button("📊 RAG Status", size="sm")
            
            # Dynamic check for RAG availability 
            try:
                try:
                    from langchain_huggingface import HuggingFaceEmbeddings
                except ImportError:
                    from langchain_community.embeddings import HuggingFaceEmbeddings
                rag_available_now = True
            except ImportError:
                rag_available_now = False
            
            if not rag_available_now:
                gr.Markdown("""
                ⚠️ **RAG not available**: Install dependencies with:
                ```bash
                # Basic RAG functionality
                pip install langchain faiss-cpu sentence-transformers
                
                # Advanced features (Phase 3)
                pip install unstructured[local-inference] langchain-unstructured torch
                ```
                """)

        # Agent Tools Section (Phase 3.3)
        with gr.Accordion("🤖 AI Agent Tools", open=False):
            if AGENT_AVAILABLE:
                agent_tools_display = gr.Markdown("""
                **Available Agent Tools:**
                
                🧮 **Calculator**: Perform mathematical calculations  
                📅 **Date/Time**: Get current time, dates, and calculate date differences  
                🔍 **Web Search**: Search for information (mock implementation)  
                📊 **Text Analysis**: Analyze text for word count, readability, etc.
                
                **Usage**: Simply ask naturally! The agent will automatically use tools when needed.
                
                **Examples:**
                - "What's 25 * 4 + 10?"
                - "What time is it?"
                - "Analyze this text: [your text]"
                - "Search for information about AI"
                """)
                
                with gr.Row():
                    agent_status_btn = gr.Button("🤖 Agent Status", variant="secondary", size="sm")
                    agent_tools_btn = gr.Button("🛠️ List Tools", variant="secondary", size="sm")
                
                agent_status_output = gr.Textbox(
                    label="Agent System Status",
                    interactive=False,
                    visible=False,
                    max_lines=5
                )
            else:
                gr.Markdown("""
                ⚠️ **Agent system not available**: Install dependencies with:
                ```bash
                pip install langchain-core langchain-experimental requests python-dateutil
                ```
                
                Once installed, the AI will be able to use tools like calculators, web search, 
                and text analysis automatically based on your requests.
                """)

        # Advanced Generation Settings
        with gr.Accordion("⚙️ Advanced Generation Settings", open=False):
            with gr.Row():
                with gr.Column(scale=1):
                    temperature_slider = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        step=0.1,
                        value=config.get('generation', 'temperature', 0.6),
                        label="Temperature",
                        info="Controls randomness (0.1=focused, 2.0=creative)"
                    )
                    
                with gr.Column(scale=1):
                    top_p_slider = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        step=0.05,
                        value=config.get('generation', 'top_p', 0.95),
                        label="Top-p (Nucleus Sampling)",
                        info="Cumulative probability threshold"
                    )
                
                with gr.Column(scale=1):
                    max_tokens_number = gr.Number(
                        minimum=50,
                        maximum=2048,
                        step=50,
                        value=config.get('generation', 'max_new_tokens', 512),
                        label="Max New Tokens",
                        info="Maximum tokens to generate"
                    )
            
            # Generation settings controls
            with gr.Row():
                reset_gen_settings_btn = gr.Button("🔄 Reset to Defaults", variant="secondary", size="sm")
                apply_gen_settings_btn = gr.Button("✅ Apply Settings", variant="primary", size="sm")
            
            # Current generation settings display
            gen_settings_status = gr.Textbox(
                value=f"Temperature: {config.get('generation', 'temperature', 0.6)}, Top-p: {config.get('generation', 'top_p', 0.95)}, Max tokens: {config.get('generation', 'max_new_tokens', 512)}",
                label="Current Settings",
                interactive=False,
                max_lines=1
            )

        # Input controls
        with gr.Row():
            msg_input = gr.Textbox(
                placeholder=f"Chat with Phi-3 on {device_used} (max {config.get('ui', 'max_message_length', 2000)} chars)...",
                scale=7,
                max_lines=4,
                show_label=False,
                container=False
            )
            
            with gr.Column(scale=1):
                send_btn = gr.Button("💬 Send", variant="primary", size="lg")
                clear_btn = gr.Button("🗑️ Clear", variant="secondary", size="sm")
        
        # Advanced controls panel
        with gr.Row():
            with gr.Column(scale=2):
                metrics_btn = gr.Button("📊 Performance Metrics", variant="secondary")
                system_btn = gr.Button("ℹ️ System Info", variant="secondary")
            
            with gr.Column(scale=2):
                if ENHANCED_CONTEXT_AVAILABLE and QWEN3_NPU_PROFILES:
                    profile_selector = gr.Dropdown(
                        choices=list(QWEN3_NPU_PROFILES.keys()),
                        value=config.get('deployment', 'npu_profile', 'balanced'),
                        label="NPU Profile",
                        interactive=False  # Would need restart to change
                    )
                
                reset_metrics_btn = gr.Button("🔄 Reset Metrics", variant="secondary")
        
        # Collapsible metrics panel
        with gr.Row(visible=False) as metrics_row:
            with gr.Column(elem_classes=["metrics-panel"]):
                gr.Markdown("### 📊 Real-time Performance Metrics")
                metrics_json = gr.JSON(label="System Metrics", container=True)
                
                if ENHANCED_CONTEXT_AVAILABLE:
                    gr.Markdown("### 🎯 Model-Specific Stats")
                    phi3_stats = gr.JSON(label="Token Filtering & Processing", container=True)
        
        # Examples section
        with gr.Row():
            gr.Examples(
                examples=[
                    "Explain quantum computing in simple terms",
                    "Write a Python function to implement quicksort", 
                    "What are the advantages of using Intel NPU for AI inference?",
                    "Compare different neural network architectures",
                    "Help me debug this code: def factorial(n): return n * factorial(n)",
                    "Explain the concept of attention in transformer models",
                    "What does the uploaded document say about...?",
                    "Summarize the key points from the uploaded files",
                    # Agent examples
                    "What's 15 * 23 + 47?",
                    "What time is it right now?", 
                    "Calculate the square root of 144",
                    "Analyze this text: The quick brown fox jumps over the lazy dog",
                    "What's tomorrow's date?"
                ] if AGENT_AVAILABLE else [
                    "Explain quantum computing in simple terms",
                    "Write a Python function to implement quicksort",
                    "What are the advantages of using Intel NPU for AI inference?",
                    "Compare different neural network architectures", 
                    "Help me debug this code: def factorial(n): return n * factorial(n)",
                    "Explain the concept of attention in transformer models",
                    "What does the uploaded document say about...?",
                    "Summarize the key points from the uploaded files"
                ],
                inputs=msg_input,
                label="💡 Example Questions (Upload documents for context-aware answers)"
            )
        
        # Global generation settings storage
        generation_settings = {
            'temperature': config.get('generation', 'temperature', 0.6),
            'top_p': config.get('generation', 'top_p', 0.95),
            'max_new_tokens': config.get('generation', 'max_new_tokens', 512)
        }
        
        # Event handlers with enhanced functionality
        def handle_send(message, history):
            """Handle send with proper session management"""
            return enhanced_llm_chat(message, history, generation_settings)
        
        def handle_clear(current_system_prompt):
            """Handle clear with proper session reset"""
            global SYSTEM_PROMPT
            try:
                # Update global system prompt if changed
                if current_system_prompt.strip():
                    SYSTEM_PROMPT = current_system_prompt.strip()
                else:
                    SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT
                
                # End current session and start new one
                pipe.finish_chat()
                pipe.start_chat(SYSTEM_PROMPT)
                print("🔄 Chat session reset with updated system prompt")
                return [], "", SYSTEM_PROMPT
            except Exception as e:
                print(f"⚠️ Session reset error: {e}")
                return [], "", current_system_prompt
        
        def show_metrics():
            """Display comprehensive performance metrics"""
            base_metrics = streaming_metrics.get_summary()
            
            qwen3_specific = {}
            if ENHANCED_CONTEXT_AVAILABLE:
                qwen3_specific = {
                    "Enhanced Features": "Active",
                    "NPUW Profile": config.get('deployment', 'npu_profile', 'balanced'),
                    "Model Architecture": f"Phi-3-mini-128k-instruct",
                    "Max Context": f"{QWEN3_8B_ARCHITECTURE.get('max_position_embeddings', 40960):,} tokens",
                    "Special Tokens Available": len(QWEN3_SPECIAL_TOKENS) if 'QWEN3_SPECIAL_TOKENS' in globals() else 0
                }
            else:
                qwen3_specific = {
                    "Enhanced Features": "Fallback Mode",
                    "Note": "Install enhanced context for full optimization"
                }
            
            return (
                gr.update(value=base_metrics, visible=True),
                gr.update(value=qwen3_specific, visible=True) if ENHANCED_CONTEXT_AVAILABLE else gr.update(visible=False),
                gr.update(visible=True)
            )
        
        def show_system_info():
            """Display comprehensive system information"""
            config = get_config()
            model_path = config.get("model", "path")
            cache_dir = config.get("deployment", "cache_directory")
            npu_profile = config.get("deployment", "npu_profile", "balanced")
            
            info_text = f"""
            ## 🖥️ System Configuration
            
            **Hardware & Device:**
            - Target Device: {device_used}
            - Configuration: {config_used}
            - Cache Directory: `{cache_dir}`
            - NPU Profile: {npu_profile}
            
            **Model Details:**
            - Model: Phi-3-mini-128k-instruct INT4 Quantized
            - Path: `{model_path}`
            - Load Time: {load_time:.1f} seconds
            - Tokenizer: HuggingFace AutoTokenizer
            
            **OpenVINO GenAI Configuration:**
            - API Mode: Stateful (start_chat/finish_chat)
            - Conversation Management: Automatic KV-cache
            - Token Limits: {config.get('ui', 'max_conversation_tokens', 1800)} (conversation), {config.get('ui', 'max_message_length', 400)} (message)
            - Generation: Temperature={config.get('generation', 'temperature', 0.6)}, Top-p={config.get('generation', 'top_p', 0.95)}
            
            **Enhanced Features:**
            {"✅ Complete Phi-3 NPUW optimization" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Basic NPUW configuration"}
            {"✅ Advanced special token filtering" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Basic token filtering"}
            {"✅ Phi-3-specific optimizations" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Standard templates"}
            {"✅ Advanced performance monitoring" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Basic metrics"}
            {"✅ RAG document processing" if rag_system.available else "⚠️ RAG not available"}
            {"✅ AI Agent with function-calling" if AGENT_AVAILABLE else "⚠️ Agent system not available"}
            
            **Performance Targets (NPU):**
            - Load Time: <90s (first run), <30s (cached)
            - First Token: <2s latency
            - Generation: 15-25 tokens/second
            - Memory: Optimized for NPU constraints
            """
            
            gr.Info(info_text)
        
        def reset_metrics():
            """Reset performance metrics"""
            streaming_metrics.reset()
            gr.Info("📊 Performance metrics reset successfully")
        
        def reset_system_prompt():
            """Reset system prompt to default"""
            return DEFAULT_SYSTEM_PROMPT
        
        def apply_system_prompt(new_prompt):
            """Apply new system prompt and clear chat"""
            global SYSTEM_PROMPT
            if new_prompt.strip():
                SYSTEM_PROMPT = new_prompt.strip()
            else:
                SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT
            
            try:
                pipe.finish_chat()
                pipe.start_chat(SYSTEM_PROMPT)
                gr.Info("✅ System prompt updated and chat cleared")
                return [], "", SYSTEM_PROMPT
            except Exception as e:
                gr.Warning(f"⚠️ Error applying prompt: {e}")
                return [], "", new_prompt
        
        def handle_file_upload(files):
            """Handle uploaded files for RAG processing"""
            if not files:
                return "No files selected"
            
            results = []
            for file in files:
                if file is None:
                    continue
                
                file_name = os.path.basename(file.name)
                result = rag_system.process_uploaded_file(file.name, file_name)
                results.append(result)
            
            return "\n\n".join(results)
        
        def clear_documents():
            """Clear all uploaded documents"""
            result = rag_system.clear_documents()
            gr.Info(result)
            return result
        
        def show_rag_status():
            """Show RAG system status"""
            status = rag_system.get_status()
            gr.Info(f"RAG Status: {status}")
            return str(status)
        
        def reset_generation_settings():
            """Reset generation settings to defaults"""
            default_temp = config.get('generation', 'temperature', 0.6)
            default_top_p = config.get('generation', 'top_p', 0.95)
            default_max_tokens = config.get('generation', 'max_new_tokens', 512)
            
            generation_settings.update({
                'temperature': default_temp,
                'top_p': default_top_p,
                'max_new_tokens': default_max_tokens
            })
            
            status_text = f"Temperature: {default_temp}, Top-p: {default_top_p}, Max tokens: {default_max_tokens}"
            gr.Info("🔄 Generation settings reset to defaults")
            return default_temp, default_top_p, default_max_tokens, status_text
        
        def apply_generation_settings(temp, top_p, max_tokens):
            """Apply new generation settings"""
            generation_settings.update({
                'temperature': temp,
                'top_p': top_p,
                'max_new_tokens': int(max_tokens)
            })
            
            status_text = f"Temperature: {temp}, Top-p: {top_p}, Max tokens: {int(max_tokens)}"
            gr.Info("✅ Generation settings applied")
            return status_text
        
        def show_agent_status():
            """Show agent system status and capabilities"""
            if not AGENT_AVAILABLE:
                status = "❌ Agent system not available. Install dependencies to enable."
                gr.Info(status)
                return status, gr.update(visible=True)
            
            agent = get_agent()
            if agent:
                status = f"""✅ Agent system active
                
Available Tools: {len(agent.tools)}
Framework: ReAct (Reasoning + Acting)
Integration: OpenVINO GenAI + LangChain

The agent automatically detects when to use tools based on your questions.
Ask naturally and it will use the appropriate tools to help you!"""
            else:
                status = "⚠️ Agent system loaded but not initialized"
            
            gr.Info("Agent status displayed")
            return status, gr.update(visible=True)
        
        def list_agent_tools():
            """List all available agent tools with descriptions"""
            if not AGENT_AVAILABLE:
                status = "❌ No tools available - agent system not loaded"
                gr.Info(status)
                return status, gr.update(visible=True)
            
            agent = get_agent()
            if agent:
                tools_info = []
                for name, info in agent.tools.items():
                    tools_info.append(f"🔧 **{name}**: {info['description']}")
                
                status = "Available Agent Tools:\n\n" + "\n\n".join(tools_info)
            else:
                status = "⚠️ Agent not initialized"
            
            gr.Info("Tools list displayed")
            return status, gr.update(visible=True)
        
        # Wire up event handlers
        msg_input.submit(handle_send, [msg_input, chatbot], chatbot).then(
            lambda: gr.update(value=""), None, [msg_input]
        )
        
        send_btn.click(handle_send, [msg_input, chatbot], chatbot).then(
            lambda: gr.update(value=""), None, [msg_input]
        )
        
        clear_btn.click(handle_clear, [system_prompt_input], [chatbot, msg_input, system_prompt_input])
        
        metrics_btn.click(
            show_metrics, 
            None, 
            [metrics_json, phi3_stats if ENHANCED_CONTEXT_AVAILABLE else None, metrics_row]
        )
        
        system_btn.click(show_system_info, None, None)
        reset_metrics_btn.click(reset_metrics, None, None)
        
        # System prompt event handlers
        reset_prompt_btn.click(reset_system_prompt, None, [system_prompt_input])
        apply_prompt_btn.click(
            apply_system_prompt, 
            [system_prompt_input], 
            [chatbot, msg_input, system_prompt_input]
        )
        
        # RAG event handlers - always enable, will show error if RAG not available
        file_upload.upload(handle_file_upload, [file_upload], [upload_status])
        clear_docs_btn.click(clear_documents, None, [upload_status])
        rag_status_btn.click(show_rag_status, None, [upload_status])
        
        # Generation settings event handlers
        reset_gen_settings_btn.click(
            reset_generation_settings,
            None,
            [temperature_slider, top_p_slider, max_tokens_number, gen_settings_status]
        )
        
        apply_gen_settings_btn.click(
            apply_generation_settings,
            [temperature_slider, top_p_slider, max_tokens_number],
            [gen_settings_status]
        )
        
        # Agent event handlers (if available)
        if AGENT_AVAILABLE:
            agent_status_btn.click(
                show_agent_status,
                None,
                [agent_status_output, agent_status_output]
            )
            
            agent_tools_btn.click(
                list_agent_tools,
                None,
                [agent_status_output, agent_status_output]
            )
        
        # Initialize chat session when interface loads
        def initialize_session():
            """Initialize chat session with system prompt"""
            try:
                pipe.start_chat(SYSTEM_PROMPT)
                print("✅ Chat session initialized with system prompt")
            except Exception as e:
                print(f"⚠️ Session initialization error: {e}")
        
        demo.load(initialize_session, None, None)
    
    return demo


def initialize_ui_globals(pipeline, device, config, load_time_val):
    """Initialize global UI variables"""
    global pipe, device_used, config_used, load_time
    pipe = pipeline
    device_used = device
    config_used = config
    load_time = load_time_val

================================================================================

FILE: archive\gradio_qwen_debug.py
----------------------------------------
import gradio as gr
import openvino_genai as ov_genai
import openvino as ov
import openvino.properties as props
import openvino.properties.hint as hints
from transformers import AutoTokenizer
import time
import queue
from threading import Thread

# --- Strategy 4: Hardware-Level Performance Tuning ---
# Configure the OpenVINO Core for maximum latency performance.
print("Configuring OpenVINO for optimal performance...")
ov_config = {
    # High-level hint (recommended starting point)
    hints.performance_mode: hints.PerformanceMode.LATENCY,
    # Essential for eliminating 'cold start' latency on subsequent runs
    props.cache_dir: ".ovcache"
}

# NPU Pipeline-specific configuration (passed as kwargs)
pipeline_config = {
    "MAX_PROMPT_LEN": 2048,  # Increase from default 1024 for NPU
    "MIN_RESPONSE_LEN": 256   # Ensure minimum response length
}

# NPUW (NPU Wrapper) specific configurations for compilation success
npuw_config = {
    "NPU_USE_NPUW": "YES",           # Enable NPU Wrapper
    "NPUW_LLM": "YES",               # Enable LLM-specific NPU optimizations
    "NPUW_LLM_BATCH_DIM": 0,         # Batch dimension index
    "NPUW_LLM_SEQ_LEN_DIM": 1,       # Sequence length dimension index
    "NPUW_LLM_MAX_PROMPT_LEN": 2048, # Must match MAX_PROMPT_LEN
    "NPUW_LLM_MIN_RESPONSE_LEN": 256, # Must match MIN_RESPONSE_LEN
}

# Advanced NPU optimization properties (minimal - only use core NPUW properties)
advanced_npu_config = {
    # Only use properties that are known to work with NPUW
}

# Try to add NPU-specific settings if available
try:
    # Try the correct NPU properties for 2025
    from openvino.properties import intel_npu
    npu_optimizations = {}
    
    # Try to add available NPU properties
    npu_props_to_try = {
        'turbo': True,
        'compilation_mode_params': 'fast-compile'
    }
    
    for prop_name, prop_value in npu_props_to_try.items():
        try:
            if hasattr(intel_npu, prop_name):
                prop = getattr(intel_npu, prop_name)
                npu_optimizations[prop] = prop_value
                print(f"Added NPU property: {prop_name} = {prop_value}")
            else:
                print(f"NPU property {prop_name} not available in this version")
        except Exception as prop_e:
            print(f"Failed to add NPU property {prop_name}: {prop_e}")
    
    if npu_optimizations:
        ov_config.update(npu_optimizations)
        print(f"NPU-specific optimizations enabled: {len(npu_optimizations)} properties")
    else:
        print("No NPU-specific properties available")
    
except ImportError as ie:
    print(f"Intel NPU properties not available: {ie}")
except Exception as e:
    print(f"Error configuring NPU properties: {e}")

# Add standard performance optimizations
print("Adding standard performance settings...")
ov_config.update({
    props.hint.performance_mode: hints.PerformanceMode.LATENCY,
    props.streams.num: 1,
    props.inference_num_threads: 1
})
print(f"Final device configuration: {list(ov_config.keys())}")

# --- Strategy 1: Prompt Engineering for Conciseness ---
# Define a system prompt to instruct the model on its behavior.
system_prompt = (
    "You are a helpful, direct, and concise AI assistant. "
    "Provide answers immediately without any preamble, self-reflection, or hesitation. "
    "Your responses must be brief and to the point. Do not use think blocks."
)

# --- Strategy 2: Optimizing Generation Parameters ---
# This config uses greedy search for speed and sets strict limits.
generation_config = ov_genai.GenerationConfig()
generation_config.do_sample = False             # Disables sampling for fast, deterministic output.
generation_config.max_new_tokens = 1536         # Hard limit on response length.
generation_config.repetition_penalty = 1.1      # Discourages repetitive loops.

# --- 1. Configuration ---
model_path = r"C:\OpenVinoModels\qwen3-8b-int4-cw-ov"
device = "NPU"  # Can be switched to "CPU" for benchmarking.

# --- 2. Load Model and Tokenizer ---
print(f"Loading model and tokenizer for device: {device}...")
load_start_time = time.time()
try:
    # Try with proper LLMPipeline initialization pattern from samples
    # For NPU device, we need NPUW configuration for compilation success
    if device == "NPU":
        all_config = {**ov_config, **pipeline_config, **npuw_config, **advanced_npu_config}
        print("Using NPU with full NPUW (NPU Wrapper) optimization configuration")
    else:
        all_config = {**ov_config, **pipeline_config}  # Standard config for non-NPU devices
    
    try:
        print(f"Attempting to load with combined config: {list(all_config.keys())}")
        pipe = ov_genai.LLMPipeline(model_path, device, **all_config)
        print("Successfully loaded with full configuration")
    except Exception as config_error:
        print(f"Full config failed ({config_error}), trying NPUW only...")
        try:
            # Try only NPUW config without device properties that might conflict
            pipe = ov_genai.LLMPipeline(model_path, device, **npuw_config, **pipeline_config)
            print("Loaded with NPUW + pipeline configuration only")
        except Exception as npuw_error:
            print(f"NPUW config failed ({npuw_error}), trying device properties with NPUW...")
            try:
                # Try device properties + minimal NPUW config for compilation
                minimal_npuw_config = {**ov_config, **npuw_config}
                pipe = ov_genai.LLMPipeline(model_path, device, **minimal_npuw_config)
                print("Loaded with device properties + NPUW configuration")
            except Exception as mixed_error:
                print(f"Mixed config failed ({mixed_error}), trying device properties only...")
                try:
                    pipe = ov_genai.LLMPipeline(model_path, device, **ov_config)
                    print("Loaded with device properties only")
                except Exception as props_error:
                    print(f"Device properties failed ({props_error}), loading with basic settings...")
                    pipe = ov_genai.LLMPipeline(model_path, device)
                    print("Loaded with basic settings")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    print(f"Successfully loaded model on {device}")
except Exception as e:
    print(f"Error loading model on {device}: {e}")
    if device == "NPU":
        print("Falling back to CPU...")
        try:
            device = "CPU"
            # Try the same configuration hierarchy for CPU (no NPUW needed)
            all_config_cpu = {**ov_config, **pipeline_config}
            try:
                pipe = ov_genai.LLMPipeline(model_path, device, **all_config_cpu)
            except:
                try:
                    pipe = ov_genai.LLMPipeline(model_path, device, **ov_config)
                except:
                    pipe = ov_genai.LLMPipeline(model_path, device)
            
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            print(f"Successfully loaded model on {device}")
        except Exception as cpu_e:
            print(f"Error loading model on CPU: {cpu_e}")
            exit()
    else:
        exit()

load_end_time = time.time()
print(f"Model and tokenizer loaded successfully in {load_end_time - load_start_time:.2f} seconds.")


# --- Strategy 3: Architecting for Real-Time Interaction with Streaming ---
# This custom streamer bridges the backend generation with the Gradio UI queue.
class GradioStreamer(ov_genai.StreamerBase):
    def __init__(self, tokenizer: AutoTokenizer):
        super().__init__()
        self.tokenizer = tokenizer
        self.q = queue.Queue()
        self.accumulated_tokens = []
        self.full_response = ""

    def clean_text(self, text: str) -> str:
        """Remove internal model tokens from text"""
        # Remove common chat template tokens
        tokens_to_remove = [
            '<|im_start|>', '<|im_end|>', 
            '<|system|>', '<|user|>', '<|assistant|>',
            '<|endoftext|>', '<|end|>', '<|start|>',
            '</s>', '<s>', '[INST]', '[/INST]'
        ]
        
        cleaned_text = text
        for token in tokens_to_remove:
            cleaned_text = cleaned_text.replace(token, '')
        
        return cleaned_text

    def put(self, token_id: int) -> bool:
        self.accumulated_tokens.append(token_id)
        # Decode only the new part of the text
        decoded_text = self.tokenizer.decode(self.accumulated_tokens)
        if len(decoded_text) > len(self.full_response):
            new_text = decoded_text[len(self.full_response):]
            # Clean the new text before sending
            cleaned_new_text = self.clean_text(new_text)
            self.full_response = decoded_text
            if cleaned_new_text:  # Only send if there's content after cleaning
                self.q.put(cleaned_new_text)
        return False # Return False to continue generation

    def end(self):
        self.q.put(None) # Signal the end of the stream

    def __iter__(self):
        return self

    def __next__(self):
        item = self.q.get()
        if item is None:
            raise StopIteration
        return item

# --- 3. Define the Generation Function for Gradio ---
def truncate_conversation(conversation, tokenizer, max_tokens=800):
    """Truncate conversation history to fit within NPU token limits"""
    # Start with a more conservative limit for NPU (800 tokens instead of 1500)
    while len(conversation) > 2:  # Keep at least system + current user message
        # Calculate current token count
        test_prompt = tokenizer.apply_chat_template(
            conversation, add_generation_prompt=True, tokenize=False
        )
        token_count = len(tokenizer.encode(test_prompt))
        
        print(f"Current conversation token count: {token_count}")
        
        if token_count <= max_tokens:
            break
            
        # Remove oldest user-assistant pair (but keep system message)
        if len(conversation) > 2:
            removed_msg = conversation.pop(1)  # Remove oldest message
            print(f"Removed message: {removed_msg.get('role', 'unknown')} - {len(removed_msg.get('content', ''))} chars")
        else:
            break
    
    return conversation

def chat_function(message: str, history: list):
    try:
        streamer = GradioStreamer(tokenizer)
        
        # Manually construct the conversation with the system prompt
        conversation = [{"role": "system", "content": system_prompt}] + history
        conversation.append({"role": "user", "content": message})
        
        # Truncate conversation if it exceeds token limits (NPU-optimized)
        conversation = truncate_conversation(conversation, tokenizer, max_tokens=800)
        
        prompt = tokenizer.apply_chat_template(
            conversation, add_generation_prompt=True, tokenize=False
        )
        
        # Check final prompt length and apply aggressive truncation for NPU
        token_count = len(tokenizer.encode(prompt))
        print(f"Final prompt token count: {token_count}")
        
        if token_count > 1000:  # Much more conservative limit for NPU
            # If still too long, truncate the current user message aggressively
            max_msg_length = 200  # Much shorter limit
            if len(message) > max_msg_length:
                truncated_message = message[:max_msg_length] + "... [truncated]"
                conversation[-1]["content"] = truncated_message
                prompt = tokenizer.apply_chat_template(
                    conversation, add_generation_prompt=True, tokenize=False
                )
                final_token_count = len(tokenizer.encode(prompt))
                print(f"After message truncation, token count: {final_token_count}")
            
            # Last resort: if still too long, keep only system prompt + current message
            if len(tokenizer.encode(prompt)) > 1000:
                conversation = [conversation[0], conversation[-1]]  # Keep only system + current user
                prompt = tokenizer.apply_chat_template(
                    conversation, add_generation_prompt=True, tokenize=False
                )
                print(f"Emergency truncation - final token count: {len(tokenizer.encode(prompt))}")
        
        # Add a placeholder for the assistant's response to the history
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ""})

        # Run generation in a separate thread to avoid blocking the UI
        generation_thread = Thread(
            target=pipe.generate,
            args=(prompt, generation_config, streamer)
        )
        generation_thread.start()

        # Yield updates to the UI as they come from the streamer
        for new_text in streamer:
            history[-1]["content"] += new_text
            yield history

    except Exception as e:
        error_msg = str(e)
        print(f"\n!!! An error occurred during generation: {error_msg}")
        
        # Handle specific token limit errors
        if "1024 tokens" in error_msg or "MAX_PROMPT_LEN" in error_msg:
            error_response = "❌ **Token limit exceeded!** The conversation history is too long. Try starting a new chat or use shorter messages."
        else:
            error_response = f"❌ **Error:** {error_msg}"
            
        history.append({"role": "assistant", "content": error_response})
        yield history


# --- 4. Create and Launch the Gradio Interface ---
# The Gradio UI code remains largely the same
with gr.Blocks(theme=gr.themes.Base(), fill_height=True) as demo:
    gr.Markdown("## Qwen-8B OpenVINO Chat (NPU) - Optimized")
    chatbot = gr.Chatbot(label="Conversation", height=600, type='messages',
                         avatar_images=(None, "🤖"))
    with gr.Row():
        msg = gr.Textbox(label="Your Message", placeholder="Type your message here...", scale=7)
        submit_btn = gr.Button("Submit", variant="primary", scale=1)
    clear_btn = gr.Button("Clear Chat", variant="secondary")

    submit_event = msg.submit(chat_function, [msg, chatbot], chatbot)
    submit_event.then(lambda: gr.update(value=""), None, [msg], queue=False)
    button_event = submit_btn.click(chat_function, [msg, chatbot], chatbot)
    button_event.then(lambda: gr.update(value=""), None, [msg], queue=False)
    clear_btn.click(lambda: ([], ""), None, [chatbot, msg], queue=False)

    gr.Examples(examples=["Explain Project Governance in simple terms.", "Write a Python function for Fibonacci."], inputs=msg)

if __name__ == "__main__":
    demo.queue().launch(share=False)

================================================================================

FILE: archive\gradio_qwen_enhanced.py
----------------------------------------
#!/usr/bin/env python3
"""
Enhanced Qwen3 OpenVINO GenAI Chat Application

Copyright (c) 2025 sbran
Licensed under the MIT License - see LICENSE file for details

This application integrates with third-party components:
- OpenVINO GenAI (Apache 2.0, Intel Corporation)
- Qwen3 models (Apache 2.0 with additional terms, Alibaba Cloud)  
- Gradio (Apache 2.0, HuggingFace)
- Transformers (Apache 2.0, HuggingFace)

See ACKNOWLEDGMENTS.md for detailed attributions.

Features:
- Complete Qwen3-specific NPUW configuration and optimization
- Proper special token filtering for 26+ Qwen3 tokens
- Official Gradio streaming and ChatInterface patterns  
- Professional performance monitoring and diagnostics
- Robust error handling and deployment strategies
- Chat template support and session management

Based on enhanced context from:
- qwen3_model_context/ (model-specific optimizations)
- gradio_patterns/ (official Gradio best practices)  
- gradio_testing/ (professional testing patterns)

This is a hobby/educational project demonstrating OpenVINO GenAI capabilities.
"""

import gradio as gr
import openvino_genai as ov_genai
from transformers import AutoTokenizer

# Try to import OpenVINO properties with fallback for different versions
try:
    import openvino.properties as props
    import openvino.properties.hint as hints
    OPENVINO_PROPERTIES_AVAILABLE = True
    print("✅ OpenVINO properties imported successfully")
except ImportError as e:
    print(f"⚠️ OpenVINO properties not available: {e}")
    print("🔄 Using fallback configuration...")
    OPENVINO_PROPERTIES_AVAILABLE = False
    # Create mock objects for compatibility
    class MockHints:
        class PerformanceMode:
            LATENCY = "LATENCY"
            THROUGHPUT = "THROUGHPUT"
    
    class MockProps:
        class cache_dir:
            pass
        class streams:
            class num:
                pass
        class inference_num_threads:
            pass
    
    hints = MockHints()
    props = MockProps()
import time
import queue
import threading
import json
from typing import Optional, Dict, Any, List, Tuple, Iterator, Union, Callable
from typing_extensions import TypedDict, Literal
from dataclasses import dataclass, asdict
from pathlib import Path

# Type definitions for better code clarity
DeviceType = Literal["NPU", "CPU", "GPU", "AUTO"]
ProfileType = Literal["conservative", "balanced", "aggressive"]
ConfigDict = Dict[str, Any]
ChatMessage = TypedDict('ChatMessage', {'role': str, 'content': str})
ChatHistory = List[ChatMessage]

# Configuration result type
class DeploymentResult(TypedDict):
    pipeline: Any  # ov_genai.LLMPipeline
    device: str
    config: str
    load_time: float

# Import enhanced context patterns
import sys
import os
context_path = os.path.join(os.path.dirname(__file__), "context")
sys.path.insert(0, context_path)

# RAG system imports with fallback
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    RAG_AVAILABLE = True
    print("✅ RAG dependencies loaded successfully")
except ImportError as e:
    print(f"⚠️ RAG dependencies not available: {e}")
    print("📝 Install with: pip install langchain faiss-cpu sentence-transformers")
    RAG_AVAILABLE = False

# Import Qwen3-specific optimizations
try:
    from qwen3_model_context.npu_optimization import (
        Qwen3NPUConfigBuilder, 
        Qwen3NPUDeployment,
        Qwen3NPUPerformanceMonitor,
        QWEN3_NPU_PROFILES
    )
    from qwen3_model_context.special_tokens import (
        Qwen3StreamingFilter,
        Qwen3TokenFilter, 
        Qwen3ChatTemplate,
        QWEN3_SPECIAL_TOKENS
    )
    from qwen3_model_context.model_architecture import (
        QWEN3_8B_ARCHITECTURE,
        initialize_qwen3_pipeline
    )
    ENHANCED_CONTEXT_AVAILABLE = True
    print("✅ Enhanced Qwen3 context loaded successfully")
except ImportError as e:
    print(f"⚠️ Enhanced context not available: {e}")
    print("📝 Using fallback patterns - consider updating context path")
    ENHANCED_CONTEXT_AVAILABLE = False

# --- Configuration System ---
class ConfigurationLoader:
    """Load and manage application configuration from multiple sources"""
    
    def __init__(self, config_file: str = "config.json") -> None:
        """
        Initialize configuration loader.
        
        Args:
            config_file: Path to JSON configuration file
        """
        self.config_file = config_file
        self._config: ConfigDict = {}
        self.load_configuration()
    
    def load_configuration(self) -> None:
        """Load configuration from file and environment variables"""
        # Load default configuration
        self._config = self._get_default_config()
        
        # Try to load from file
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    file_config = json.load(f)
                self._merge_config(file_config)
                print(f"✅ Loaded configuration from {self.config_file}")
            else:
                print(f"📝 Using default configuration (no {self.config_file} found)")
        except Exception as e:
            print(f"⚠️ Failed to load {self.config_file}: {e}")
            print("📝 Using default configuration")
        
        # Override with environment variables
        self._apply_env_overrides()
    
    def _get_default_config(self) -> ConfigDict:
        """Get default configuration values"""
        return {
            "model": {
                "path": "C:\\OpenVinoModels\\phi3-128k-npu",
                "name": "Phi-3-mini-128k-instruct",
                "type": "phi3"
            },
            "deployment": {
                "target_device": "NPU",
                "npu_profile": "balanced",
                "fallback_device": "CPU",
                "cache_directory": "./cache/.ovcache_qwen3"
            },
            "generation": {
                "max_new_tokens": 1024,
                "temperature": 0.6,
                "top_p": 0.95,
                "top_k": 20,
                "repetition_penalty": 1.1,
                "do_sample": True
            },
            "ui": {
                "max_message_length": 400,
                "max_conversation_tokens": 1800,
                "emergency_limit": 2048,
                "show_performance_metrics": True,
                "theme": "soft"
            },
            "performance": {
                "generation_timeout": 30.0,
                "truncation_warning_delay": 0.5,
                "ui_update_interval": 0.1
            }
        }
    
    def _merge_config(self, new_config: ConfigDict) -> None:
        """Merge new configuration with existing configuration"""
        def merge_dict(base: Dict[str, Any], update: Dict[str, Any]) -> None:
            for key, value in update.items():
                if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                    merge_dict(base[key], value)
                else:
                    base[key] = value
        
        merge_dict(self._config, new_config)
    
    def _apply_env_overrides(self) -> None:
        """Apply environment variable overrides"""
        env_mappings = {
            "QWEN3_MODEL_PATH": ("model", "path"),
            "TARGET_DEVICE": ("deployment", "target_device"),
            "NPU_PROFILE": ("deployment", "npu_profile"),
            "CACHE_DIR": ("deployment", "cache_directory"),
            "MAX_MESSAGE_LENGTH": ("ui", "max_message_length"),
            "GENERATION_TIMEOUT": ("performance", "generation_timeout")
        }
        
        for env_var, (section, key) in env_mappings.items():
            value = os.getenv(env_var)
            if value is not None:
                # Type conversion based on key
                if key in ["max_message_length"]:
                    value = int(value)
                elif key in ["generation_timeout"]:
                    value = float(value)
                elif key in ["show_performance_metrics", "do_sample"]:
                    value = value.lower() in ('true', '1', 'yes', 'on')
                
                self._config[section][key] = value
                print(f"🔧 Environment override: {env_var} = {value}")
    
    def get(self, section: str, key: str, default: Any = None) -> Any:
        """Get configuration value"""
        return self._config.get(section, {}).get(key, default)
    
    def get_section(self, section: str) -> Dict[str, Any]:
        """Get entire configuration section"""
        return self._config.get(section, {})
    
    @property
    def config(self) -> ConfigDict:
        """Get full configuration"""
        return self._config.copy()

# Initialize configuration system
config = ConfigurationLoader()

# --- Constants and Configuration ---
# Use configuration system instead of hardcoded values
MODEL_PATH = config.get("model", "path", "C:\\OpenVinoModels\\phi3-128k-npu")
DEVICE = config.get("deployment", "target_device", "NPU")
CACHE_DIR = config.get("deployment", "cache_directory", "./cache/.ovcache_qwen3_enhanced")

# Qwen3-optimized settings from configuration
MAX_CONVERSATION_TOKENS = config.get("ui", "max_conversation_tokens", 1800)
EMERGENCY_LIMIT = config.get("ui", "emergency_limit", 2048)
MAX_MESSAGE_LENGTH = config.get("ui", "max_message_length", 400)
NPU_OPTIMIZATION_PROFILE = config.get("deployment", "npu_profile", "balanced")

@dataclass
class SystemMetrics:
    """Comprehensive system metrics tracking"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    avg_response_time: float = 0.0
    avg_first_token_latency: float = 0.0
    avg_tokens_per_second: float = 0.0
    total_tokens_generated: int = 0
    session_start_time: float = 0.0
    device_used: str = "unknown"
    config_used: str = "unknown"
    model_load_time: float = 0.0
    cache_hits: int = 0
    compilation_errors: int = 0
    special_tokens_filtered: int = 0
    
    def to_display_dict(self) -> Dict[str, Any]:
        """Convert to user-friendly display format"""
        session_duration = time.time() - self.session_start_time
        success_rate = (self.successful_requests / max(self.total_requests, 1)) * 100
        
        return {
            "Session Duration": f"{session_duration:.1f}s",
            "Total Requests": self.total_requests,
            "Success Rate": f"{success_rate:.1f}%",
            "Avg Response Time": f"{self.avg_response_time:.2f}s",
            "Avg Tokens/Second": f"{self.avg_tokens_per_second:.1f}",
            "Total Tokens Generated": self.total_tokens_generated,
            "Device": self.device_used,
            "Configuration": self.config_used,
            "Model Load Time": f"{self.model_load_time:.1f}s",
            "Special Tokens Filtered": self.special_tokens_filtered,
            "Cache Directory": CACHE_DIR
        }

# Global metrics instance
system_metrics = SystemMetrics(session_start_time=time.time())

# --- RAG System ---
class DocumentRAGSystem:
    """Retrieval-Augmented Generation system for document processing"""
    
    def __init__(self):
        """Initialize RAG system with fallback handling"""
        self.vector_store = None
        self.embeddings = None
        self.text_splitter = None
        self.processed_docs_count = 0
        self.available = RAG_AVAILABLE
        
        if RAG_AVAILABLE:
            try:
                # Initialize embeddings model (lightweight for fast loading)
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2",
                    model_kwargs={'device': 'cpu'}
                )
                
                # Initialize text splitter with optimized settings
                self.text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800,  # Smaller chunks for better retrieval
                    chunk_overlap=100,  # Overlap for context preservation
                    separators=["\n\n", "\n", ". ", " ", ""]
                )
                
                print("✅ RAG system initialized successfully")
                
            except Exception as e:
                print(f"⚠️ RAG initialization failed: {e}")
                self.available = False
        else:
            print("📝 RAG system not available - install dependencies to enable")
    
    def process_uploaded_file(self, file_path: str, file_name: str) -> str:
        """
        Process uploaded file for RAG retrieval.
        
        Args:
            file_path: Path to the uploaded file
            file_name: Original name of the file
            
        Returns:
            Status message about processing result
        """
        if not self.available:
            return "❌ RAG system not available. Install langchain and faiss-cpu to enable document processing."
        
        try:
            # Read file content with encoding detection
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            except UnicodeDecodeError:
                # Try with different encoding
                with open(file_path, 'r', encoding='latin-1') as f:
                    text = f.read()
            
            if not text.strip():
                return f"⚠️ File '{file_name}' appears to be empty."
            
            # Split text into chunks
            chunks = self.text_splitter.split_text(text)
            
            if not chunks:
                return f"⚠️ No processable content found in '{file_name}'."
            
            # Create or update vector store
            if self.vector_store is None:
                self.vector_store = FAISS.from_texts(
                    texts=chunks, 
                    embedding=self.embeddings,
                    metadatas=[{"source": file_name, "chunk": i} for i in range(len(chunks))]
                )
            else:
                # Add new documents to existing store
                new_store = FAISS.from_texts(
                    texts=chunks, 
                    embedding=self.embeddings,
                    metadatas=[{"source": file_name, "chunk": i} for i in range(len(chunks))]
                )
                self.vector_store.merge_from(new_store)
            
            self.processed_docs_count += 1
            
            return f"✅ Successfully processed '{file_name}': {len(chunks)} chunks created from {len(text):,} characters. Ready to answer questions about this document."
            
        except Exception as e:
            return f"❌ Error processing '{file_name}': {str(e)}"
    
    def retrieve_context(self, query: str, k: int = 3) -> str:
        """
        Retrieve relevant context for a query.
        
        Args:
            query: User question to find relevant context for
            k: Number of top chunks to retrieve
            
        Returns:
            Concatenated context from relevant document chunks
        """
        if not self.available or self.vector_store is None:
            return ""
        
        try:
            # Search for relevant documents
            docs = self.vector_store.similarity_search(query, k=k)
            
            if not docs:
                return ""
            
            # Format context with source attribution
            context_parts = []
            for doc in docs:
                source = doc.metadata.get("source", "Unknown")
                content = doc.page_content.strip()
                context_parts.append(f"[From {source}]\n{content}")
            
            return "\n\n---\n\n".join(context_parts)
            
        except Exception as e:
            print(f"⚠️ Context retrieval error: {e}")
            return ""
    
    def clear_documents(self) -> str:
        """Clear all processed documents"""
        self.vector_store = None
        self.processed_docs_count = 0
        return "✅ All documents cleared from memory."
    
    def get_status(self) -> dict:
        """Get current RAG system status"""
        return {
            "Available": self.available,
            "Documents Processed": self.processed_docs_count,
            "Vector Store": "Loaded" if self.vector_store is not None else "Empty",
            "Embedding Model": "all-MiniLM-L6-v2" if self.available else "None"
        }

# Global RAG system instance
rag_system = DocumentRAGSystem()

# Enhanced system prompt with Qwen3 optimization
DEFAULT_SYSTEM_PROMPT = """You are a helpful, concise AI assistant powered by Qwen3-8B running on Intel NPU via OpenVINO GenAI. 

Key behaviors:
- Provide accurate, well-structured responses
- Be concise but comprehensive 
- Use clear formatting when helpful
- Acknowledge when you're uncertain
- Optimize for NPU constraints (prefer shorter, focused responses)

You excel at: reasoning, coding, analysis, creative writing, and technical explanations."""

# Current system prompt (can be modified by user)
SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT

# --- Enhanced Configuration Management ---
class Qwen3ConfigurationManager:
    """Advanced configuration management with Qwen3 optimization"""
    
    def __init__(self, profile: ProfileType = "balanced") -> None:
        """
        Initialize configuration manager with specified profile.
        
        Args:
            profile: NPU optimization profile (conservative, balanced, aggressive)
        """
        self.profile = profile
        self.config_builder: Optional[Any] = None  # Qwen3NPUConfigBuilder if available
        self.performance_monitor: Optional[Any] = None  # Qwen3NPUPerformanceMonitor if available
        
        if ENHANCED_CONTEXT_AVAILABLE:
            self.config_builder = Qwen3NPUConfigBuilder(profile)
            self.performance_monitor = Qwen3NPUPerformanceMonitor()
    
    def get_npu_config(self) -> ConfigDict:
        """
        Get complete NPU configuration with NPUW optimization.
        
        Returns:
            Dictionary containing NPU-specific configuration parameters
        """
        if ENHANCED_CONTEXT_AVAILABLE and self.config_builder:
            # Use enhanced Qwen3-specific configuration
            return self.config_builder.build_complete_config()
        else:
            # Fallback configuration with compatibility handling
            config = {
                "NPU_USE_NPUW": "YES",
                "NPUW_LLM": "YES", 
                "NPUW_LLM_BATCH_DIM": 0,
                "NPUW_LLM_SEQ_LEN_DIM": 1,
                "NPUW_LLM_MAX_PROMPT_LEN": 2048,
                "NPUW_LLM_MIN_RESPONSE_LEN": 256,
                "CACHE_MODE": "OPTIMIZE_SPEED",
                "NPUW_LLM_PREFILL_HINT": "BEST_PERF",
                "NPUW_LLM_GENERATE_HINT": "BEST_PERF"
            }
            
            # Add OpenVINO properties if available (no generic PERFORMANCE_HINT for NPU)
            if OPENVINO_PROPERTIES_AVAILABLE:
                config.update({
                    props.cache_dir: CACHE_DIR
                })
            else:
                config.update({
                    "CACHE_DIR": CACHE_DIR
                })
            
            return config
    
    def get_cpu_config(self) -> ConfigDict:
        """
        Get optimized CPU configuration.
        
        Returns:
            Dictionary containing CPU-specific configuration parameters
        """
        if ENHANCED_CONTEXT_AVAILABLE and self.config_builder:
            return self.config_builder.build_complete_config()
        else:
            config = {
                "MAX_PROMPT_LEN": 4096,  # Larger context on CPU
                "MIN_RESPONSE_LEN": 512
            }
            
            # Add OpenVINO properties if available
            if OPENVINO_PROPERTIES_AVAILABLE:
                config.update({
                    hints.performance_mode: hints.PerformanceMode.THROUGHPUT,
                    props.cache_dir: CACHE_DIR + "_cpu",
                    props.streams.num: 2,
                    props.inference_num_threads: 0  # Auto-detect
                })
            else:
                config.update({
                    "PERFORMANCE_HINT": "THROUGHPUT",
                    "CACHE_DIR": CACHE_DIR + "_cpu",
                    "NUM_STREAMS": 2,
                    "INFERENCE_NUM_THREADS": 0  # Auto-detect
                })
            
            return config

# --- Enhanced Pipeline Deployment ---
def deploy_qwen3_pipeline(
    model_path: str, 
    target_device: DeviceType, 
    profile: ProfileType = "balanced"
) -> Tuple[Any, str, str, float]:
    """
    Deploy Qwen3 pipeline with comprehensive error handling and optimization.
    
    Args:
        model_path: Path to the Qwen3 OpenVINO model directory
        target_device: Target device for deployment (NPU, CPU, GPU, AUTO)
        profile: NPU optimization profile
        
    Returns:
        Tuple of (pipeline, device_used, config_used, load_time)
        
    Raises:
        RuntimeError: If all deployment configurations fail
    """
    load_start_time = time.time()
    
    if ENHANCED_CONTEXT_AVAILABLE:
        print(f"🚀 Deploying Qwen3 with enhanced context (profile: {profile})")
        
        # Use enhanced deployment
        deployment = Qwen3NPUDeployment(model_path, profile)
        pipeline = deployment.deploy()
        
        if pipeline:
            load_time = time.time() - load_start_time
            return pipeline, target_device, f"enhanced_{profile}", load_time
        else:
            print("⚠️ Enhanced deployment failed, falling back to manual configuration")
    
    # Fallback to manual configuration
    print(f"🔄 Using manual pipeline deployment (target: {target_device})")
    
    config_manager = Qwen3ConfigurationManager(profile)
    
    configurations = []
    
    # Create basic configurations with compatibility handling
    if OPENVINO_PROPERTIES_AVAILABLE:
        basic_npu_config = {hints.performance_mode: hints.PerformanceMode.LATENCY, props.cache_dir: CACHE_DIR}
        basic_cpu_config = {hints.performance_mode: hints.PerformanceMode.THROUGHPUT, props.cache_dir: CACHE_DIR}
    else:
        basic_npu_config = {"PERFORMANCE_HINT": "LATENCY", "CACHE_DIR": CACHE_DIR}
        basic_cpu_config = {"PERFORMANCE_HINT": "THROUGHPUT", "CACHE_DIR": CACHE_DIR}
    
    if target_device == "NPU":
        configurations = [
            ("enhanced_npu_qwen3", target_device, config_manager.get_npu_config()),
            ("basic_npu", target_device, basic_npu_config),
            ("minimal_npu", target_device, {}),
            ("cpu_fallback", "CPU", config_manager.get_cpu_config())
        ]
    else:
        configurations = [
            ("optimized_cpu_qwen3", target_device, config_manager.get_cpu_config()),
            ("basic_cpu", target_device, basic_cpu_config),
            ("minimal_cpu", target_device, {})
        ]
    
    for config_name, device, config in configurations:
        try:
            print(f"🔄 Trying {device} with {config_name} configuration...")
            
            if ENHANCED_CONTEXT_AVAILABLE:
                # Use enhanced initialization if available
                pipeline = initialize_qwen3_pipeline(model_path, device, **config)
            else:
                # Fallback initialization
                if config:
                    pipeline = ov_genai.LLMPipeline(model_path, device, **config)
                else:
                    pipeline = ov_genai.LLMPipeline(model_path, device)
                
            load_time = time.time() - load_start_time
            print(f"✅ Success: {device} with {config_name} ({load_time:.1f}s)")
            return pipeline, device, config_name, load_time
            
        except Exception as e:
            print(f"⚠️ {config_name} failed: {e}")
            if "compile" in str(e).lower():
                system_metrics.compilation_errors += 1
            continue
    
    raise RuntimeError("All configurations failed. Check model path, device drivers, and NPUW configuration.")

# --- Enhanced Streaming with Qwen3 Token Filtering ---
class EnhancedQwen3Streamer(ov_genai.StreamerBase):
    """
    Production-ready streamer with Qwen3-specific optimizations:
    - Proper special token filtering (26+ tokens)
    - Performance monitoring
    - Robust error handling
    - Token-level streaming control
    """
    
    def __init__(self, tokenizer: AutoTokenizer):
        super().__init__()
        self.tokenizer = tokenizer
        self.text_queue = queue.Queue()
        self.accumulated_tokens = []
        self.current_text = ""
        self.start_time = time.time()
        self.first_token_time = None
        self.tokens_generated = 0
        
        # Initialize Qwen3-specific filtering
        if ENHANCED_CONTEXT_AVAILABLE:
            self.token_filter = Qwen3StreamingFilter()
            print("✅ Using enhanced Qwen3 token filtering")
        else:
            self.token_filter = None
            print("⚠️ Using basic token filtering")
    
    def put(self, token_id: int) -> bool:
        """Process token with Qwen3-specific filtering"""
        self.accumulated_tokens.append(token_id)
        self.tokens_generated += 1
        
        # Record first token latency
        if self.first_token_time is None:
            self.first_token_time = time.time()
        
        try:
            # Decode with special token handling
            if ENHANCED_CONTEXT_AVAILABLE and self.token_filter:
                # Use enhanced filtering
                try:
                    token_text = self.tokenizer.decode([token_id], skip_special_tokens=False)
                except:
                    token_text = f"[UNK_{token_id}]"
                
                # Process through Qwen3 filter
                display_text = self.token_filter.process_token(token_id, token_text)
                
                if display_text is not None:
                    self.current_text += display_text
                    self.text_queue.put(display_text)
                else:
                    # Token was filtered (special token)
                    system_metrics.special_tokens_filtered += 1
                    
            else:
                # Fallback filtering
                try:
                    # Decode incrementally
                    full_text = self.tokenizer.decode(self.accumulated_tokens, skip_special_tokens=True)
                    
                    if len(full_text) > len(self.current_text):
                        new_text = full_text[len(self.current_text):]
                        self.current_text = full_text
                        
                        # Basic special token filtering
                        if not self._is_special_token_text(new_text):
                            self.text_queue.put(new_text)
                except Exception as e:
                    print(f"⚠️ Decoding error: {e}")
                    return False
                    
        except Exception as e:
            print(f"❌ Token processing error: {e}")
            return False
        
        return False  # Continue generation
    
    def _is_special_token_text(self, text: str) -> bool:
        """Basic special token detection for fallback"""
        special_patterns = [
            '<|im_start|>', '<|im_end|>', '<|endoftext|>',
            '<think>', '</think>', '<tool_call>', '</tool_call>',
            '<|system|>', '<|user|>', '<|assistant|>'
        ]
        
        for pattern in special_patterns:
            if pattern in text:
                system_metrics.special_tokens_filtered += 1
                return True
        
        return False
    
    def end(self):
        """Finalize streaming and calculate performance metrics"""
        # Calculate performance metrics
        total_time = time.time() - self.start_time
        first_token_latency = (self.first_token_time - self.start_time) if self.first_token_time else 0
        tokens_per_second = self.tokens_generated / total_time if total_time > 0 else 0
        
        # Update global metrics
        system_metrics.avg_first_token_latency = (
            (system_metrics.avg_first_token_latency * (system_metrics.successful_requests - 1) + first_token_latency)
            / system_metrics.successful_requests
        ) if system_metrics.successful_requests > 0 else first_token_latency
        
        system_metrics.avg_tokens_per_second = (
            (system_metrics.avg_tokens_per_second * (system_metrics.successful_requests - 1) + tokens_per_second)
            / system_metrics.successful_requests
        ) if system_metrics.successful_requests > 0 else tokens_per_second
        
        system_metrics.total_tokens_generated += self.tokens_generated
        
        # Log performance
        print(f"🚀 Generation complete: {self.tokens_generated} tokens in {total_time:.2f}s")
        print(f"   First token: {first_token_latency:.3f}s, Speed: {tokens_per_second:.1f} tok/s")
        
        if ENHANCED_CONTEXT_AVAILABLE and self.token_filter:
            thinking_content = self.token_filter.get_thinking_content()
            if thinking_content.strip():
                print(f"🧠 Model thinking: {thinking_content[:100]}...")
        
        # Signal end of generation
        self.text_queue.put(None)
    
    def __iter__(self):
        return self
    
    def __next__(self):
        item = self.text_queue.get()
        if item is None:
            raise StopIteration
        return item

# --- Enhanced Generation Configuration ---
# --- Security and Validation ---
class InputValidator:
    """Security-focused input validation and sanitization"""
    
    @staticmethod
    def validate_message(message: str) -> Tuple[bool, str]:
        """
        Validate user message for security and content policy compliance.
        
        Args:
            message: User input to validate
            
        Returns:
            Tuple of (is_valid, reason_if_invalid)
        """
        if not message or not isinstance(message, str):
            return False, "Empty or invalid message"
        
        # Check for excessively long messages (security)
        if len(message) > 10000:  # Much higher than UI limit
            return False, "Message exceeds maximum length"
        
        # Check for potential injection patterns
        suspicious_patterns = [
            r'<script[^>]*>',  # Script injection
            r'javascript:',     # JavaScript URLs
            r'data:.*base64',   # Data URLs
            r'eval\s*\(',      # Eval calls
            r'exec\s*\(',      # Exec calls
        ]
        
        import re
        for pattern in suspicious_patterns:
            if re.search(pattern, message, re.IGNORECASE):
                return False, "Message contains potentially unsafe content"
        
        # Check for excessive special characters (potential encoding attacks)
        special_char_ratio = len([c for c in message if not c.isalnum() and not c.isspace()]) / len(message)
        if special_char_ratio > 0.5:  # More than 50% special characters
            return False, "Message contains excessive special characters"
        
        return True, ""
    
    @staticmethod
    def sanitize_message(message: str) -> str:
        """
        Sanitize user message while preserving readability.
        
        Args:
            message: Raw user input
            
        Returns:
            Sanitized message safe for processing
        """
        # Remove null bytes and control characters (except newlines and tabs)
        sanitized = ''.join(char for char in message if ord(char) >= 32 or char in '\n\t')
        
        # Normalize whitespace
        sanitized = ' '.join(sanitized.split())
        
        # Limit consecutive repeated characters (potential DoS protection)
        import re
        sanitized = re.sub(r'(.)\1{10,}', r'\1\1\1', sanitized)
        
        return sanitized.strip()

def create_qwen3_generation_config() -> ov_genai.GenerationConfig:
    """
    Create optimized generation configuration for Qwen3 from configuration file.
    
    Returns:
        Configured GenerationConfig with security-conscious defaults
    """
    gen_config = ov_genai.GenerationConfig()
    
    # Load generation settings from configuration
    gen_settings = config.get_section("generation")
    
    if ENHANCED_CONTEXT_AVAILABLE:
        # Use Qwen3-specific defaults from context
        gen_config.do_sample = gen_settings.get("do_sample", True)
        gen_config.temperature = min(gen_settings.get("temperature", 0.6), 2.0)  # Security: cap temperature
        gen_config.top_p = min(gen_settings.get("top_p", 0.95), 1.0)  # Security: cap top_p
        gen_config.top_k = min(gen_settings.get("top_k", 20), 100)  # Security: reasonable top_k
        gen_config.max_new_tokens = min(gen_settings.get("max_new_tokens", 1024), 2048)  # Security: limit tokens
        gen_config.repetition_penalty = max(1.0, min(gen_settings.get("repetition_penalty", 1.1), 2.0))  # Security: reasonable range
    else:
        # Fallback configuration with security limits
        gen_config.do_sample = gen_settings.get("do_sample", True)
        gen_config.temperature = min(gen_settings.get("temperature", 0.7), 2.0)
        gen_config.top_p = min(gen_settings.get("top_p", 0.9), 1.0)
        gen_config.top_k = min(gen_settings.get("top_k", 50), 100)
        gen_config.max_new_tokens = min(gen_settings.get("max_new_tokens", 1024), 2048)
        gen_config.repetition_penalty = max(1.0, min(gen_settings.get("repetition_penalty", 1.1), 2.0))
    
    return gen_config

# --- Smart Message Processing ---
def process_user_message(message: str, history: ChatHistory) -> Tuple[str, bool]:
    """
    Process user message with Qwen3-optimized handling.
    
    Args:
        message: Raw user input message
        history: Current chat conversation history
        
    Returns:
        Tuple of (processed_message, was_truncated)
    """
    original_length = len(message)
    
    # Handle overly long messages
    if original_length > MAX_MESSAGE_LENGTH:
        # Smart truncation
        if '.' in message:
            sentences = message.split('.')
            truncated = []
            current_length = 0
            
            for sentence in sentences:
                if current_length + len(sentence) + 1 <= MAX_MESSAGE_LENGTH * 0.85:
                    truncated.append(sentence)
                    current_length += len(sentence) + 1
                else:
                    break
            
            if truncated:
                processed = '. '.join(truncated) + '.'
                if len(processed) < original_length * 0.5:
                    processed = message[:MAX_MESSAGE_LENGTH-50] + "..."
            else:
                processed = message[:MAX_MESSAGE_LENGTH-50] + "..."
        else:
            processed = message[:MAX_MESSAGE_LENGTH-50] + "..."
        
        print(f"📏 Message truncated: {original_length} → {len(processed)} chars")
        return processed, True
    
    return message, False

# --- Core Chat Processing Functions ---
def prepare_chat_input(message: str, history: ChatHistory) -> Tuple[str, bool, ChatHistory]:
    """
    Prepare and validate chat input with smart message handling and security validation.
    
    Args:
        message: Raw user input
        history: Current chat history
        
    Returns:
        Tuple of (processed_message, was_truncated, updated_history)
        
    Raises:
        ValueError: If message fails security validation
    """
    # Input validation
    if not message.strip():
        return message, False, history
    
    # Security validation
    is_valid, reason = InputValidator.validate_message(message)
    if not is_valid:
        error_history = history.copy()
        error_history.append({
            "role": "assistant", 
            "content": f"🚫 Message rejected: {reason}. Please try a different message."
        })
        raise ValueError(f"Security validation failed: {reason}")
    
    # Sanitize input
    sanitized_message = InputValidator.sanitize_message(message)
    
    # Process message with smart handling
    processed_message, was_truncated = process_user_message(sanitized_message, history)
    
    # Update history with user message and truncation warning if needed
    updated_history = history.copy()
    
    if was_truncated:
        truncation_warning = {
            "role": "assistant",
            "content": f"⚠️ Your message was truncated from {len(message):,} to {len(processed_message)} characters due to NPU memory limits. Processing the truncated version..."
        }
        updated_history.append({"role": "user", "content": message})
        updated_history.append(truncation_warning)
    else:
        updated_history.append({"role": "user", "content": processed_message})
    
    # Add assistant placeholder
    updated_history.append({"role": "assistant", "content": ""})
    
    return processed_message, was_truncated, updated_history

def execute_generation(processed_message: str, streamer: EnhancedQwen3Streamer) -> bool:
    """
    Execute model generation in a controlled manner.
    
    Args:
        processed_message: Message to generate response for
        streamer: Configured streamer for token processing
        
    Returns:
        True if generation succeeded, False otherwise
    """
    try:
        generation_config = create_qwen3_generation_config()
        pipe.generate(processed_message, generation_config, streamer)
        return True
    except Exception as e:
        print(f"❌ Generation error: {e}")
        # Send error through streamer
        error_msg = f"❌ Generation error: {str(e)[:100]}..."
        streamer.text_queue.put(error_msg)
        streamer.text_queue.put(None)
        return False

def stream_response_to_history(streamer: 'EnhancedQwen3Streamer', history: ChatHistory) -> Iterator[ChatHistory]:
    """
    Stream model response tokens to chat history.
    
    Args:
        streamer: Active streamer with generation in progress
        history: Chat history to update
        
    Yields:
        Updated history with streaming response
    """
    try:
        for chunk in streamer:
            if chunk:  # Only add non-empty chunks
                history[-1]["content"] += chunk
                yield history
    except Exception as e:
        print(f"❌ Streaming error: {e}")
        history[-1]["content"] = f"❌ Streaming error: {str(e)[:100]}..."
        yield history

def handle_chat_error(error: Exception, history: ChatHistory) -> ChatHistory:
    """
    Handle chat errors with user-friendly messages.
    
    Args:
        error: Exception that occurred
        history: Current chat history
        
    Returns:
        Updated history with error message
    """
    print(f"❌ Chat function error: {error}")
    
    # Determine error type and provide helpful message
    error_message = "❌ An error occurred. "
    error_str = str(error).lower()
    
    if "memory" in error_str:
        error_message += "Memory limit reached. Try starting a new conversation."
    elif "token" in error_str or "length" in error_str:
        error_message += "Message too long. Please try a shorter message."
    elif "compile" in error_str:
        error_message += "NPU compilation issue. Check NPUW configuration."
    elif "timeout" in error_str:
        error_message += "Generation timed out. Try a simpler request."
    elif "device" in error_str:
        error_message += "Device error. NPU may not be available."
    else:
        error_message += f"Details: {str(error)[:100]}..."
    
    # Add error to history
    updated_history = history.copy()
    if not updated_history or updated_history[-1]["role"] != "assistant":
        updated_history.append({"role": "assistant", "content": error_message})
    else:
        updated_history[-1]["content"] = error_message
    
    return updated_history

# --- Enhanced Chat Function (Refactored) ---
def enhanced_qwen3_chat(message: str, history: ChatHistory) -> Iterator[ChatHistory]:
    """
    Enhanced chat function with comprehensive Qwen3 optimization and RAG support.
    
    This is the main chat processing function that handles user input,
    processes it through the Qwen3 model with optional document context,
    and streams back the response with comprehensive error handling and performance monitoring.
    
    Args:
        message: User input message to process
        history: Current chat conversation history
        
    Yields:
        Updated chat history with streaming response as it's generated
        
    Note:
        Uses global system_metrics for performance tracking and
        requires global pipe and tokenizer to be initialized.
    """
    global system_metrics
    
    request_start_time = time.time()
    system_metrics.total_requests += 1
    
    try:
        # Step 1: Prepare and validate input
        processed_message, was_truncated, updated_history = prepare_chat_input(message, history)
        
        # Early return for empty messages
        if not processed_message.strip():
            yield updated_history
            return
        
        # Show truncation warning with brief pause
        if was_truncated:
            yield updated_history
            time.sleep(0.5)  # Brief pause for user to see warning
        
        # Step 1.5: RAG Context Retrieval
        rag_context = ""
        if rag_system.available and rag_system.vector_store is not None:
            rag_context = rag_system.retrieve_context(processed_message)
            if rag_context:
                # Augment the message with context
                augmented_message = f"""Based on the following context from uploaded documents, please answer the user's question. If the context doesn't contain relevant information, please indicate that and provide a general response.

Context:
{rag_context}

Question: {processed_message}"""
                processed_message = augmented_message
                print(f"📚 Using RAG context: {len(rag_context)} characters from documents")
        
        # Step 2: Initialize streaming components
        streamer = EnhancedQwen3Streamer(tokenizer)
        
        # Step 3: Execute generation in separate thread
        def generation_worker():
            success = execute_generation(processed_message, streamer)
            if success:
                system_metrics.successful_requests += 1
            else:
                system_metrics.failed_requests += 1
        
        generation_thread = threading.Thread(target=generation_worker, daemon=True)
        generation_thread.start()
        
        # Step 4: Stream response to UI
        yield from stream_response_to_history(streamer, updated_history)
        
        # Step 5: Wait for generation completion with timeout
        generation_thread.join(timeout=30.0)
        if generation_thread.is_alive():
            print("⚠️ Generation timeout - thread still running")
        
        # Step 6: Update performance metrics
        elapsed_time = time.time() - request_start_time
        system_metrics.avg_response_time = (
            (system_metrics.avg_response_time * (system_metrics.total_requests - 1) + elapsed_time)
            / system_metrics.total_requests
        )
        
        print(f"📊 Request complete: {elapsed_time:.2f}s total")
        
    except Exception as e:
        system_metrics.failed_requests += 1
        error_history = handle_chat_error(e, history)
        yield error_history

# --- System Initialization ---
print("🚀 Initializing Enhanced Qwen3 Chat System")
print(f"📂 Model: {MODEL_PATH}")
print(f"🎯 Target Device: {DEVICE}")
print(f"📊 Optimization Profile: {NPU_OPTIMIZATION_PROFILE}")
print(f"🔧 Enhanced Context: {'Available' if ENHANCED_CONTEXT_AVAILABLE else 'Fallback Mode'}")

def validate_system_requirements() -> List[str]:
    """Validate system requirements and return list of issues."""
    issues = []
    
    # Check model path
    if not os.path.exists(MODEL_PATH):
        issues.append(f"Model path does not exist: {MODEL_PATH}")
    elif not os.path.isdir(MODEL_PATH):
        issues.append(f"Model path is not a directory: {MODEL_PATH}")
    else:
        # Check for required OpenVINO files
        required_files = ['openvino_model.xml', 'openvino_model.bin']
        for file_name in required_files:
            if not os.path.exists(os.path.join(MODEL_PATH, file_name)):
                issues.append(f"Missing OpenVINO model file: {file_name}")
    
    # Check cache directory
    cache_dir = os.path.dirname(CACHE_DIR)
    if not os.path.exists(cache_dir):
        try:
            os.makedirs(cache_dir, exist_ok=True)
        except PermissionError:
            issues.append(f"Cannot create cache directory: {cache_dir} (permission denied)")
        except Exception as e:
            issues.append(f"Cannot create cache directory: {cache_dir} ({str(e)})")
    
    # Check OpenVINO installation
    try:
        import openvino as ov
        core = ov.Core()
        available_devices = core.available_devices
        if DEVICE not in available_devices and DEVICE != "AUTO":
            issues.append(f"Target device '{DEVICE}' not available. Available: {available_devices}")
    except Exception as e:
        issues.append(f"OpenVINO not properly installed: {str(e)}")
    
    return issues

def initialize_system_with_validation():
    """Initialize system with comprehensive validation and error handling."""
    global pipe, tokenizer, system_metrics
    
    print("🔍 Validating system requirements...")
    issues = validate_system_requirements()
    
    if issues:
        print("❌ System validation failed:")
        for i, issue in enumerate(issues, 1):
            print(f"   {i}. {issue}")
        print("\n🔧 Suggested fixes:")
        print("   • Set QWEN3_MODEL_PATH environment variable to correct model location")
        print("   • Install OpenVINO with: pip install openvino")
        print("   • For NPU: Install Intel NPU drivers from official site")
        print("   • Ensure model is in OpenVINO format (.xml/.bin files)")
        raise SystemExit(1)
    
    try:
        print("🚀 Initializing Enhanced Qwen3 Chat System...")
        
        # Deploy pipeline with comprehensive error handling
        pipe, device_used, config_used, load_time = deploy_qwen3_pipeline(
            MODEL_PATH, DEVICE, NPU_OPTIMIZATION_PROFILE
        )
        
        # Update system metrics
        system_metrics.device_used = device_used
        system_metrics.config_used = config_used
        system_metrics.model_load_time = load_time
        
        # Initialize tokenizer with error handling
        print("📚 Loading Qwen3 tokenizer...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
            
            # Configure tokenizer for Qwen3
            if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
                tokenizer.pad_token_id = tokenizer.eos_token_id
                
        except Exception as tokenizer_error:
            print(f"⚠️ Tokenizer loading failed: {tokenizer_error}")
            print("🔄 Attempting fallback tokenizer initialization...")
            try:
                # Fallback: try without trust_remote_code
                tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=False)
                if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
                    tokenizer.pad_token_id = tokenizer.eos_token_id
                print("✅ Fallback tokenizer loaded successfully")
            except Exception as fallback_error:
                print(f"❌ Fallback tokenizer also failed: {fallback_error}")
                raise RuntimeError("Unable to initialize tokenizer with any method") from fallback_error
        
        print(f"✅ System Ready!")
        print(f"   Device: {device_used}")
        print(f"   Config: {config_used}")
        print(f"   Load Time: {load_time:.1f}s")
        print(f"   Model Path: {MODEL_PATH}")
        print(f"   Tokenizer: {tokenizer.__class__.__name__}")
        if ENHANCED_CONTEXT_AVAILABLE:
            print(f"   Special Tokens: {len(QWEN3_SPECIAL_TOKENS)} Qwen3 tokens loaded")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"❌ System initialization failed: {e}")
        print("\n🔧 Detailed diagnostics:")
        print(f"   Model Path: {MODEL_PATH}")
        print(f"   Target Device: {DEVICE}")
        print(f"   Cache Directory: {CACHE_DIR}")
        print(f"   Enhanced Context: {ENHANCED_CONTEXT_AVAILABLE}")
        
        # Provide specific guidance based on error type
        error_str = str(e).lower()
        if "compile" in error_str:
            print("\n💡 NPU Compilation Error - Try:")
            print("   • Verify NPU drivers are installed")
            print("   • Check NPUW configuration compatibility")
            print("   • Try CPU fallback with: export TARGET_DEVICE=CPU")
        elif "file" in error_str or "path" in error_str:
            print("\n💡 File/Path Error - Try:")
            print("   • Verify model path contains .xml and .bin files")
            print("   • Check file permissions and access rights")
        elif "memory" in error_str:
            print("\n💡 Memory Error - Try:")
            print("   • Use conservative NPU profile")
            print("   • Ensure sufficient system RAM")
            print("   • Close other applications")
        
        raise SystemExit(1)

# Initialize system with enhanced error handling
try:
    initialize_system_with_validation()
except SystemExit:
    raise
except Exception as unexpected_error:
    print(f"💥 Unexpected initialization error: {unexpected_error}")
    print("🆘 This may be a bug - please report with full error details")
    raise

# --- Enhanced Gradio Interface ---
def create_enhanced_interface():
    """Create production-ready Gradio interface with advanced features"""
    
    # Custom CSS for professional appearance
    custom_css = """
    .gradio-container { max-width: 1400px; margin: auto; }
    .chatbot { height: 650px; }
    .metrics-panel { 
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        padding: 15px; 
        border-radius: 8px; 
        border: 1px solid #dee2e6;
    }
    .system-info {
        background: #f8f9fa;
        padding: 10px;
        border-radius: 6px;
        border-left: 4px solid #28a745;
        margin: 10px 0;
    }
    .warning-banner {
        background: #fff3cd;
        padding: 8px;
        border-radius: 4px;
        border-left: 4px solid #ffc107;
        margin: 5px 0;
    }
    """
    
    with gr.Blocks(
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="gray",
            neutral_hue="slate"
        ),
        title="Enhanced Qwen3 Chat",
        css=custom_css,
    ) as demo:
        
        # Header with system status
        with gr.Row():
            with gr.Column(scale=3):
                gr.Markdown(f"""
                # 🤖 Enhanced Qwen3 Chat System
                
                **Production-Ready Implementation with Complete Optimization**
                """)
            
            with gr.Column(scale=2, elem_classes=["system-info"]):
                system_status = gr.Markdown(f"""
                **Device**: {device_used} | **Config**: {config_used}  
                **Model**: Qwen3-8B INT4 | **Load Time**: {load_time:.1f}s  
                **Enhanced Context**: {'✅ Active' if ENHANCED_CONTEXT_AVAILABLE else '⚠️ Fallback'}  
                **Profile**: {NPU_OPTIMIZATION_PROFILE.title()}
                """)
        
        # Warning banner if fallback mode
        if not ENHANCED_CONTEXT_AVAILABLE:
            gr.Markdown("""
            <div class="warning-banner">
            ⚠️ <strong>Fallback Mode</strong>: Enhanced context not loaded. Some optimizations may be limited.
            </div>
            """)
        
        # Main chat interface using official ChatInterface pattern
        chatbot = gr.Chatbot(
            label=f"Conversation (Qwen3-8B on {device_used})",
            height=650,
            type='messages',
            avatar_images=(None, "🤖"),
            show_copy_button=True,
            show_share_button=False,
            bubble_full_width=False,
            render_markdown=True
        )
        
        # System prompt control
        with gr.Accordion("🎯 System Prompt Configuration", open=False):
            system_prompt_input = gr.Textbox(
                value=SYSTEM_PROMPT,
                lines=6,
                label="System Prompt",
                placeholder="Configure the AI's behavior and persona...",
                interactive=True,
                info="This prompt sets the AI's behavior, expertise, and response style. Changes take effect after clearing the chat."
            )
            
            with gr.Row():
                reset_prompt_btn = gr.Button("🔄 Reset to Default", size="sm")
                apply_prompt_btn = gr.Button("✅ Apply & Clear Chat", variant="primary", size="sm")
        
        # Document upload for RAG
        with gr.Accordion("📚 Document Upload (RAG)", open=False):
            with gr.Row():
                with gr.Column(scale=3):
                    file_upload = gr.File(
                        label="Upload Documents",
                        file_types=[".txt", ".md", ".py", ".js", ".html", ".css", ".json"],
                        file_count="multiple",
                        interactive=True
                    )
                    
                with gr.Column(scale=2):
                    upload_status = gr.Textbox(
                        label="Upload Status",
                        interactive=False,
                        placeholder="No documents uploaded"
                    )
            
            with gr.Row():
                clear_docs_btn = gr.Button("🗑️ Clear Documents", variant="secondary", size="sm")
                rag_status_btn = gr.Button("📊 RAG Status", size="sm")
            
            if not RAG_AVAILABLE:
                gr.Markdown("""
                ⚠️ **RAG not available**: Install dependencies with:
                ```
                pip install langchain faiss-cpu sentence-transformers
                ```
                """)

        # Input controls
        with gr.Row():
            msg_input = gr.Textbox(
                placeholder=f"Chat with Qwen3 on {device_used} (max {MAX_MESSAGE_LENGTH} chars)...",
                scale=7,
                max_lines=4,
                show_label=False,
                container=False
            )
            
            with gr.Column(scale=1):
                send_btn = gr.Button("💬 Send", variant="primary", size="lg")
                clear_btn = gr.Button("🗑️ Clear", variant="secondary", size="sm")
        
        # Advanced controls panel
        with gr.Row():
            with gr.Column(scale=2):
                metrics_btn = gr.Button("📊 Performance Metrics", variant="secondary")
                system_btn = gr.Button("ℹ️ System Info", variant="secondary")
            
            with gr.Column(scale=2):
                if ENHANCED_CONTEXT_AVAILABLE:
                    profile_selector = gr.Dropdown(
                        choices=list(QWEN3_NPU_PROFILES.keys()),
                        value=NPU_OPTIMIZATION_PROFILE,
                        label="NPU Profile",
                        interactive=False  # Would need restart to change
                    )
                
                reset_metrics_btn = gr.Button("🔄 Reset Metrics", variant="secondary")
        
        # Collapsible metrics panel
        with gr.Row(visible=False) as metrics_row:
            with gr.Column(elem_classes=["metrics-panel"]):
                gr.Markdown("### 📊 Real-time Performance Metrics")
                metrics_json = gr.JSON(label="System Metrics", container=True)
                
                if ENHANCED_CONTEXT_AVAILABLE:
                    gr.Markdown("### 🎯 Qwen3-Specific Stats")
                    qwen3_stats = gr.JSON(label="Token Filtering & Processing", container=True)
        
        # Examples section
        with gr.Row():
            gr.Examples(
                examples=[
                    "Explain quantum computing in simple terms",
                    "Write a Python function to implement quicksort",
                    "What are the advantages of using Intel NPU for AI inference?",
                    "Compare different neural network architectures",
                    "Help me debug this code: def factorial(n): return n * factorial(n)",
                    "Explain the concept of attention in transformer models",
                    "What does the uploaded document say about...?",
                    "Summarize the key points from the uploaded files"
                ],
                inputs=msg_input,
                label="💡 Example Questions (Upload documents for context-aware answers)"
            )
        
        # Event handlers with enhanced functionality
        def handle_send(message, history):
            """Handle send with proper session management"""
            return enhanced_qwen3_chat(message, history)
        
        def handle_clear(current_system_prompt):
            """Handle clear with proper session reset"""
            global SYSTEM_PROMPT
            try:
                # Update global system prompt if changed
                if current_system_prompt.strip():
                    SYSTEM_PROMPT = current_system_prompt.strip()
                else:
                    SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT
                
                # End current session and start new one
                pipe.finish_chat()
                pipe.start_chat(SYSTEM_PROMPT)
                print("🔄 Chat session reset with updated system prompt")
                return [], "", SYSTEM_PROMPT
            except Exception as e:
                print(f"⚠️ Session reset error: {e}")
                return [], "", current_system_prompt
        
        def show_metrics():
            """Display comprehensive performance metrics"""
            base_metrics = system_metrics.to_display_dict()
            
            qwen3_specific = {}
            if ENHANCED_CONTEXT_AVAILABLE:
                qwen3_specific = {
                    "Special Tokens Filtered": system_metrics.special_tokens_filtered,
                    "Compilation Errors": system_metrics.compilation_errors,
                    "NPUW Profile": NPU_OPTIMIZATION_PROFILE,
                    "Enhanced Features": "Active",
                    "Qwen3 Architecture": f"{QWEN3_8B_ARCHITECTURE.get('parameters', '8B')} parameters",
                    "Max Context": f"{QWEN3_8B_ARCHITECTURE.get('max_position_embeddings', 40960):,} tokens"
                }
            else:
                qwen3_specific = {
                    "Enhanced Features": "Fallback Mode",
                    "Note": "Install enhanced context for full optimization"
                }
            
            return (
                gr.update(value=base_metrics, visible=True),
                gr.update(value=qwen3_specific, visible=True) if ENHANCED_CONTEXT_AVAILABLE else gr.update(visible=False),
                gr.update(visible=True)
            )
        
        def show_system_info():
            """Display comprehensive system information"""
            info_text = f"""
            ## 🖥️ System Configuration
            
            **Hardware & Device:**
            - Target Device: {device_used}
            - Configuration: {config_used}
            - Cache Directory: `{CACHE_DIR}`
            - NPU Profile: {NPU_OPTIMIZATION_PROFILE}
            
            **Model Details:**
            - Model: Qwen3-8B INT4 Quantized
            - Path: `{MODEL_PATH}`
            - Load Time: {load_time:.1f} seconds
            - Tokenizer: {tokenizer.__class__.__name__}
            
            **OpenVINO GenAI Configuration:**
            - API Mode: Stateful (start_chat/finish_chat)
            - Conversation Management: Automatic KV-cache
            - Token Limits: {MAX_CONVERSATION_TOKENS} (conversation), {MAX_MESSAGE_LENGTH} (message)
            - Generation: Temperature={"0.6" if ENHANCED_CONTEXT_AVAILABLE else "0.7"}, Top-p={"0.95" if ENHANCED_CONTEXT_AVAILABLE else "0.9"}
            
            **Enhanced Features:**
            {"✅ Complete Qwen3 NPUW optimization" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Basic NPUW configuration"}
            {"✅ 26+ special token filtering" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Basic token filtering"}
            {"✅ Qwen3-specific chat templates" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Standard templates"}
            {"✅ Advanced performance monitoring" if ENHANCED_CONTEXT_AVAILABLE else "⚠️ Basic metrics"}
            
            **Performance Targets (NPU):**
            - Load Time: <90s (first run), <30s (cached)
            - First Token: <2s latency
            - Generation: 15-25 tokens/second
            - Memory: Optimized for NPU constraints
            """
            
            gr.Info(info_text)
        
        def reset_metrics():
            """Reset performance metrics"""
            global system_metrics
            session_start = system_metrics.session_start_time
            device = system_metrics.device_used
            config = system_metrics.config_used
            load_time = system_metrics.model_load_time
            
            system_metrics = SystemMetrics(
                session_start_time=session_start,
                device_used=device,
                config_used=config,
                model_load_time=load_time
            )
            
            gr.Info("📊 Performance metrics reset successfully")
        
        def reset_system_prompt():
            """Reset system prompt to default"""
            return DEFAULT_SYSTEM_PROMPT
        
        def apply_system_prompt(new_prompt):
            """Apply new system prompt and clear chat"""
            global SYSTEM_PROMPT
            if new_prompt.strip():
                SYSTEM_PROMPT = new_prompt.strip()
            else:
                SYSTEM_PROMPT = DEFAULT_SYSTEM_PROMPT
            
            try:
                pipe.finish_chat()
                pipe.start_chat(SYSTEM_PROMPT)
                gr.Info("✅ System prompt updated and chat cleared")
                return [], "", SYSTEM_PROMPT
            except Exception as e:
                gr.Warning(f"⚠️ Error applying prompt: {e}")
                return [], "", new_prompt
        
        def handle_file_upload(files):
            """Handle uploaded files for RAG processing"""
            if not files:
                return "No files selected"
            
            results = []
            for file in files:
                if file is None:
                    continue
                
                file_name = os.path.basename(file.name)
                result = rag_system.process_uploaded_file(file.name, file_name)
                results.append(result)
            
            return "\n\n".join(results)
        
        def clear_documents():
            """Clear all uploaded documents"""
            result = rag_system.clear_documents()
            gr.Info(result)
            return result
        
        def show_rag_status():
            """Show RAG system status"""
            status = rag_system.get_status()
            gr.Info(f"RAG Status: {status}")
            return str(status)
        
        # Wire up event handlers
        msg_input.submit(handle_send, [msg_input, chatbot], chatbot).then(
            lambda: gr.update(value=""), None, [msg_input]
        )
        
        send_btn.click(handle_send, [msg_input, chatbot], chatbot).then(
            lambda: gr.update(value=""), None, [msg_input]
        )
        
        clear_btn.click(handle_clear, [system_prompt_input], [chatbot, msg_input, system_prompt_input])
        
        metrics_btn.click(
            show_metrics, 
            None, 
            [metrics_json, qwen3_stats if ENHANCED_CONTEXT_AVAILABLE else None, metrics_row]
        )
        
        system_btn.click(show_system_info, None, None)
        reset_metrics_btn.click(reset_metrics, None, None)
        
        # System prompt event handlers
        reset_prompt_btn.click(reset_system_prompt, None, [system_prompt_input])
        apply_prompt_btn.click(
            apply_system_prompt, 
            [system_prompt_input], 
            [chatbot, msg_input, system_prompt_input]
        )
        
        # RAG event handlers
        file_upload.upload(handle_file_upload, [file_upload], [upload_status])
        clear_docs_btn.click(clear_documents, None, [upload_status])
        rag_status_btn.click(show_rag_status, None, [upload_status])
        
        # Initialize chat session when interface loads
        def initialize_session():
            """Initialize chat session with system prompt"""
            try:
                pipe.start_chat(SYSTEM_PROMPT)
                print("✅ Chat session initialized with system prompt")
            except Exception as e:
                print(f"⚠️ Session initialization error: {e}")
        
        demo.load(initialize_session, None, None)
    
    return demo

# --- Launch Application ---
if __name__ == "__main__":
    print("🌐 Launching Enhanced Qwen3 Chat Interface...")
    print("✨ Features:")
    print("   🎯 Complete Qwen3 NPUW optimization")
    print("   🔍 26+ special token filtering")
    print("   📊 Professional performance monitoring") 
    print("   🛡️ Robust error handling and diagnostics")
    print("   🔧 Official Gradio patterns integration")
    print("   💡 Intelligent message processing")
    print("=" * 60)
    
    demo = create_enhanced_interface()
    
    # Security-conscious launch configuration
    launch_config = {
        "share": False,  # Security: Never share publicly by default
        "server_name": "127.0.0.1",  # Security: Bind to localhost only
        "server_port": 7860,
        "show_error": True,
        "show_tips": True,
        "quiet": False,
        "auth": None,  # No authentication by default (add if needed)
        "max_file_size": "10mb",  # Limit file upload size
        "allowed_paths": []  # No file access by default
    }
    
    # Security warning if share is enabled via environment
    if os.getenv("GRADIO_SHARE", "").lower() in ('true', '1', 'yes'):
        print("⚠️ WARNING: Public sharing enabled via GRADIO_SHARE environment variable")
        print("🔒 Ensure your system is secure and consider adding authentication")
        launch_config["share"] = True
    
    demo.queue(
        max_size=20,
        default_concurrency_limit=1  # NPU works best with single concurrent requests
    ).launch(**launch_config)

================================================================================

FILE: archive\gradio_qwen_hybrid.py
----------------------------------------
#!/usr/bin/env python3
"""
Hybrid Gradio Qwen Chat - Best of Both Worlds
=============================================

This hybrid version combines:
- Comprehensive fallback logic from gradio_qwen_debug.py (stability)
- RAG-inspired optimizations from gradio_qwen_optimized.py (performance)
- Consultant-recommended architectural improvements (maintainability)

Features:
- Exhaustive configuration cascade for maximum compatibility
- Semantic-aware truncation with punctuation breaks
- Rolling performance metrics and throughput monitoring
- Enhanced streaming with throttling options
- Professional error handling with detailed diagnostics
- Mode switching between development (debug) and production (optimized)
"""

import gradio as gr
import openvino_genai as ov_genai
import openvino as ov
import openvino.properties as props
import openvino.properties.hint as hints
from transformers import AutoTokenizer
import time
import queue
from threading import Thread
import re
from typing import Optional, Tuple, List, Dict, Any

# --- Configuration Constants ---
print("🚀 Starting Hybrid Qwen Chat System...")
model_path = r"C:\OpenVinoModels\qwen3-8b-int4-cw-ov"
device = "NPU"  # Can be switched to "CPU" for comparison

# Operational mode: "debug" for development, "production" for deployment
OPERATION_MODE = "debug"  # Change to "production" for cleaner output

# Named constants for maintainability
MAX_CONVERSATION_TOKENS = 1024
EMERGENCY_TOKEN_LIMIT = 1500
BASIC_TOKEN_LIMIT = 800
MAX_MESSAGE_LENGTH = 200
TRUNCATION_PREVIEW_LENGTH = 100
TRUNCATION_SUFFIX_LENGTH = 50
STREAMING_DELAY_MS = 30  # Throttle streaming for smoother UX
PERFORMANCE_WINDOW_SIZE = 15  # Rolling window for metrics

def log_debug(message: str):
    """Debug logging that respects operation mode"""
    if OPERATION_MODE == "debug":
        print(f"🔧 DEBUG: {message}")

def log_info(message: str):
    """Info logging for both modes"""
    print(f"ℹ️  {message}")

def log_error(message: str):
    """Error logging for both modes"""
    print(f"❌ ERROR: {message}")

def log_success(message: str):
    """Success logging for both modes"""
    print(f"✅ {message}")

# --- Advanced Configuration Factory ---
class ConfigurationManager:
    """Manages OpenVINO configurations with comprehensive fallback strategies"""
    
    @staticmethod
    def get_base_config() -> Dict[str, Any]:
        """Base configuration that works across all devices"""
        return {
            hints.performance_mode: hints.PerformanceMode.LATENCY,
            props.cache_dir: r"C:\temp\.ovcache"  # Cloud-friendly cache
        }
    
    @staticmethod
    def get_npu_config_cascade() -> List[Tuple[str, Dict[str, Any]]]:
        """Complete NPU configuration cascade from most to least optimized"""
        base = ConfigurationManager.get_base_config()
        
        # Configuration cascade - try from most optimized to most compatible
        configs = [
            # Tier 1: Full NPUW with advanced optimizations
            ("full_npuw_advanced", {
                **base,
                "NPU_USE_NPUW": "YES",
                "NPUW_LLM": "YES",
                "NPUW_LLM_BATCH_DIM": 0,
                "NPUW_LLM_SEQ_LEN_DIM": 1,
                "NPUW_LLM_MAX_PROMPT_LEN": 2048,
                "NPUW_LLM_MIN_RESPONSE_LEN": 256,
                "CACHE_MODE": "OPTIMIZE_SPEED",
                "INFERENCE_PRECISION_HINT": "f16",
                "MAX_PROMPT_LEN": 2048,
                "MIN_RESPONSE_LEN": 256
            }),
            
            # Tier 2: Basic NPUW without advanced properties
            ("basic_npuw", {
                **base,
                "NPU_USE_NPUW": "YES",
                "NPUW_LLM": "YES",
                "NPUW_LLM_MAX_PROMPT_LEN": 1024,
                "NPUW_LLM_MIN_RESPONSE_LEN": 128,
                "MAX_PROMPT_LEN": 1024,
                "MIN_RESPONSE_LEN": 128
            }),
            
            # Tier 3: NPU-specific optimizations without NPUW
            ("npu_specific", {
                **base,
                "CACHE_MODE": "OPTIMIZE_SPEED",
                "MAX_PROMPT_LEN": 1024
            }),
            
            # Tier 4: Base configuration with NPU device properties
            ("device_properties", base),
            
            # Tier 5: Minimal - no additional config
            ("minimal", {})
        ]
        
        return configs
    
    @staticmethod
    def get_cpu_config_cascade() -> List[Tuple[str, Dict[str, Any]]]:
        """CPU configuration cascade"""
        base = ConfigurationManager.get_base_config()
        
        return [
            ("cpu_optimized", {
                **base,
                props.streams.num: 1,
                props.inference_num_threads: 4,
                "MAX_PROMPT_LEN": 2048
            }),
            ("cpu_basic", base),
            ("cpu_minimal", {})
        ]

def try_load_pipeline_cascade(model_path: str, target_device: str) -> Tuple[Optional[ov_genai.LLMPipeline], str, str]:
    """
    Comprehensive pipeline loading with full fallback cascade
    Returns: (pipeline, device_used, config_used)
    """
    
    # Get appropriate configuration cascade
    if target_device == "NPU":
        primary_configs = ConfigurationManager.get_npu_config_cascade()
        fallback_device = "CPU"
        fallback_configs = ConfigurationManager.get_cpu_config_cascade()
    else:
        primary_configs = ConfigurationManager.get_cpu_config_cascade()
        fallback_device = None
        fallback_configs = []
    
    # Try primary device with all configurations
    log_info(f"Attempting to load model on {target_device}")
    for config_name, config in primary_configs:
        try:
            log_debug(f"Trying {target_device} with {config_name} configuration")
            
            if config:
                pipe = ov_genai.LLMPipeline(model_path, target_device, **config)
            else:
                pipe = ov_genai.LLMPipeline(model_path, target_device)
                
            log_success(f"Successfully loaded on {target_device} with {config_name} configuration")
            return pipe, target_device, config_name
            
        except Exception as e:
            log_debug(f"{target_device}/{config_name} failed: {e}")
            continue
    
    # Try fallback device if primary failed
    if fallback_device and fallback_configs:
        log_info(f"Primary device {target_device} failed, trying fallback {fallback_device}")
        
        for config_name, config in fallback_configs:
            try:
                log_debug(f"Trying {fallback_device} with {config_name} configuration")
                
                if config:
                    pipe = ov_genai.LLMPipeline(model_path, fallback_device, **config)
                else:
                    pipe = ov_genai.LLMPipeline(model_path, fallback_device)
                    
                log_success(f"Successfully loaded on {fallback_device} with {config_name} configuration")
                return pipe, fallback_device, config_name
                
            except Exception as e:
                log_debug(f"{fallback_device}/{config_name} failed: {e}")
                continue
    
    log_error("All device and configuration combinations failed")
    return None, "", ""

# --- Enhanced Generation Configuration ---
def create_generation_config(tokenizer: AutoTokenizer, mode: str = "balanced") -> ov_genai.GenerationConfig:
    """Create generation configuration based on operational mode"""
    config = ov_genai.GenerationConfig()
    
    if mode == "speed":
        # Speed-optimized for debug/testing
        config.do_sample = False
        config.max_new_tokens = 512
        config.repetition_penalty = 1.1
    elif mode == "quality":
        # Quality-optimized for production
        config.do_sample = True
        config.temperature = 0.7
        config.top_p = 0.9
        config.top_k = 50
        config.max_new_tokens = 1024
        config.repetition_penalty = 1.15
    else:  # balanced
        config.do_sample = True
        config.temperature = 0.6
        config.top_p = 0.85
        config.top_k = 40
        config.max_new_tokens = 768
        config.repetition_penalty = 1.12
    
    # Note: pad_token_id is handled by tokenizer in 2025 API, not GenerationConfig
    
    return config

# --- Semantic-Aware Text Processing ---
def semantic_aware_truncation(text: str, max_length: int) -> str:
    """Advanced truncation preserving semantic boundaries"""
    if len(text) <= max_length:
        return text
    
    # Try sentence boundaries first
    sentences = text.split('. ')
    if len(sentences) > 1:
        current_length = 0
        result_sentences = []
        
        for sentence in sentences:
            if current_length + len(sentence) + 2 <= max_length * 0.8:  # Leave room for ellipsis
                result_sentences.append(sentence)
                current_length += len(sentence) + 2
            else:
                break
        
        if result_sentences:
            remaining_text = '. '.join(sentences[len(result_sentences):])
            result = '. '.join(result_sentences) + '.'
            
            if len(remaining_text) > 30:
                preview = remaining_text[:30].strip()
                result += f" ... [continuing: {preview}...]"
            
            return result
    
    # Try phrase boundaries (punctuation)
    phrase_pattern = r'[,.;:!?]\s+'
    matches = list(re.finditer(phrase_pattern, text[:max_length]))
    
    if matches:
        best_break = max(match.end() for match in matches)
        if best_break > max_length * 0.6:
            prefix = text[:best_break].strip()
            suffix_start = best_break
            suffix_end = min(len(text), suffix_start + TRUNCATION_SUFFIX_LENGTH)
            suffix = text[suffix_start:suffix_end].strip()
            
            return f"{prefix} ... [truncated: {suffix}...]"
    
    # Fallback: clean word boundary truncation
    truncated = text[:TRUNCATION_PREVIEW_LENGTH]
    last_space = truncated.rfind(' ')
    
    if last_space > TRUNCATION_PREVIEW_LENGTH * 0.7:
        truncated = truncated[:last_space]
    
    suffix = text[-TRUNCATION_SUFFIX_LENGTH:].strip()
    return f"{truncated} ... [content truncated] ... {suffix}"

def intelligent_conversation_management(conversation: List[Dict], tokenizer: AutoTokenizer, 
                                      max_tokens: int, config_mode: str) -> List[Dict]:
    """Advanced conversation management with context preservation"""
    
    if len(conversation) <= 2:  # Keep system + current user
        return conversation
    
    iteration_count = 0
    max_iterations = 10  # Prevent infinite loops
    
    while len(conversation) > 2 and iteration_count < max_iterations:
        # Calculate token count
        try:
            test_prompt = tokenizer.apply_chat_template(
                conversation, add_generation_prompt=True, tokenize=False
            )
            token_count = len(tokenizer.encode(test_prompt))
        except Exception as e:
            log_debug(f"Token counting failed: {e}, using conservative estimate")
            # Conservative estimate: ~4 chars per token
            char_count = sum(len(msg.get('content', '')) for msg in conversation)
            token_count = char_count // 4
        
        log_debug(f"Conversation management: {token_count} tokens, iteration {iteration_count}")
        
        if token_count <= max_tokens:
            break
        
        # Smart removal strategy
        if len(conversation) >= 4:  # Remove user-assistant pairs
            removed_user = conversation.pop(1)
            removed_assistant = conversation.pop(1) if len(conversation) > 1 else None
            
            user_chars = len(removed_user.get('content', ''))
            assistant_chars = len(removed_assistant.get('content', '')) if removed_assistant else 0
            
            if OPERATION_MODE == "debug":
                log_debug(f"Removed exchange: {user_chars} + {assistant_chars} chars")
                
        elif len(conversation) > 2:  # Remove single message
            removed = conversation.pop(1)
            if OPERATION_MODE == "debug":
                log_debug(f"Removed message: {len(removed.get('content', ''))} chars")
        else:
            break
            
        iteration_count += 1
    
    if iteration_count >= max_iterations:
        log_debug("Conversation management hit max iterations, proceeding anyway")
    
    return conversation

# --- Enhanced Streaming with Performance Optimization ---
class HybridStreamer(ov_genai.StreamerBase):
    """Advanced streaming with performance monitoring and smooth UX"""
    
    def __init__(self, tokenizer: AutoTokenizer, enable_throttling: bool = True):
        super().__init__()
        self.tokenizer = tokenizer
        self.q = queue.Queue()
        self.accumulated_tokens = []
        self.full_response = ""
        self.enable_throttling = enable_throttling
        self.token_count = 0
        self.start_time = time.time()
    
    def clean_text(self, text: str) -> str:
        """Advanced text cleaning with space preservation"""
        special_tokens = [
            '<|im_start|>', '<|im_end|>', 
            '<|system|>', '<|user|>', '<|assistant|>',
            '<|endoftext|>', '<|end|>', '<|start|>',
            '</s>', '<s>', '[INST]', '[/INST]',
            '<pad>', '<unk>', '<mask>'
        ]
        
        cleaned = text
        for token in special_tokens:
            cleaned = cleaned.replace(token, '')
        
        # Normalize whitespace but preserve structure
        cleaned = re.sub(r'\s+', ' ', cleaned)
        return cleaned.strip()
    
    def put(self, token_id: int) -> bool:
        self.accumulated_tokens.append(token_id)
        self.token_count += 1
        
        try:
            decoded_text = self.tokenizer.decode(self.accumulated_tokens, skip_special_tokens=True)
        except Exception as e:
            log_debug(f"Decoding error: {e}")
            decoded_text = self.tokenizer.decode(self.accumulated_tokens)
        
        if len(decoded_text) > len(self.full_response):
            new_text = decoded_text[len(self.full_response):]
            self.full_response = decoded_text
            
            if new_text.strip():
                cleaned_text = self.clean_text(new_text)
                if cleaned_text:
                    self.q.put(cleaned_text)
                    
                    # Throttling for smoother UX
                    if self.enable_throttling and STREAMING_DELAY_MS > 0:
                        time.sleep(STREAMING_DELAY_MS / 1000.0)
        
        return False
    
    def end(self):
        elapsed = time.time() - self.start_time
        tokens_per_sec = self.token_count / elapsed if elapsed > 0 else 0
        log_debug(f"Streaming completed: {self.token_count} tokens in {elapsed:.2f}s ({tokens_per_sec:.1f} tok/s)")
        self.q.put(None)
    
    def __iter__(self):
        return self
    
    def __next__(self):
        item = self.q.get()
        if item is None:
            raise StopIteration
        return item

# --- Performance Monitoring System ---
class AdvancedPerformanceTracker:
    """Comprehensive performance tracking with multiple metrics"""
    
    def __init__(self, window_size: int = PERFORMANCE_WINDOW_SIZE):
        self.window_size = window_size
        self.reset()
    
    def reset(self):
        self.total_requests = 0
        self.response_times = []
        self.token_counts = []
        self.input_token_counts = []
        self.error_counts = {"token_limit": 0, "model": 0, "other": 0}
        self.configuration = "unknown"
        self.device_used = "unknown"
        self.start_time = time.time()
    
    def update_request(self, response_time: float, input_tokens: int, output_tokens: int, 
                      config_type: str, device: str, error_type: str = None):
        self.total_requests += 1
        self.response_times.append(response_time)
        self.token_counts.append(output_tokens)
        self.input_token_counts.append(input_tokens)
        self.configuration = config_type
        self.device_used = device
        
        if error_type:
            self.error_counts[error_type] = self.error_counts.get(error_type, 0) + 1
        
        # Maintain rolling window
        if len(self.response_times) > self.window_size:
            self.response_times.pop(0)
            self.token_counts.pop(0)
            self.input_token_counts.pop(0)
    
    def get_comprehensive_metrics(self) -> Dict[str, Any]:
        if not self.response_times:
            return self._empty_metrics()
        
        # Calculate averages
        avg_response_time = sum(self.response_times) / len(self.response_times)
        avg_output_tokens = sum(self.token_counts) / len(self.token_counts)
        avg_input_tokens = sum(self.input_token_counts) / len(self.input_token_counts)
        
        # Calculate throughput
        total_tokens = sum(self.token_counts)
        total_time = sum(self.response_times)
        throughput = total_tokens / total_time if total_time > 0 else 0
        
        # Session statistics
        session_time = time.time() - self.start_time
        total_errors = sum(self.error_counts.values())
        success_rate = ((self.total_requests - total_errors) / self.total_requests * 100) if self.total_requests > 0 else 0
        
        return {
            "session": {
                "total_requests": self.total_requests,
                "session_duration": round(session_time, 1),
                "success_rate": round(success_rate, 1),
                "total_errors": total_errors
            },
            "performance": {
                "avg_response_time": round(avg_response_time, 2),
                "avg_input_tokens": round(avg_input_tokens, 1),
                "avg_output_tokens": round(avg_output_tokens, 1),
                "throughput_tokens_per_sec": round(throughput, 1),
                "last_response_time": round(self.response_times[-1], 2) if self.response_times else 0
            },
            "system": {
                "device": self.device_used,
                "configuration": self.configuration,
                "rolling_window_size": len(self.response_times),
                "operation_mode": OPERATION_MODE
            },
            "errors": dict(self.error_counts)
        }
    
    def _empty_metrics(self) -> Dict[str, Any]:
        return {
            "session": {"total_requests": 0, "session_duration": 0, "success_rate": 100, "total_errors": 0},
            "performance": {"avg_response_time": 0, "avg_input_tokens": 0, "avg_output_tokens": 0, "throughput_tokens_per_sec": 0, "last_response_time": 0},
            "system": {"device": "unknown", "configuration": "unknown", "rolling_window_size": 0, "operation_mode": OPERATION_MODE},
            "errors": {"token_limit": 0, "model": 0, "other": 0}
        }

# --- Initialize System ---
log_info(f"Model path: {model_path}")
log_info(f"Target device: {device}")
log_info(f"Operation mode: {OPERATION_MODE}")

print("\n⏳ Loading model with comprehensive fallback system...")
load_start_time = time.time()

# Load pipeline with cascade
pipe, device_used, config_used = try_load_pipeline_cascade(model_path, device)

if not pipe:
    log_error("Failed to load model with any configuration. Please check:")
    log_error("1. Model path exists and is valid")
    log_error("2. OpenVINO installation is correct")
    log_error("3. Device drivers are properly installed")
    exit(1)

# Load tokenizer
try:
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # Configure tokenizer
    if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
        
    log_success("Tokenizer loaded and configured")
    
except Exception as e:
    log_error(f"Failed to load tokenizer: {e}")
    exit(1)

# Create generation configuration
generation_mode = "quality" if OPERATION_MODE == "production" else "balanced"
generation_config = create_generation_config(tokenizer, generation_mode)

# Initialize performance tracker
performance_tracker = AdvancedPerformanceTracker()

load_end_time = time.time()
log_success(f"System initialized in {load_end_time - load_start_time:.2f} seconds")
log_info(f"Using device: {device_used} with configuration: {config_used}")

# --- System Prompt ---
system_prompt = (
    "You are a helpful AI assistant powered by OpenVINO on NPU. "
    "Provide accurate, concise responses while being friendly and informative. "
    "If asked about technical topics, explain them clearly and provide practical examples when helpful."
)

# --- Main Chat Function ---
def hybrid_chat_function(message: str, history: list):
    """
    Hybrid chat function combining robust error handling with performance optimization
    """
    request_start_time = time.time()
    input_token_count = 0
    output_token_count = 0
    error_type = None
    
    try:
        # Input validation
        if not message or not message.strip():
            yield history
            return
            
        if len(message) > 2000:  # Reasonable limit
            message = semantic_aware_truncation(message, 1500)
            log_debug("Input message truncated due to length")
        
        # Build conversation
        conversation = [{"role": "system", "content": system_prompt}] + history
        conversation.append({"role": "user", "content": message})
        
        # Determine token limits based on configuration
        if "full_npuw" in config_used or "advanced" in config_used:
            max_conversation_tokens = MAX_CONVERSATION_TOKENS * 2  # Higher limit for advanced configs
            emergency_limit = EMERGENCY_TOKEN_LIMIT * 2
        else:
            max_conversation_tokens = MAX_CONVERSATION_TOKENS
            emergency_limit = EMERGENCY_TOKEN_LIMIT
        
        # Intelligent conversation management
        conversation = intelligent_conversation_management(
            conversation, tokenizer, max_conversation_tokens, config_used
        )
        
        # Generate prompt
        try:
            prompt = tokenizer.apply_chat_template(
                conversation, add_generation_prompt=True, tokenize=False
            )
            input_token_count = len(tokenizer.encode(prompt))
            
        except Exception as e:
            log_error(f"Failed to apply chat template: {e}")
            error_type = "other"
            yield history + [{"role": "assistant", "content": "❌ Error processing conversation format. Please try starting a new chat."}]
            return
        
        # Emergency truncation check
        if input_token_count > emergency_limit:
            log_debug(f"Emergency truncation needed: {input_token_count} > {emergency_limit}")
            
            # Try semantic truncation of user message
            if len(message) > MAX_MESSAGE_LENGTH:
                truncated_message = semantic_aware_truncation(message, MAX_MESSAGE_LENGTH)
                conversation[-1]["content"] = truncated_message
                
                try:
                    prompt = tokenizer.apply_chat_template(
                        conversation, add_generation_prompt=True, tokenize=False
                    )
                    input_token_count = len(tokenizer.encode(prompt))
                    log_debug(f"After emergency truncation: {input_token_count} tokens")
                    
                except Exception as e:
                    log_error(f"Emergency truncation failed: {e}")
                    error_type = "token_limit"
                    yield history + [{"role": "assistant", "content": "❌ Message too long for current configuration. Please try a shorter message."}]
                    return
        
        # Update history for UI
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ""})
        
        # Initialize streaming
        streamer = HybridStreamer(tokenizer, enable_throttling=(OPERATION_MODE == "production"))
        
        # Generate response in separate thread
        def generate():
            try:
                pipe.generate(prompt, generation_config, streamer)
            except Exception as e:
                log_error(f"Generation error: {e}")
                streamer.q.put(f"❌ Generation failed: {str(e)[:100]}...")
                streamer.q.put(None)
        
        generation_thread = Thread(target=generate, daemon=True)
        generation_thread.start()
        
        # Stream response to UI
        try:
            for new_text in streamer:
                if new_text:
                    history[-1]["content"] += new_text
                    output_token_count = len(tokenizer.encode(history[-1]["content"]))
                    yield history
                    
        except Exception as e:
            log_error(f"Streaming error: {e}")
            error_type = "other"
            
        generation_thread.join(timeout=30.0)  # Prevent hanging
        
        if generation_thread.is_alive():
            log_error("Generation thread timed out")
            error_type = "other"
        
    except Exception as e:
        log_error(f"Chat function error: {e}")
        error_type = "other"
        
        # Categorize error for better user feedback
        error_msg = str(e).lower()
        if any(keyword in error_msg for keyword in ["token", "length", "sequence"]):
            error_type = "token_limit"
            user_msg = "⚠️ Input too long for current model configuration. Try a shorter message or start a new chat."
        elif any(keyword in error_msg for keyword in ["memory", "resource", "allocation"]):
            user_msg = "⚠️ System resources exceeded. Please start a new conversation."
        else:
            user_msg = f"❌ Unexpected error occurred. Error details: {str(e)[:150]}..."
        
        if not history or history[-1]["role"] != "assistant":
            history.append({"role": "assistant", "content": user_msg})
        else:
            history[-1]["content"] = user_msg
            
        yield history
    
    finally:
        # Update performance metrics
        request_time = time.time() - request_start_time
        performance_tracker.update_request(
            request_time, input_token_count, output_token_count, 
            config_used, device_used, error_type
        )
        
        # Log performance for debug mode
        if OPERATION_MODE == "debug":
            metrics = performance_tracker.get_comprehensive_metrics()
            perf = metrics["performance"]
            log_debug(f"Request: {request_time:.2f}s, {input_token_count}→{output_token_count} tokens, {perf['throughput_tokens_per_sec']:.1f} tok/s")

# --- Gradio Interface ---
def create_gradio_interface():
    """Create the hybrid Gradio interface"""
    
    with gr.Blocks(
        theme=gr.themes.Soft(),
        title="Hybrid Qwen Chat",
        css="""
        .gradio-container { max-width: 1400px; margin: auto; }
        .chatbot { height: 650px; }
        .performance-panel { background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 10px 0; }
        """,
        fill_height=True
    ) as demo:
        
        # Header
        gr.Markdown(f"""
        # 🤖 Hybrid Qwen Chat System
        
        **Status**: ✅ Loaded on **{device_used}** using **{config_used}** configuration  
        **Model**: Qwen3-8B (INT4 Channel-wise)  
        **Mode**: {OPERATION_MODE.title()} mode with comprehensive fallback system  
        **Generation**: {generation_mode.title()} quality settings
        """)
        
        # Main chat interface
        chatbot = gr.Chatbot(
            label="Conversation", 
            height=650, 
            type='messages',
            avatar_images=(None, "🤖"),
            show_copy_button=True,
            show_share_button=False
        )
        
        # Input section
        with gr.Row():
            msg = gr.Textbox(
                label="Your Message", 
                placeholder=f"Ask me anything... (Mode: {OPERATION_MODE}, Device: {device_used})", 
                scale=8,
                max_lines=4,
                show_label=False
            )
            submit_btn = gr.Button("Send", variant="primary", scale=1)
        
        # Control buttons
        with gr.Row():
            clear_btn = gr.Button("🗑️ Clear Chat", variant="secondary")
            info_btn = gr.Button("ℹ️ System Info", variant="secondary") 
            perf_btn = gr.Button("📊 Performance", variant="secondary")
            mode_btn = gr.Button("🔧 Toggle Debug", variant="secondary")
        
        # Performance panel (collapsible)
        with gr.Row(visible=False) as perf_row:
            with gr.Column():
                perf_display = gr.JSON(label="Performance Metrics", visible=False)
        
        # System info panel (collapsible)  
        with gr.Row(visible=False) as info_row:
            with gr.Column():
                info_display = gr.Markdown(visible=False)
        
        # Event handlers
        msg.submit(hybrid_chat_function, [msg, chatbot], chatbot, queue=True).then(
            lambda: gr.update(value=""), None, [msg], queue=False
        )
        
        submit_btn.click(hybrid_chat_function, [msg, chatbot], chatbot, queue=True).then(
            lambda: gr.update(value=""), None, [msg], queue=False
        )
        
        clear_btn.click(
            lambda: ([], ""), None, [chatbot, msg], queue=False
        ).then(
            lambda: performance_tracker.reset(), None, None, queue=False
        )
        
        # System info function
        def show_system_info():
            metrics = performance_tracker.get_comprehensive_metrics()
            
            info = f"""
            ## 🖥️ System Configuration
            - **Device**: {device_used}
            - **Configuration**: {config_used}
            - **Model Path**: `{model_path}`
            - **Operation Mode**: {OPERATION_MODE}
            - **Generation Mode**: {generation_mode}
            
            ## ⚙️ Current Settings  
            - **Max Conversation Tokens**: {MAX_CONVERSATION_TOKENS}
            - **Emergency Token Limit**: {EMERGENCY_TOKEN_LIMIT}
            - **Streaming Delay**: {STREAMING_DELAY_MS}ms
            - **Performance Window**: {PERFORMANCE_WINDOW_SIZE} requests
            
            ## 📊 Generation Config
            - **Sampling**: {"Enabled" if generation_config.do_sample else "Disabled"}
            - **Temperature**: {getattr(generation_config, 'temperature', 'N/A')}
            - **Top-p**: {getattr(generation_config, 'top_p', 'N/A')}
            - **Max Tokens**: {generation_config.max_new_tokens}
            - **Repetition Penalty**: {generation_config.repetition_penalty}
            
            ## 🔄 Session Statistics
            - **Total Requests**: {metrics['session']['total_requests']}
            - **Session Duration**: {metrics['session']['session_duration']}s  
            - **Success Rate**: {metrics['session']['success_rate']}%
            """
            
            return gr.update(value=info, visible=True), gr.update(visible=True)
        
        info_btn.click(show_system_info, None, [info_display, info_row])
        
        # Performance metrics function
        def show_performance():
            metrics = performance_tracker.get_comprehensive_metrics()
            return gr.update(value=metrics, visible=True), gr.update(visible=True)
        
        perf_btn.click(show_performance, None, [perf_display, perf_row])
        
        # Mode toggle function
        def toggle_debug_mode():
            global OPERATION_MODE
            OPERATION_MODE = "production" if OPERATION_MODE == "debug" else "debug"
            log_info(f"Switched to {OPERATION_MODE} mode")
            return gr.update(value=f"Mode: {OPERATION_MODE}")
        
        mode_btn.click(
            toggle_debug_mode, None, None
        ).then(
            lambda: gr.update(placeholder=f"Ask me anything... (Mode: {OPERATION_MODE}, Device: {device_used})"),
            None, msg
        )
        
        # Example prompts
        gr.Examples(
            examples=[
                "Explain how neural networks learn and adapt",
                "Write a Python function to implement binary search", 
                "What are the latest developments in AI hardware acceleration?",
                "Compare different optimization techniques for deep learning models",
                "How does OpenVINO optimize inference for different hardware?"
            ], 
            inputs=msg,
            label="Example Prompts"
        )
        
        return demo

# --- Launch Application ---
if __name__ == "__main__":
    log_info("🌐 Starting Hybrid Gradio Interface...")
    log_info("🎯 Hybrid system features:")
    log_info("   • Comprehensive device and configuration fallback cascade")
    log_info("   • Semantic-aware truncation with punctuation boundary detection")
    log_info("   • Advanced performance monitoring with rolling metrics") 
    log_info("   • Enhanced streaming with configurable throttling")
    log_info("   • Professional error handling with categorized user feedback")
    log_info("   • Switchable debug/production modes")
    
    demo = create_gradio_interface()
    
    try:
        demo.queue(
            max_size=20,
            default_concurrency_limit=3
        ).launch(
            share=False,
            server_name="127.0.0.1", 
            server_port=7860,
            show_error=True,
            quiet=(OPERATION_MODE == "production")
        )
    except KeyboardInterrupt:
        log_info("🛑 Application stopped by user")
    except Exception as e:
        log_error(f"Application error: {e}")

================================================================================

FILE: archive\gradio_qwen_optimized.py
----------------------------------------
#!/usr/bin/env python3
"""
RAG-Enhanced Qwen Chat - Advanced NPU Optimization
==================================================

RAG notebook-inspired optimizations applied:
- Stateful chat sessions with proper session management
- NPU model reshaping with static shapes for better compilation
- Enhanced generation configuration with temperature/sampling controls
- Professional streaming patterns with advanced text processing
- Comprehensive error handling with intelligent fallback strategies
- Real-time performance monitoring and metrics collection
- Advanced UI controls for generation parameters
- Context-aware conversation management
"""

import gradio as gr
import openvino_genai as ov_genai
import openvino as ov
import openvino.properties as props
import openvino.properties.hint as hints
from transformers import AutoTokenizer
import time
import queue
import re
import json
from threading import Thread
from typing import Optional, Dict, Any, Tuple, List
from dataclasses import dataclass
import numpy as np

# --- Enhanced Configuration Constants ---
print("🚀 Starting RAG-Enhanced Qwen Chat with Advanced NPU Optimization...")

@dataclass
class ModelConfig:
    """Centralized model configuration with NPU optimizations"""
    model_path: str = r"C:\OpenVinoModels\qwen3-8b-int4-cw-ov"
    primary_device: str = "NPU"
    fallback_device: str = "CPU"
    cache_dir: str = r"C:\temp\.ovcache"
    enable_reshaping: bool = True
    use_stateful_chat: bool = True
    
config = ModelConfig()
print(f"📁 Model: {config.model_path}")
print(f"🖥️  Primary device: {config.primary_device}")
print(f"🎯 NPU reshaping: {'enabled' if config.enable_reshaping else 'disabled'}")
print(f"💬 Stateful chat: {'enabled' if config.use_stateful_chat else 'disabled'}")

# Enhanced constants with NPU-specific optimizations
@dataclass
class TokenLimits:
    """NPU-optimized token management constants"""
    MAX_PROMPT_LEN: int = 2048  # NPU supports up to 2048 for prefill
    MIN_RESPONSE_LEN: int = 256  # NPU optimization requirement
    CONVERSATION_LIMIT: int = 1024  # Conservative conversation limit
    EMERGENCY_LIMIT: int = 1500    # Hard emergency limit
    MAX_MESSAGE_LENGTH: int = 300  # Individual message limit
    TRUNCATION_PREVIEW: int = 150  # Preview length for truncation
    STREAMING_BATCH_SIZE: int = 4  # Process tokens in batches for smoothness
    
limits = TokenLimits()
print(f"📊 Token limits - Conversation: {limits.CONVERSATION_LIMIT}, Max prompt: {limits.MAX_PROMPT_LEN}")

# Advanced configuration factory with NPU reshaping support
def get_npu_reshape_config() -> Dict[str, Any]:
    """NPU-specific reshaping configuration for static shapes"""
    return {
        "NPUW_LLM": "YES",
        "NPUW_LLM_MAX_PROMPT_LEN": limits.MAX_PROMPT_LEN,
        "NPUW_LLM_MIN_RESPONSE_LEN": limits.MIN_RESPONSE_LEN, 
        "NPUW_LLM_BATCH_DIM": 0,
        "NPUW_LLM_SEQ_LEN_DIM": 1,
        "NPU_USE_NPUW": "YES",
        "CACHE_MODE": "OPTIMIZE_SPEED",
        "INFERENCE_PRECISION_HINT": "f16",
        "NPUW_LLM_PREFILL_HINT": "LATENCY",
        "NPUW_LLM_GENERATE_HINT": "LATENCY"
    }

def get_device_config(device_type: str, mode: str = "basic") -> Dict[str, Any]:
    """Enhanced device configuration with multiple optimization levels"""
    base_config = {
        hints.performance_mode: hints.PerformanceMode.LATENCY,
        props.cache_dir: config.cache_dir,
        props.streams.num: 1,
        props.inference_num_threads: 1 if device_type == "NPU" else 4
    }
    
    if device_type == "NPU":
        if mode == "reshape_npu":
            # Full NPU reshaping with static shapes
            return {**base_config, **get_npu_reshape_config()}
        elif mode == "enhanced_npu":
            # Enhanced NPU without full reshaping
            return {
                **base_config,
                "NPUW_LLM": "YES",
                "NPUW_LLM_MAX_PROMPT_LEN": limits.MAX_PROMPT_LEN,
                "CACHE_MODE": "OPTIMIZE_SPEED"
            }
    
    return base_config

print("🔧 Using basic NPU configuration (bypasses NPUW compilation issues)")
print("📁 Model path:", model_path)
print("🖥️  Target device:", device)

# --- Enhanced System Prompt with NPU Optimization Context ---
SYSTEM_PROMPT = (
    "You are an intelligent AI assistant optimized for efficient NPU inference. "
    "Provide accurate, helpful responses while being mindful of computational constraints. "
    "Be concise yet informative, and adapt your response length to the complexity of the question."
)

# --- Advanced Generation Configuration Management ---
class GenerationConfigManager:
    """Manages generation configurations with different quality/speed profiles"""
    
    @staticmethod
    def get_base_config() -> ov_genai.GenerationConfig:
        config = ov_genai.GenerationConfig()
        config.max_new_tokens = limits.MAX_PROMPT_LEN  # Use NPU-optimized limit
        config.repetition_penalty = 1.1
        return config
    
    @staticmethod 
    def get_balanced_config() -> ov_genai.GenerationConfig:
        """Balanced quality/speed for general use"""
        config = GenerationConfigManager.get_base_config()
        config.do_sample = True
        config.temperature = 0.7
        config.top_p = 0.9
        config.top_k = 50
        return config
        
    @staticmethod
    def get_creative_config() -> ov_genai.GenerationConfig:
        """Higher creativity for creative tasks"""
        config = GenerationConfigManager.get_base_config()
        config.do_sample = True
        config.temperature = 0.8
        config.top_p = 0.95
        config.top_k = 40
        config.repetition_penalty = 1.05
        return config
    
    @staticmethod
    def get_precise_config() -> ov_genai.GenerationConfig:
        """Lower temperature for factual tasks"""
        config = GenerationConfigManager.get_base_config()
        config.do_sample = True  
        config.temperature = 0.3
        config.top_p = 0.8
        config.top_k = 30
        config.repetition_penalty = 1.15
        return config

# Default to balanced configuration
generation_config = GenerationConfigManager.get_balanced_config()
print(f"🎛️  Generation config: T={generation_config.temperature}, top_p={generation_config.top_p}, max_tokens={generation_config.max_new_tokens}")

# --- Advanced Pipeline Loading with Comprehensive Fallback Strategy ---
print("\n⏳ Loading model with RAG-enhanced configuration and intelligent fallback...")
load_start_time = time.time()

class PipelineLoader:
    """Advanced pipeline loader with comprehensive fallback strategies"""
    
    @staticmethod
    def get_configuration_hierarchy(device: str) -> List[Tuple[str, Dict[str, Any]]]:
        """Get prioritized configuration list for device"""
        if device == "NPU":
            return [
                ("reshape_npu", get_device_config("NPU", "reshape_npu")),
                ("enhanced_npu", get_device_config("NPU", "enhanced_npu")), 
                ("basic_npu", get_device_config("NPU", "basic")),
                ("minimal_npu", {}),
                ("cpu_fallback", get_device_config("CPU", "basic"))
            ]
        else:
            return [
                ("optimized_cpu", get_device_config("CPU", "enhanced")),
                ("basic_cpu", get_device_config("CPU", "basic")),
                ("minimal_cpu", {})
            ]
    
    @staticmethod
    def try_load_pipeline(model_path: str, device: str) -> Tuple[Optional[ov_genai.LLMPipeline], str, str]:
        """Try loading pipeline with comprehensive fallback strategy"""
        configs = PipelineLoader.get_configuration_hierarchy(device)
        
        for config_name, config in configs:
            try:
                target_device = "CPU" if "cpu" in config_name else device
                print(f"🔄 Attempting {target_device} with {config_name} configuration...")
                
                if config:
                    pipe = ov_genai.LLMPipeline(model_path, target_device, **config)
                else:
                    pipe = ov_genai.LLMPipeline(model_path, target_device)
                    
                print(f"✅ Success: {target_device} with {config_name}")
                return pipe, target_device, config_name
                
            except Exception as e:
                error_msg = str(e)
                if "compile" in error_msg.lower() or "npuw" in error_msg.lower():
                    print(f"⚠️  {config_name} failed (compilation): {error_msg[:100]}...")
                elif "memory" in error_msg.lower():
                    print(f"⚠️  {config_name} failed (memory): {error_msg[:100]}...")
                else:
                    print(f"⚠️  {config_name} failed: {error_msg[:100]}...")
                continue
        
        return None, device, "failed"

# Load pipeline with comprehensive fallback
try:
    pipe, device_used, config_used = PipelineLoader.try_load_pipeline(config.model_path, config.primary_device)
    
    if not pipe:
        raise RuntimeError("All configuration attempts failed")
    
    # Load and configure tokenizer with enhanced settings
    print("🔤 Loading tokenizer with enhanced configuration...")
    tokenizer = AutoTokenizer.from_pretrained(config.model_path)
    
    # Enhanced tokenizer configuration
    if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # Determine capabilities based on loaded configuration
    npu_optimized = ("npu" in config_used.lower() and device_used == "NPU")
    supports_reshaping = ("reshape" in config_used)
    
    print(f"✅ Model loaded: {device_used} with {config_used} configuration")
    print(f"🎯 NPU optimized: {npu_optimized}")
    print(f"📐 Supports reshaping: {supports_reshaping}")
    
except Exception as e:
    print(f"❌ Fatal error during model loading: {e}")
    print("💡 Try running with CPU device or check model path")
    exit(1)

load_end_time = time.time()
load_end_time = time.time()
load_time_seconds = load_end_time - load_start_time
print(f"⚡ Model loaded in {load_time_seconds:.2f} seconds")
print(f"🔋 Final device: {device_used}")
print(f"⚙️ Final configuration: {config_used}")
print(f"🏁 Ready for RAG-enhanced inference!")

# Store load time for performance tracking
performance_tracker.load_time = load_time_seconds

# --- Professional Streaming Class with Advanced Text Processing ---
class ProfessionalStreamer(ov_genai.StreamerBase):
    """Enhanced streamer with professional text processing and performance monitoring"""
    
    def __init__(self, tokenizer: AutoTokenizer, batch_size: int = None):
        super().__init__()
        self.tokenizer = tokenizer
        self.q = queue.Queue()
        self.accumulated_tokens = []
        self.full_response = ""
        self.batch_size = batch_size or limits.STREAMING_BATCH_SIZE
        self.token_count = 0
        self.start_time = time.time()
        
    def clean_text(self, text: str) -> str:
        """Advanced text cleaning with context-aware processing"""
        # Comprehensive special token removal
        special_tokens = [
            '<|im_start|>', '<|im_end|>', '<|system|>', '<|user|>', '<|assistant|>',
            '<|endoftext|>', '<|end|>', '<|start|>', '</s>', '<s>',
            '[INST]', '[/INST]', '<pad>', '<unk>', '<mask>',
            '\n\n\n', '\t\t', '  '  # Extra whitespace patterns
        ]
        
        cleaned = text
        for token in special_tokens:
            cleaned = cleaned.replace(token, '' if token.startswith('<') else ' ')
        
        # Advanced whitespace normalization while preserving structure
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)  # Max 2 consecutive newlines
        cleaned = re.sub(r'[ \t]+', ' ', cleaned)     # Normalize spaces/tabs
        cleaned = re.sub(r'\n ', '\n', cleaned)       # Remove space after newline
        
        return cleaned.strip()
    
    def put(self, token_id: int) -> bool:
        self.accumulated_tokens.append(token_id)
        self.token_count += 1
        
        # Process tokens in batches for smoother streaming
        if len(self.accumulated_tokens) % self.batch_size == 0 or self.token_count < 5:
            try:
                # Use skip_special_tokens for cleaner output
                decoded_text = self.tokenizer.decode(
                    self.accumulated_tokens, 
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=True
                )
            except Exception:
                # Fallback decoding
                decoded_text = self.tokenizer.decode(self.accumulated_tokens)
            
            if len(decoded_text) > len(self.full_response):
                new_text = decoded_text[len(self.full_response):]
                self.full_response = decoded_text
                
                if new_text.strip():  # Only send meaningful text
                    cleaned_new_text = self.clean_text(new_text)
                    if cleaned_new_text:
                        self.q.put(cleaned_new_text)
        
        return False
    
    def end(self):
        # Final cleanup and performance logging
        if self.full_response:
            final_cleaned = self.clean_text(self.full_response)
            if final_cleaned != self.full_response.strip():
                # Send final cleaned version if different
                self.q.put("\n[Final cleanup applied]")
        
        elapsed = time.time() - self.start_time
        tokens_per_sec = self.token_count / elapsed if elapsed > 0 else 0
        print(f"🚀 Streaming completed: {self.token_count} tokens in {elapsed:.2f}s ({tokens_per_sec:.1f} tok/s)")
        
        self.q.put(None)
    
    def __iter__(self):
        return self
    
    def __next__(self):
        item = self.q.get()
        if item is None:
            raise StopIteration
        return item

# --- Advanced Token Management with Context Awareness ---
class ContextAwareTokenManager:
    """Advanced token management with semantic awareness and NPU optimization"""
    
    @staticmethod
    def semantic_truncation(text: str, max_length: int, preserve_context: bool = True) -> str:
        """Advanced semantic-aware truncation with context preservation"""
        if len(text) <= max_length:
            return text
        
        # Strategy 1: Sentence boundary truncation
        sentences = re.split(r'[.!?]+\s+', text)
        if len(sentences) > 1:
            current_length = 0
            preserved_sentences = []
            
            for sentence in sentences:
                test_length = current_length + len(sentence) + 2
                if test_length <= max_length * 0.85:  # Leave buffer for context
                    preserved_sentences.append(sentence)
                    current_length = test_length
                else:
                    break
            
            if preserved_sentences and preserve_context:
                result = '. '.join(preserved_sentences) + '.'
                remaining = text[len(result):].strip()
                if len(remaining) > 30:
                    context_preview = remaining[:50].strip()
                    result += f" [...context: {context_preview}...]"
                return result
        
        # Strategy 2: Paragraph boundary truncation  
        paragraphs = text.split('\n\n')
        if len(paragraphs) > 1 and len(paragraphs[0]) < max_length * 0.7:
            return paragraphs[0] + f" [...{len(paragraphs)-1} more paragraphs...]"
        
        # Strategy 3: Smart word boundary with context
        truncated = text[:max_length - 100]  # Leave room for context
        last_space = truncated.rfind(' ')
        
        if last_space > max_length * 0.6:
            truncated = truncated[:last_space]
        
        # Add intelligent context preview
        remaining_preview = text[len(truncated):len(truncated) + 60].strip()
        return f"{truncated} [...continues: {remaining_preview}...]"
    
    @staticmethod
    def calculate_conversation_tokens(conversation: List[Dict], tokenizer) -> int:
        """Accurate token counting for conversation management"""
        if not conversation:
            return 0
            
        try:
            # Use chat template for accurate token counting
            formatted = tokenizer.apply_chat_template(
                conversation, 
                add_generation_prompt=True, 
                tokenize=False
            )
            return len(tokenizer.encode(formatted))
        except Exception as e:
            print(f"⚠️  Token calculation fallback: {e}")
            # Fallback: rough estimation
            total_chars = sum(len(msg.get('content', '')) for msg in conversation)
            return int(total_chars * 0.25)  # Rough char-to-token ratio

    @staticmethod
    def smart_truncate_conversation(
        conversation: List[Dict], 
        tokenizer, 
        max_tokens: int = None,
        preserve_system: bool = True
    ) -> List[Dict]:
        """Intelligent conversation truncation with context preservation"""
        if max_tokens is None:
            max_tokens = limits.CONVERSATION_LIMIT
        
        if len(conversation) <= 2:  # System + current message
            return conversation
        
        print(f"🔄 Smart conversation truncation (target: {max_tokens} tokens)")
        
        # Calculate initial token count
        initial_tokens = ContextAwareTokenManager.calculate_conversation_tokens(conversation, tokenizer)
        print(f"📊 Initial tokens: {initial_tokens}")
        
        if initial_tokens <= max_tokens:
            return conversation
        
        # Preserve system message and current user message
        system_msg = conversation[0] if preserve_system else None
        current_msg = conversation[-1] if conversation else None
        middle_conversation = conversation[1:-1] if len(conversation) > 2 else []
        
        # Smart removal strategy: remove oldest exchanges first
        while middle_conversation and initial_tokens > max_tokens:
            # Remove in pairs (user-assistant exchanges) when possible
            if len(middle_conversation) >= 2:
                # Find user-assistant pairs to remove together
                for i in range(len(middle_conversation) - 1):
                    if (middle_conversation[i].get('role') == 'user' and 
                        middle_conversation[i + 1].get('role') == 'assistant'):
                        removed_pair = [middle_conversation.pop(i), middle_conversation.pop(i)]
                        print(f"🗑️  Removed exchange: {len(removed_pair[0].get('content', ''))} + {len(removed_pair[1].get('content', ''))} chars")
                        break
                else:
                    # No pairs found, remove oldest single message
                    removed = middle_conversation.pop(0)
                    print(f"🗑️  Removed: {removed.get('role')} ({len(removed.get('content', ''))} chars)")
            else:
                # Remove remaining single message
                removed = middle_conversation.pop(0)
                print(f"🗑️  Removed: {removed.get('role')} ({len(removed.get('content', ''))} chars)")
            
            # Recalculate token count
            test_conversation = []
            if system_msg:
                test_conversation.append(system_msg)
            test_conversation.extend(middle_conversation)
            if current_msg:
                test_conversation.append(current_msg)
            
            initial_tokens = ContextAwareTokenManager.calculate_conversation_tokens(test_conversation, tokenizer)
            print(f"📊 Updated tokens: {initial_tokens}")
        
        # Reconstruct final conversation
        final_conversation = []
        if system_msg:
            final_conversation.append(system_msg)
        final_conversation.extend(middle_conversation)
        if current_msg:
            final_conversation.append(current_msg)
        
        return final_conversation

# --- RAG-Enhanced Chat Function with Comprehensive Features ---
def rag_enhanced_chat_function(message: str, history: list, generation_mode: str = "balanced"):
    """RAG-enhanced chat function with advanced features:
    - Stateful session management
    - Dynamic generation configuration
    - Context-aware token management  
    - Professional streaming with monitoring
    - Intelligent error handling
    """
    request_start = time.time()
    
    try:
        # Validate input
        if not message.strip():
            yield history
            return
        
        # Select generation configuration based on mode
        if generation_mode == "creative":
            current_config = GenerationConfigManager.get_creative_config()
        elif generation_mode == "precise":
            current_config = GenerationConfigManager.get_precise_config()
        else:
            current_config = GenerationConfigManager.get_balanced_config()
        
        print(f"🎛️  Using {generation_mode} mode (T={current_config.temperature})")
        
        # Enhanced message processing with semantic truncation
        if len(message) > limits.MAX_MESSAGE_LENGTH:
            original_length = len(message)
            message = ContextAwareTokenManager.semantic_truncation(message, limits.MAX_MESSAGE_LENGTH)
            print(f"📝 Message truncated: {original_length} → {len(message)} chars")
            
            # Provide user feedback about truncation
            history.append({
                "role": "assistant",
                "content": f"⚠️  Your message was truncated from {original_length:,} to {len(message)} characters for NPU optimization."
            })
            yield history
        
        # Initialize professional streaming
        streamer = ProfessionalStreamer(tokenizer)
        
        # Build conversation with system prompt
        conversation = [{"role": "system", "content": SYSTEM_PROMPT}] + history
        conversation.append({"role": "user", "content": message})
        
        # Apply intelligent conversation management
        max_tokens = limits.MAX_PROMPT_LEN if npu_optimized else limits.CONVERSATION_LIMIT
        conversation = ContextAwareTokenManager.smart_truncate_conversation(
            conversation, tokenizer, max_tokens=max_tokens
        )
        
        # Calculate final token usage
        final_tokens = ContextAwareTokenManager.calculate_conversation_tokens(conversation, tokenizer)
        print(f"🎯 Final conversation tokens: {final_tokens}")
        
        # Emergency token limit check
        if final_tokens > limits.EMERGENCY_LIMIT:
            error_msg = f"⚠️  Conversation too long ({final_tokens} tokens). Please start a new chat."
            history.append({"role": "assistant", "content": error_msg})
            yield history
            return
        
        # Update UI with user message
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ""})
        yield history
        
        # Check if we should use stateful chat API
        if config.use_stateful_chat and hasattr(pipe, 'generate'):
            # For stateful API, we only send the current message
            print("💬 Using stateful chat mode (sending current message only)")
            
            def generate_stateful():
                try:
                    pipe.generate(message, current_config, streamer)
                except Exception as e:
                    print(f"❌ Stateful generation error: {e}")
                    streamer.q.put(f"❌ Generation failed: {str(e)[:100]}...")
                    streamer.q.put(None)
        else:
            # Traditional full-prompt mode
            print("📜 Using full-prompt mode")
            prompt = tokenizer.apply_chat_template(
                conversation, add_generation_prompt=True, tokenize=False
            )
            
            def generate_full():
                try:
                    pipe.generate(prompt, current_config, streamer)
                except Exception as e:
                    print(f"❌ Full-prompt generation error: {e}")
                    streamer.q.put(f"❌ Generation failed: {str(e)[:100]}...")
                    streamer.q.put(None)
        
        # Execute generation in separate thread
        generation_thread = Thread(target=generate_stateful if config.use_stateful_chat else generate_full)
        generation_thread.start()
        
        # Stream updates to UI with enhanced processing
        for chunk in streamer:
            if chunk:  # Only process non-empty chunks
                history[-1]["content"] += chunk
                yield history
        
        generation_thread.join(timeout=30.0)  # Prevent hanging
        
        # Update performance metrics
        request_time = time.time() - request_start
        output_tokens = len(tokenizer.encode(history[-1]["content"])) if history and history[-1]["content"] else 0
        
        performance_tracker.update(request_time, final_tokens, output_tokens, config_used)
        metrics = performance_tracker.get_metrics()
        
        print(f"🏁 Request completed: {request_time:.2f}s, {final_tokens}→{output_tokens} tokens ({metrics.get('tokens_per_second', 0):.1f} tok/s)")

    except Exception as e:
        error_msg = str(e)
        print(f"❌ RAG-enhanced chat error: {error_msg}")
        
        # Intelligent error handling with specific responses
        if "token" in error_msg.lower() or "length" in error_msg.lower():
            if "2048" in error_msg or "1024" in error_msg:
                error_response = f"⚠️  NPU token limit exceeded. Current conversation: {len(history)} messages. Please start a new chat or use a shorter message."
            else:
                error_response = "⚠️  Message too long for current NPU configuration. Try breaking it into smaller parts."
        elif "memory" in error_msg.lower() or "allocation" in error_msg.lower():
            error_response = "⚠️  NPU memory limit reached. Starting a new conversation will help."
        elif "compile" in error_msg.lower() or "npuw" in error_msg.lower():
            error_response = "⚠️  NPU compilation issue detected. The model may need to be re-optimized for your NPU configuration."
        elif "timeout" in error_msg.lower():
            error_response = "⏱️  Generation timed out. This may be due to high NPU load. Please try again."
        else:
            error_response = f"❌ Unexpected error: {error_msg[:150]}... Please try again or restart the chat."
        
        # Ensure we don't duplicate error messages
        if not history or history[-1].get("role") != "assistant":
            history.append({"role": "assistant", "content": error_response})
        else:
            history[-1]["content"] = error_response
        
        yield history

# --- Advanced Performance Monitoring ---
class EnhancedPerformanceTracker:
    """Comprehensive performance tracking with detailed metrics"""
    
    def __init__(self, window_size: int = 20):
        self.window_size = window_size
        self.total_requests = 0
        self.response_times = []
        self.input_tokens = []
        self.output_tokens = []
        self.configurations = []
        self.session_start = time.time()
        
    def update(self, response_time: float, input_tokens: int, output_tokens: int, config_type: str):
        """Update metrics with new request data"""
        self.total_requests += 1
        self.response_times.append(response_time)
        self.input_tokens.append(input_tokens)
        self.output_tokens.append(output_tokens)
        self.configurations.append(config_type)
        
        # Maintain sliding window
        if len(self.response_times) > self.window_size:
            self.response_times.pop(0)
            self.input_tokens.pop(0)
            self.output_tokens.pop(0)
            self.configurations.pop(0)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics"""
        if not self.response_times:
            return {
                "total_requests": 0,
                "session_duration": "0s",
                "avg_response_time": 0.0,
                "tokens_per_second": 0.0,
                "current_config": "none"
            }
        
        session_duration = time.time() - self.session_start
        avg_response_time = np.mean(self.response_times)
        total_output_tokens = sum(self.output_tokens)
        total_time = sum(self.response_times)
        tokens_per_second = total_output_tokens / total_time if total_time > 0 else 0
        
        return {
            "total_requests": self.total_requests,
            "session_duration": f"{session_duration:.1f}s",
            "avg_response_time": round(avg_response_time, 2),
            "avg_input_tokens": round(np.mean(self.input_tokens), 1),
            "avg_output_tokens": round(np.mean(self.output_tokens), 1),
            "tokens_per_second": round(tokens_per_second, 1),
            "current_config": self.configurations[-1] if self.configurations else "unknown",
            "window_size": len(self.response_times),
            "device": device_used,
            "supports_reshaping": supports_reshaping
        }

# Initialize performance tracker
performance_tracker = EnhancedPerformanceTracker()

# --- RAG-Enhanced Gradio Interface ---
# Create RAG-enhanced interface with advanced controls
with gr.Blocks(
    theme=gr.themes.Soft(),
    title="RAG-Enhanced Qwen Chat",
    css="""
    .gradio-container { max-width: 1400px; margin: auto; }
    .chatbot { height: 650px; }
    .control-panel { background: linear-gradient(90deg, #f0f9ff 0%, #e0f2fe 100%); padding: 15px; border-radius: 10px; margin: 10px 0; }
    .metrics-panel { background: #f8fafc; padding: 12px; border-radius: 8px; border: 1px solid #e2e8f0; }
    .status-indicator { font-weight: bold; padding: 4px 8px; border-radius: 4px; }
    .status-success { background: #dcfce7; color: #166534; }
    .status-warning { background: #fef3c7; color: #92400e; }
    """,
    fill_height=True
) as demo:
    
    # Enhanced header with detailed status
    with gr.Row():
        gr.Markdown(f"""
        # 🤖 RAG-Enhanced Qwen Chat with Advanced NPU Optimization
        
        <div class="status-indicator status-success">**Device**: {device_used}</div> 
        <div class="status-indicator status-{'success' if npu_optimized else 'warning'}">**Config**: {config_used}</div>
        <div class="status-indicator status-success">**Model**: Qwen3-8B INT4-CW</div>
        
        **Features**: Stateful sessions • NPU reshaping • Context-aware truncation • Performance monitoring  
        **Token Limits**: {limits.MAX_PROMPT_LEN} prompt • {limits.CONVERSATION_LIMIT} conversation • Dynamic management
        """)
    
    # Main chat interface with enhanced features
    chatbot = gr.Chatbot(
        label="RAG-Enhanced Conversation", 
        height=650,
        type='messages',
        avatar_images=("👤", "🤖"),
        show_copy_button=True,
        show_share_button=False,
        bubble_full_width=False,
        render_markdown=True
    )
    
    # Enhanced input controls with generation mode selection
    with gr.Row():
        with gr.Column(scale=8):
            msg = gr.Textbox(
                label="Your Message", 
                placeholder=f"Ask me anything... (Auto-truncated at {limits.MAX_MESSAGE_LENGTH} chars for NPU efficiency)", 
                max_lines=4,
                lines=2
            )
        with gr.Column(scale=2):
            generation_mode = gr.Dropdown(
                label="Generation Mode",
                choices=["balanced", "creative", "precise"],
                value="balanced",
                info="Adjust creativity vs accuracy"
            )
            submit_btn = gr.Button("🚀 Send", variant="primary", size="lg")
    
    # Advanced control panel
    with gr.Row(elem_classes=["control-panel"]):
        with gr.Column(scale=1):
            clear_btn = gr.Button("🗑️ New Chat", variant="secondary")
            info_btn = gr.Button("ℹ️ System Info", variant="secondary")
        with gr.Column(scale=1):
            perf_btn = gr.Button("📊 Metrics", variant="secondary")
            export_btn = gr.Button("💾 Export Chat", variant="secondary")
        with gr.Column(scale=2):
            # Real-time status display
            status_display = gr.HTML(
                f"<div class='metrics-panel'>Ready • {config_used} mode • {device_used} device</div>"
            )
        
    # Collapsible advanced panels
    with gr.Row(visible=False) as metrics_row:
        with gr.Column(scale=1, elem_classes=["metrics-panel"]):
            gr.Markdown("### 📊 Performance Metrics")
            metrics_display = gr.JSON(label="Detailed Metrics", visible=True)
        with gr.Column(scale=1, elem_classes=["metrics-panel"]):
            gr.Markdown("### 🎯 Generation Settings")
            current_temp = gr.Number(label="Temperature", value=0.7, interactive=False)
            current_top_p = gr.Number(label="Top-p", value=0.9, interactive=False)
            current_tokens = gr.Number(label="Max Tokens", value=limits.MAX_PROMPT_LEN, interactive=False)
    
    with gr.Row(visible=False) as export_row:
        with gr.Column():
            gr.Markdown("### 💾 Export Options")
            export_format = gr.Dropdown(
                label="Format",
                choices=["JSON", "Markdown", "Plain Text"],
                value="Markdown"
            )
            export_output = gr.Textbox(
                label="Exported Chat",
                lines=10,
                max_lines=20
            )
            
    # Helper functions for UI interactions
    def update_generation_settings(mode: str):
        """Update generation settings display based on mode"""
        if mode == "creative":
            config = GenerationConfigManager.get_creative_config()
        elif mode == "precise":
            config = GenerationConfigManager.get_precise_config()
        else:
            config = GenerationConfigManager.get_balanced_config()
        
        return (
            gr.update(value=config.temperature),
            gr.update(value=config.top_p),
            gr.update(value=config.max_new_tokens)
        )
    
    def export_conversation(history: list, format_type: str) -> str:
        """Export conversation in specified format"""
        if not history:
            return "No conversation to export."
        
        timestamp = time.strftime("%Y-%m-%d_%H-%M-%S")
        
        if format_type == "JSON":
            export_data = {
                "timestamp": timestamp,
                "device": device_used,
                "configuration": config_used,
                "conversation": history
            }
            return json.dumps(export_data, indent=2)
        
        elif format_type == "Markdown":
            lines = [f"# RAG-Enhanced Qwen Chat Export"]
            lines.append(f"**Date**: {timestamp}")
            lines.append(f"**Device**: {device_used}")
            lines.append(f"**Configuration**: {config_used}")
            lines.append("\n---\n")
            
            for msg in history:
                role = msg.get('role', 'unknown').title()
                content = msg.get('content', '')
                lines.append(f"## {role}\n")
                lines.append(f"{content}\n")
            
            return "\n".join(lines)
        
        else:  # Plain Text
            lines = [f"RAG-Enhanced Qwen Chat Export - {timestamp}"]
            lines.append(f"Device: {device_used} | Config: {config_used}")
            lines.append("=" * 50)
            
            for msg in history:
                role = msg.get('role', 'unknown').upper()
                content = msg.get('content', '')
                lines.append(f"\n[{role}]\n{content}")
            
            return "\n".join(lines)
    
    # Enhanced event handlers with advanced features
    def submit_with_mode(message: str, history: list, mode: str):
        """Submit message with specified generation mode"""
        return rag_enhanced_chat_function(message, history, mode)
    
    # Message submission with generation mode
    msg.submit(
        submit_with_mode, 
        [msg, chatbot, generation_mode], 
        chatbot
    ).then(
        lambda: gr.update(value=""), None, [msg], queue=False
    ).then(
        update_generation_settings,
        [generation_mode],
        [current_temp, current_top_p, current_tokens]
    )
    
    submit_btn.click(
        submit_with_mode, 
        [msg, chatbot, generation_mode], 
        chatbot
    ).then(
        lambda: gr.update(value=""), None, [msg], queue=False
    ).then(
        update_generation_settings,
        [generation_mode],
        [current_temp, current_top_p, current_tokens]
    )
    
    # Generation mode change updates settings display
    generation_mode.change(
        update_generation_settings,
        [generation_mode],
        [current_temp, current_top_p, current_tokens]
    )
    
    # Enhanced clear function with session management
    def clear_chat_session():
        """Clear chat and reset session if using stateful mode"""
        if config.use_stateful_chat and hasattr(pipe, 'finish_chat'):
            try:
                pipe.finish_chat()
                pipe.start_chat(SYSTEM_PROMPT)
                print("🔄 Stateful chat session reset")
            except Exception as e:
                print(f"⚠️  Session reset warning: {e}")
        return [], "", "Chat cleared • New session started"
    
    clear_btn.click(
        clear_chat_session,
        None, 
        [chatbot, msg, status_display],
        queue=False
    )
    
    def show_system_info():
        """Display comprehensive system information"""
        config_details = {
            "reshape_npu": "Full NPU reshaping with static shapes",
            "enhanced_npu": "Enhanced NPU with NPUW optimizations", 
            "basic_npu": "Basic NPU configuration",
            "minimal_npu": "Minimal NPU settings",
            "cpu_fallback": "CPU fallback mode"
        }
        
        info = f"""
        ## 🔧 System Configuration
        - **Device**: {device_used} ({"NPU-optimized" if npu_optimized else "Standard"})
        - **Configuration**: {config_details.get(config_used, config_used)}
        - **Model Path**: `{config.model_path}`
        - **Cache Directory**: `{config.cache_dir}`
        
        ## 🎯 Performance Specifications  
        - **Max Prompt Length**: {limits.MAX_PROMPT_LEN:,} tokens
        - **Conversation Limit**: {limits.CONVERSATION_LIMIT:,} tokens
        - **Message Length**: {limits.MAX_MESSAGE_LENGTH} characters
        - **Supports Reshaping**: {'Yes' if supports_reshaping else 'No'}
        
        ## 🧠 Model Architecture
        - **Base Model**: Qwen3ForCausalLM (8B parameters)
        - **Quantization**: INT4 channel-wise
        - **Optimization**: {'Stateful chat sessions' if config.use_stateful_chat else 'Traditional prompting'}
        - **Streaming**: Professional with batch processing
        
        ## 🚀 RAG Enhancements
        - NPU model reshaping with static shapes
        - Context-aware conversation management
        - Semantic truncation algorithms
        - Multi-tier configuration fallback
        - Real-time performance monitoring
        - Advanced generation controls
        """
        return info
    
    def toggle_metrics_display():
        return gr.update(visible=True)
    
    def toggle_export_display():
        return gr.update(visible=True)
    
    info_btn.click(
        lambda: gr.Info(show_system_info()),
        None, None
    )
    
    perf_btn.click(
        toggle_metrics_display,
        None, [metrics_row]
    ).then(
        lambda: performance_tracker.get_metrics(),
        None, metrics_display
    )
    
    export_btn.click(
        toggle_export_display,
        None, [export_row]
    )
    
    # Export functionality
    export_format.change(
        export_conversation,
        [chatbot, export_format],
        export_output
    )
    
    # Enhanced example prompts with generation mode suggestions
    gr.Examples(
        examples=[
            ["Explain quantum computing in simple terms", "precise"],
            ["Write a creative story about AI and humans", "creative"], 
            ["Write a Python function to calculate factorial", "precise"],
            ["What are some innovative ideas for renewable energy?", "creative"],
            ["How do neural networks work?", "balanced"],
            ["Compare NPU vs GPU for AI inference", "balanced"]
        ],
        inputs=[msg, generation_mode],
        label="Example Prompts (with suggested generation modes)"
    )

    # Initialize stateful chat session if enabled
    if config.use_stateful_chat and hasattr(pipe, 'start_chat'):
        try:
            pipe.start_chat(SYSTEM_PROMPT)
            print("💬 Stateful chat session initialized")
        except Exception as e:
            print(f"⚠️  Stateful chat initialization warning: {e}")

if __name__ == "__main__":
    print("\n🌐 Launching RAG-Enhanced Gradio Interface...")
    print(f"✨ Configuration: {device_used} with {config_used}")
    print("🚀 RAG-Enhanced Features Enabled:")
    print(f"   • NPU model reshaping: {'Yes' if supports_reshaping else 'No'}")
    print(f"   • Stateful chat sessions: {'Yes' if config.use_stateful_chat else 'No'}")
    print(f"   • Advanced generation controls: 3 modes available")
    print(f"   • Context-aware token management: {limits.MAX_PROMPT_LEN} token limit")
    print(f"   • Professional streaming with batch processing")
    print(f"   • Comprehensive error handling and fallback strategies")
    print(f"   • Real-time performance monitoring and metrics")
    print(f"   • Advanced UI with export and configuration controls")
    
    demo.queue(
        max_size=20,  # Increased queue size for better handling
        default_concurrency_limit=4
    ).launch(
        share=False,
        server_name="127.0.0.1",
        server_port=7860,
        show_error=True,
        show_tips=True,
        inbrowser=True  # Auto-open browser
    )

================================================================================

FILE: archive\gradio_qwen_refined.py
----------------------------------------
#!/usr/bin/env python3
"""
Refined Qwen Chat - Consultant-Inspired Hybrid
===============================================
Combines the best of both worlds with consultant's architectural insights:
- Clean configuration management with intelligent fallback
- Smart token-aware conversation management  
- Professional streaming with controlled decoding
- Performance metrics panel with practical insights
- Production-ready UX for local inference or demos
"""

import gradio as gr
import openvino_genai as ov_genai
import openvino.properties as props
import openvino.properties.hint as hints
from transformers import AutoTokenizer
import time
import queue
import re
from threading import Thread
from typing import Optional, Tuple, Dict, Any

# --- Constants ---
MODEL_PATH = r"C:\OpenVinoModels\qwen3-8b-int4-cw-ov"
DEVICE = "NPU"
CACHE_DIR = r"C:\temp\.ovcache"

# Token management constants
MAX_CONVERSATION_TOKENS = 1024
EMERGENCY_LIMIT = 1500
MAX_MESSAGE_LENGTH = 300

# Global performance tracking
performance_metrics = {
    "total_requests": 0,
    "avg_response_time": 0.0,
    "last_request_tokens": 0,
    "configuration": "unknown",
    "device": "unknown",
    "tokens_per_second": 0.0,
    "session_start": time.time()
}

SYSTEM_PROMPT = (
    "You are a helpful AI assistant powered by OpenVINO on NPU. "
    "Provide accurate, concise responses while being friendly and informative."
)

# --- Configuration Helpers ---
def get_basic_config() -> Dict[str, Any]:
    """Base configuration that works reliably"""
    return {
        hints.performance_mode: hints.PerformanceMode.LATENCY,
        props.cache_dir: CACHE_DIR
    }

def get_enhanced_npu_config() -> Dict[str, Any]:
    """Enhanced NPU configuration with NPUW optimizations"""
    return {
        **get_basic_config(),
        "NPUW_LLM": "YES",
        "NPUW_LLM_MAX_PROMPT_LEN": 4096,  # Increased from 2048 - supports larger initial prompts
        "NPUW_LLM_MIN_RESPONSE_LEN": 128,
        "CACHE_MODE": "OPTIMIZE_SPEED",
        "INFERENCE_PRECISION_HINT": "f16"
    }

def get_cpu_config() -> Dict[str, Any]:
    """Optimized CPU configuration"""
    return {
        **get_basic_config(),
        props.streams.num: 1,
        props.inference_num_threads: 4
    }

# --- Smart Pipeline Loading ---
def load_pipeline_with_fallback(model_path: str, target_device: str) -> Tuple[ov_genai.LLMPipeline, str, str]:
    """
    Load pipeline with intelligent configuration fallback
    Returns: (pipeline, device_used, config_used)
    """
    
    configurations = []
    
    if target_device == "NPU":
        configurations = [
            ("enhanced_npu", target_device, get_enhanced_npu_config()),
            ("basic_npu", target_device, get_basic_config()),
            ("minimal_npu", target_device, {}),
            ("cpu_fallback", "CPU", get_cpu_config())
        ]
    else:
        configurations = [
            ("optimized_cpu", target_device, get_cpu_config()),
            ("basic_cpu", target_device, get_basic_config()),
            ("minimal_cpu", target_device, {})
        ]
    
    for config_name, device, config in configurations:
        try:
            print(f"🔄 Trying {device} with {config_name} configuration...")
            
            if config:
                pipe = ov_genai.LLMPipeline(model_path, device, **config)
            else:
                pipe = ov_genai.LLMPipeline(model_path, device)
                
            print(f"✅ Success: {device} with {config_name}")
            return pipe, device, config_name
            
        except Exception as e:
            print(f"⚠️  {config_name} failed: {e}")
            continue
    
    raise RuntimeError("All configurations failed. Check model path and device drivers.")

# --- Smart Text Processing (Simplified for Stateful API) ---
def smart_truncate_message(text: str, max_length: int) -> str:
    """Intelligently truncate individual messages if too long"""
    if len(text) <= max_length:
        return text
    
    # Try to break at sentence boundaries
    sentences = re.split(r'[.!?]+\s+', text)
    if len(sentences) > 1:
        result = []
        current_length = 0
        
        for sentence in sentences:
            if current_length + len(sentence) + 2 <= max_length * 0.85:
                result.append(sentence)
                current_length += len(sentence) + 2
            else:
                break
        
        if result:
            truncated = '. '.join(result) + '.'
            remaining = text[len(truncated):].strip()
            if len(remaining) > 20:
                truncated += f" [...{remaining[:30].strip()}...]"
            return truncated
    
    # Fallback: word boundary truncation
    truncated = text[:max_length - 50]
    last_space = truncated.rfind(' ')
    
    if last_space > max_length * 0.7:
        truncated = truncated[:last_space]
    
    return truncated + f" [...continues for {len(text) - len(truncated)} more chars...]"

# NOTE: manage_conversation_tokens function REMOVED - stateful pipeline handles this automatically!

# --- Enhanced Streaming ---
class RefinedStreamer(ov_genai.StreamerBase):
    """Clean streaming with professional text processing"""
    
    def __init__(self, tokenizer: AutoTokenizer):
        super().__init__()
        self.tokenizer = tokenizer
        self.q = queue.Queue()
        self.tokens = []
        self.text = ""
        self.start_time = time.time()
        
    def clean_output(self, text: str) -> str:
        """Clean model output tokens (simplified - skip_special_tokens handles most cases)"""
        # Most cleaning is handled by skip_special_tokens=True
        # Only do minimal cleanup for edge cases
        special_tokens = [
            '<|im_start|>', '<|im_end|>',
            '<|system|>', '<|user|>', '<|assistant|>'
        ]
        
        cleaned = text
        for token in special_tokens:
            cleaned = cleaned.replace(token, '')
        
        # Normalize whitespace but preserve structure
        cleaned = re.sub(r'\s+', ' ', cleaned)
        return cleaned.strip()
    
    def put(self, token_id: int) -> bool:
        self.tokens.append(token_id)
        
        # Decode incrementally
        try:
            decoded = self.tokenizer.decode(self.tokens, skip_special_tokens=True)
        except:
            decoded = self.tokenizer.decode(self.tokens)
        
        if len(decoded) > len(self.text):
            new_text = decoded[len(self.text):]
            self.text = decoded
            
            if new_text.strip():
                self.q.put(new_text)
        
        return False
    
    def end(self):
        # Final cleanup
        if self.text:
            final_text = self.clean_output(self.text)
            if final_text != self.text:
                self.q.put("\n[Text cleaned]")
        
        elapsed = time.time() - self.start_time
        tokens_per_sec = len(self.tokens) / elapsed if elapsed > 0 else 0
        print(f"🚀 Generation: {len(self.tokens)} tokens in {elapsed:.2f}s ({tokens_per_sec:.1f} tok/s)")
        
        self.q.put(None)
    
    def __iter__(self):
        return self
    
    def __next__(self):
        item = self.q.get()
        if item is None:
            raise StopIteration
        return item

# --- Initialize System ---
print(f"🔧 Loading model from: {MODEL_PATH}")
print(f"🎯 Target device: {DEVICE}")
print(f"💡 Using STATEFUL chat API (following official OpenVINO GenAI pattern)")

try:
    # Load pipeline with smart fallback
    pipe, device_used, config_used = load_pipeline_with_fallback(MODEL_PATH, DEVICE)
    
    # Load and configure tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # Configure generation (using correct API pattern)
    generation_config = ov_genai.GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.7
    generation_config.top_p = 0.9
    generation_config.top_k = 50
    generation_config.max_new_tokens = 1024
    generation_config.repetition_penalty = 1.1
    # Note: pad_token_id may not be supported in GenerationConfig - handled by tokenizer
    
    # Update global metrics
    performance_metrics["device"] = device_used
    performance_metrics["configuration"] = config_used
    
    print(f"✅ System ready: {device_used} with {config_used} configuration")
    print(f"📚 Following official chat_sample.py pattern with start_chat()/finish_chat()")
    
except Exception as e:
    print(f"❌ Failed to initialize system: {e}")
    exit(1)

# --- Stateful Chat Function (Following Official API Pattern) ---
def stateful_chat_function(message: str, history: list):
    """Stateful chat function using OpenVINO GenAI's built-in conversation management"""
    global performance_metrics
    
    start_time = time.time()
    
    try:
        # Input validation
        if not message.strip():
            yield history
            return
        
        # Handle overly long messages with user-friendly feedback
        original_length = len(message)
        if original_length > MAX_MESSAGE_LENGTH:
            message = smart_truncate_message(message, MAX_MESSAGE_LENGTH)
            print(f"📏 Message truncated: {original_length} → {len(message)} characters")
            
            # Provide immediate feedback to user about truncation
            history.append({
                "role": "assistant", 
                "content": f"⚠️ Your message ({original_length:,} characters) exceeded the NPU limit and was automatically truncated to {len(message)} characters for processing. The response will be based on the truncated content."
            })
            yield history  # Show truncation warning immediately
        
        # Update UI immediately (history is just for display)
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ""})
        
        # Initialize streaming
        streamer = RefinedStreamer(tokenizer)
        
        # Generate using stateful API - ONLY send the new message!
        def generate():
            try:
                # The pipeline remembers conversation history internally
                # We only need to send the new user message
                pipe.generate(message, generation_config, streamer)
            except Exception as e:
                print(f"❌ Generation error: {e}")
                streamer.q.put(f"❌ Generation error: {str(e)[:100]}...")
                streamer.q.put(None)
        
        generation_thread = Thread(target=generate)
        generation_thread.start()
        
        # Stream to UI
        for chunk in streamer:
            if chunk:  # Only add non-empty chunks
                history[-1]["content"] += chunk
                yield history
        
        generation_thread.join(timeout=30.0)
        
        # Update performance metrics (simplified)
        elapsed = time.time() - start_time
        output_tokens = len(tokenizer.encode(history[-1]["content"]) if history[-1]["content"] else [])
        input_tokens = len(tokenizer.encode(message))
        
        performance_metrics["total_requests"] += 1
        performance_metrics["avg_response_time"] = (
            performance_metrics["avg_response_time"] * (performance_metrics["total_requests"] - 1) + elapsed
        ) / performance_metrics["total_requests"]
        performance_metrics["last_request_tokens"] = input_tokens
        performance_metrics["tokens_per_second"] = output_tokens / elapsed if elapsed > 0 else 0
        
        print(f"📊 Stateful Request: {elapsed:.2f}s, {input_tokens}→{output_tokens} tokens ({performance_metrics['tokens_per_second']:.1f} tok/s)")
        
    except Exception as e:
        error_message = f"❌ Chat error: {str(e)[:150]}..."
        
        # Handle different error types
        if "token" in str(e).lower() or "length" in str(e).lower():
            error_message = "⚠️ Message too long or conversation exceeded limits. Try starting a new chat."
        elif "memory" in str(e).lower():
            error_message = "⚠️ Memory limit reached. Please start a new conversation."
            
        if not history or history[-1]["role"] != "assistant":
            history.append({"role": "assistant", "content": error_message})
        else:
            history[-1]["content"] = error_message
            
        print(f"❌ Stateful chat error: {e}")
        yield history

# --- Gradio Interface ---
def create_interface():
    """Create refined Gradio interface"""
    
    with gr.Blocks(
        theme=gr.themes.Soft(),
        title="Refined Qwen Chat",
        css="""
        .gradio-container { max-width: 1200px; margin: auto; }
        .chatbot { height: 600px; }
        .metrics-panel { background: #f8f9fa; padding: 12px; border-radius: 6px; }
        """,
    ) as demo:
        
        # Header
        gr.Markdown(f"""
        # 🤖 Refined Qwen Chat (Stateful)
        
        **Device**: {device_used} | **Config**: {config_used} | **Model**: Qwen3-8B INT4  
        **Mode**: Stateful conversation management (following OpenVINO GenAI best practices)
        """)
        
        # Main chat
        chatbot = gr.Chatbot(
            label="Conversation",
            height=600,
            type='messages',
            avatar_images=(None, "🤖"),
            show_copy_button=True
        )
        
        # Input
        with gr.Row():
            msg = gr.Textbox(
                placeholder=f"Chat with Qwen on {device_used}...",
                scale=8,
                max_lines=3,
                show_label=False
            )
            send_btn = gr.Button("Send", variant="primary", scale=1)
        
        # Controls
        with gr.Row():
            clear_btn = gr.Button("🗑️ Clear", variant="secondary")
            metrics_btn = gr.Button("📊 Metrics", variant="secondary")
            info_btn = gr.Button("ℹ️ Info", variant="secondary")
        
        # Metrics panel (collapsible)
        with gr.Row(visible=False) as metrics_row:
            with gr.Column(elem_classes=["metrics-panel"]):
                metrics_display = gr.JSON(label="Performance Metrics")
        
        # Event handlers with stateful session management
        msg.submit(stateful_chat_function, [msg, chatbot], chatbot).then(
            lambda: gr.update(value=""), None, [msg]
        )
        
        send_btn.click(stateful_chat_function, [msg, chatbot], chatbot).then(
            lambda: gr.update(value=""), None, [msg]
        )
        
        # Clear chat: finish old session and start new one
        clear_btn.click(
            lambda: ([], ""), None, [chatbot, msg]
        ).then(
            lambda: pipe.finish_chat(), None, None  # End current session
        ).then(
            lambda: pipe.start_chat(SYSTEM_PROMPT), None, None   # Start new session with system prompt
        )
        
        def show_metrics():
            session_time = time.time() - performance_metrics["session_start"]
            metrics = {
                **performance_metrics,
                "session_duration": f"{session_time:.1f}s"
            }
            return gr.update(value=metrics, visible=True), gr.update(visible=True)
        
        metrics_btn.click(show_metrics, None, [metrics_display, metrics_row])
        
        def show_info():
            info = f"""
            ## System Information
            - **Device**: {device_used}
            - **Configuration**: {config_used}
            - **Model Path**: `{MODEL_PATH}`
            - **Cache Directory**: `{CACHE_DIR}`
            - **API Mode**: Stateful (using start_chat/finish_chat)
            - **Conversation Management**: Automatic (handled by OpenVINO GenAI)
            - **Generation Settings**: Temperature=0.7, Top-p=0.9, Top-k=50
            - **Key Benefit**: No manual token counting - pipeline manages KV-cache internally
            """
            gr.Info(info)
        
        info_btn.click(show_info, None, None)
        
        # Examples
        gr.Examples([
            "Explain quantum computing simply",
            "Write Python code for a binary tree",
            "What's new in AI hardware?",
            "Compare CPU vs GPU vs NPU for AI"
        ], inputs=msg)
        
        # Initialize stateful chat when interface loads (MUST be inside gr.Blocks context)
        demo.load(lambda: pipe.start_chat(SYSTEM_PROMPT), None, None)
    
    return demo

# --- Launch ---
if __name__ == "__main__":
    print("🌐 Starting Refined Qwen Chat Interface...")
    print("✨ Features: Stateful chat sessions, smart fallback, performance tracking")
    print("🎯 Key Improvement: Uses OpenVINO GenAI's built-in conversation management")
    
    demo = create_interface()
    demo.queue(max_size=10).launch(
        share=False,
        server_name="127.0.0.1",
        server_port=7860,
        show_error=True
    )

================================================================================

FILE: check_model_config.py
----------------------------------------
#!/usr/bin/env python3
"""
Check current model configuration and identify export settings
"""

import json
from pathlib import Path

def check_model_config(model_path):
    """Check the configuration of an existing OpenVINO model"""
    
    model_dir = Path(model_path)
    config_file = model_dir / "config.json"
    
    print(f"🔍 Checking model: {model_path}")
    print("=" * 50)
    
    if not model_dir.exists():
        print(f"❌ Model directory not found: {model_path}")
        return
    
    if not config_file.exists():
        print(f"❌ config.json not found in {model_path}")
        return
    
    try:
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        print("📋 Model Configuration:")
        print(f"   Model type: {config.get('model_type', 'Unknown')}")
        print(f"   Architecture: {config.get('architectures', 'Unknown')}")
        
        if 'max_position_embeddings' in config:
            print(f"   Max position embeddings: {config['max_position_embeddings']}")
        
        if 'hidden_size' in config:
            print(f"   Hidden size: {config['hidden_size']}")
            
        if 'num_attention_heads' in config:
            print(f"   Attention heads: {config['num_attention_heads']}")
            
        if 'vocab_size' in config:
            print(f"   Vocabulary size: {config['vocab_size']}")
        
        # Check for quantization info
        if 'quantization_config' in config:
            quant_config = config['quantization_config']
            print(f"\n🔧 Quantization Configuration:")
            for key, value in quant_config.items():
                print(f"   {key}: {value}")
        else:
            print("\n❓ No quantization configuration found")
        
        # Check OpenVINO files
        print(f"\n📁 OpenVINO Files:")
        xml_file = model_dir / "openvino_model.xml"
        bin_file = model_dir / "openvino_model.bin"
        
        if xml_file.exists():
            print(f"   ✓ Model XML: {xml_file.name} ({xml_file.stat().st_size / (1024*1024):.1f} MB)")
        else:
            print(f"   ❌ Missing: openvino_model.xml")
            
        if bin_file.exists():
            print(f"   ✓ Model weights: {bin_file.name} ({bin_file.stat().st_size / (1024*1024):.1f} MB)")
        else:
            print(f"   ❌ Missing: openvino_model.bin")
        
        # Check for tokenizer
        tokenizer_files = ["tokenizer.json", "tokenizer_config.json"]
        print(f"\n🔤 Tokenizer Files:")
        for tok_file in tokenizer_files:
            tok_path = model_dir / tok_file
            if tok_path.exists():
                print(f"   ✓ {tok_file}")
            else:
                print(f"   ❌ Missing: {tok_file}")
        
        # Detect likely source model
        print(f"\n🎯 Likely Source Model:")
        model_type = config.get('model_type', '').lower()
        if 'qwen' in model_type:
            print(f"   This appears to be a Qwen model")
            if config.get('hidden_size') == 4096:
                print(f"   Likely: Qwen2.5-7B or similar")
            elif config.get('hidden_size') == 3584:
                print(f"   Likely: Qwen2.5-3B or similar") 
        
        # Check if model supports NPU
        print(f"\n🖥️  NPU Compatibility Analysis:")
        print(f"   Current export likely has dynamic shapes (causing NPU compilation failure)")
        print(f"   Recommendation: Re-export with static shapes using export_model_for_npu.py")
        
        return config
        
    except Exception as e:
        print(f"❌ Error reading config: {e}")

if __name__ == "__main__":
    model_path = r"C:\OpenVinoModels\phi3-128k-npu"
    check_model_config(model_path)

================================================================================

FILE: config.example.json
----------------------------------------
{
  "_comment": "Example configuration for Enhanced Phi-3-mini-128k-instruct OpenVINO GenAI Chat",
  "_usage": "Copy to config.json and customize as needed",
  
  "model": {
    "path": "C:\\OpenVinoModels\\phi3-128k-npu",
    "name": "Phi-3-mini-128k-instruct",
    "type": "phi3",
    "_notes": "Model path should point to OpenVINO model directory with .xml/.bin files"
  },
  
  "deployment": {
    "target_device": "NPU",
    "npu_profile": "balanced",
    "fallback_device": "CPU",
    "cache_directory": "./cache/.ovcache_phi3",
    "_notes": {
      "target_device": "NPU, CPU, GPU, or AUTO",
      "npu_profile": "conservative, balanced, or aggressive",
      "cache_directory": "Directory for OpenVINO compilation cache"
    }
  },
  
  "generation": {
    "max_new_tokens": 2048,
    "temperature": 0.6,
    "top_p": 0.95,
    "top_k": 20,
    "repetition_penalty": 1.1,
    "do_sample": true,
    "_notes": {
      "temperature": "0.0-2.0, higher = more creative",
      "top_p": "0.0-1.0, nucleus sampling threshold",
      "top_k": "1-100, limits vocabulary per step",
      "max_new_tokens": "Maximum response length (1-2048), increased for Phi-3 128k context"
    }
  },
  
  "ui": {
    "max_message_length": 2000,
    "max_conversation_tokens": 8000,
    "emergency_limit": 16384,
    "show_performance_metrics": true,
    "theme": "soft",
    "_notes": {
      "max_message_length": "UI limit for single message, increased for Phi-3 128k context",
      "max_conversation_tokens": "NPU memory optimization, increased for larger context",
      "theme": "gradio theme: default, soft, monochrome"
    }
  },
  
  "performance": {
    "generation_timeout": 30.0,
    "truncation_warning_delay": 0.5,
    "ui_update_interval": 0.1,
    "thread_pool_size": 2,
    "_notes": {
      "generation_timeout": "Seconds before timeout",
      "truncation_warning_delay": "Pause to show warnings",
      "ui_update_interval": "UI refresh rate",
      "thread_pool_size": "Number of threads for processing"
    }
  },
  
  "logging": {
    "log_level": "INFO",
    "enable_performance_logging": true,
    "enable_token_filtering_logs": false,
    "_notes": {
      "log_level": "DEBUG, INFO, WARNING, ERROR",
      "enable_performance_logging": "Log timing and metrics",
      "enable_token_filtering_logs": "Detailed token processing logs"
    }
  },
  
  "security": {
    "validate_input": true,
    "max_file_uploads": 0,
    "allowed_file_types": [],
    "rate_limit_requests": false,
    "_notes": {
      "validate_input": "Enable input validation and sanitization",
      "max_file_uploads": "Maximum simultaneous file uploads",
      "rate_limit_requests": "Enable request rate limiting"
    }
  }
}

================================================================================

FILE: config.json
----------------------------------------
{
  "model": {
    "path": "C:\\OpenVinoModels\\phi3-128k-npu",
    "name": "Phi-3-mini-128k-instruct",
    "type": "phi3"
  },
  "deployment": {
    "target_device": "NPU",
    "npu_profile": "balanced",
    "fallback_device": "CPU",
    "cache_directory": "./cache/.ovcache_phi3"
  },
  "generation": {
    "max_new_tokens": 2048,
    "temperature": 0.6,
    "top_p": 0.95,
    "top_k": 20,
    "repetition_penalty": 1.1,
    "do_sample": true
  },
  "ui": {
    "max_message_length": 2000,
    "max_conversation_tokens": 8000,
    "emergency_limit": 16384,
    "show_performance_metrics": true,
    "theme": "soft"
  },
  "performance": {
    "generation_timeout": 30.0,
    "truncation_warning_delay": 0.5,
    "ui_update_interval": 0.1,
    "thread_pool_size": 2
  },
  "logging": {
    "log_level": "INFO",
    "enable_performance_logging": true,
    "enable_token_filtering_logs": false
  },
  "security": {
    "validate_input": true,
    "max_file_uploads": 0,
    "allowed_file_types": [],
    "rate_limit_requests": false
  }
}

================================================================================

FILE: context\Documentation_Open_vino.txt
----------------------------------------
https://github.com/openvinotoolkit/openvino.genai/tree/master

================================================================================

FILE: context\Project Development Strategy Outline_.md
----------------------------------------


# **Strategic Roadmap for the Enhanced Phi-3 Chat System: From Prototype to Production-Grade AI Application**

### **Expert Persona**

**Systems Architect and MLOps Strategist:** With over 15 years of experience designing and deploying scalable, high-performance machine learning systems, this expert specializes in bridging the gap between cutting-edge AI research and robust, enterprise-ready production environments. Their expertise lies in MLOps, system architecture, performance optimization for specialized hardware like NPUs, and creating strategic roadmaps that balance immediate needs with long-term scalability and maintainability. They provide a critical, systems-level perspective focused on building resilient, efficient, and future-proof AI applications.

---

### **Executive Summary**

This report presents a comprehensive analysis and strategic development roadmap for the Enhanced Phi-3 Chat System. The project currently stands as a powerful demonstration of Intel NPU-accelerated inference with OpenVINO, featuring a sophisticated modular architecture and advanced capabilities such as Retrieval-Augmented Generation (RAG) and dynamic system prompts. Its strengths lie in its deep NPU-specific optimizations, professional user interface, and robust configuration management system.

However, the analysis reveals critical areas that require immediate attention to ensure long-term viability and production readiness. The project carries significant technical debt, most notably in the form of a lingering monolithic script, legacy naming conventions from its prior use of the Qwen3 model, and a complete absence of an automated testing framework. These issues, if left unaddressed, will impede development velocity, increase maintenance costs, and introduce configuration risks.

To address these challenges, this report outlines a three-phase strategic roadmap:

1. **Phase 1: Foundational Refactoring and Consolidation.** This phase focuses on eliminating technical debt by unifying the architecture, refactoring legacy naming, and establishing a CI/CD pipeline with automated testing. These are non-negotiable steps to create a stable and maintainable codebase.  
2. **Phase 2: Productionization and Scalability.** This phase prepares the application for reliable deployment by introducing Docker containerization and enhancing the user interface with interactive controls for generation parameters, moving it from a developer tool to a robust application.  
3. **Phase 3: Advanced Feature Expansion.** This phase elevates the application's core AI capabilities to state-of-the-art by evolving the RAG pipeline with advanced document parsing and reranking, and introducing an agentic architecture with function-calling capabilities.

Beyond this roadmap, the report explores long-term strategic opportunities, including model customization through fine-tuning and a migration to the OpenVINO™ Model Server (OVMS) for an enterprise-grade microservices architecture. Executing this strategy will transform the project from an advanced prototype into a scalable, production-ready, and future-proof AI application.

---

## **Part I: Current State Architectural Assessment**

This part provides a comprehensive and critical analysis of the project as it exists today. The goal is to establish a clear, evidence-based baseline from which the strategic roadmap in Part II can be built.

### **Section 1: Analysis of the Dual Architecture and Technical Debt**

The project's most significant structural challenge stems from its evolutionary development, resulting in architectural inconsistencies and technical debt that must be addressed to ensure future maintainability and stability.

#### **1.1 The Modular vs. Monolithic Dichotomy**

The repository currently contains two distinct and conflicting architectural patterns for launching the application. The modern approach is a modular architecture orchestrated by main.py, which leverages discrete components from the app/ directory for configuration, model deployment, streaming, chat logic, and UI rendering.1 This structure adheres to software engineering best practices by promoting a clear separation of concerns, which enhances readability, simplifies debugging, and makes the system easier to extend.

In stark contrast, the legacy gradio\_qwen\_enhanced.py script represents a monolithic architecture, combining all application logic into a single, large file.1 While potentially simpler for initial prototyping, this approach is inherently difficult to maintain and scale.1

The existence of these two parallel architectures is a source of significant friction. While developer guidance in CLAUDE.md correctly directs new development toward the modular app/ structure, user-facing documentation in README.md and SETUP.md still presents the monolithic script as a valid entry point.1 This ambiguity creates cognitive overhead for developers and users alike. Any feature enhancement or bug fix must be considered in the context of both architectures, effectively doubling the maintenance burden or, more likely, leading to the legacy script becoming dangerously out of sync. This architectural duality directly impedes development velocity and increases the risk of introducing regressions, making it an unsustainable model for a production-grade application.

#### **1.2 Legacy Naming Conventions Post-Migration**

The project has successfully migrated its core functionality to the microsoft/Phi-3-mini-128k-instruct model, a move confirmed by the config.json file and the main.py entry point.1 Despite this migration, numerous artifacts throughout the codebase retain names from the previous

Qwen3 model. This includes file paths (context/qwen3\_model\_context/), class names (EnhancedQwen3Streamer), function names (deploy\_qwen3\_pipeline, enhanced\_qwen3\_chat), and environment variables (QWEN3\_MODEL\_PATH).1

This is more than a cosmetic issue; it represents a form of technical debt that actively increases configuration risk. The NPU optimization patterns, for example, reside in a directory named qwen3\_model\_context but are being applied to a Phi-3 model.1 While these patterns may be compatible now, a future update to OpenVINO or NPU drivers could introduce model-specific optimizations that a developer might overlook, assuming the "Qwen3" optimizations are irrelevant to Phi-3. This could lead to silent performance degradation or outright compilation failures. The legacy naming conventions create a misleading narrative about the code's function, making correct maintenance and future upgrades more difficult and error-prone.

### **Section 2: NPU Optimization and Hardware Coupling**

A core strength of the project is its deep and effective optimization for Intel Neural Processing Units (NPUs), though this comes at the cost of configuration flexibility.

#### **2.1 Effective Use of NPUW Configuration**

The application demonstrates a sophisticated understanding of the requirements for deploying LLMs on Intel NPUs via OpenVINO. The code correctly implements NPU Wrapper (NPUW) hints, such as "NPU\_USE\_NPUW": "YES" and "NPUW\_LLM": "YES", and sets critical parameters like NPUW\_LLM\_MAX\_PROMPT\_LEN to manage the NPU's memory and performance characteristics.1 The inclusion of NPU profiles ("conservative," "balanced," "aggressive") provides users with valuable control to tune the trade-off between performance and resource consumption.1 This level of detailed hardware-specific configuration is a significant strength and is essential for achieving optimal performance on the target device.

#### **2.2 Configuration Rigidity and Lack of Abstraction**

While the NPU settings are effective, they are hardcoded within the get\_npu\_config methods in both the modular and monolithic codebases.1 These critical values are not exposed in

config.json or as command-line arguments, tightly coupling the application logic to a specific generation of hardware and driver. If a future NPU or an updated driver requires different hint values, a developer would need to modify the core application code rather than simply updating a configuration file.

This design prioritizes deep optimization for the current hardware target over long-term portability and maintainability. As the OpenVINO ecosystem rapidly evolves with support for new models and hardware 2, this tight coupling presents a maintenance challenge. A more robust MLOps approach would abstract these hardware-specific details into the configuration layer, transforming hardware adaptation from a development task into a more flexible deployment configuration task.

### **Section 3: Core Feature Implementation Review**

The application's key features, RAG and security validation, are well-implemented but show clear areas for future maturation.

#### **3.1 RAG System \- Foundational but Limited**

The DocumentRAGSystem provides a solid foundation for retrieval-augmented generation.1 It uses a standard and effective stack, including

langchain, RecursiveCharacterTextSplitter for chunking, a lightweight sentence-transformers/all-MiniLM-L6-v2 model for embeddings, and an in-memory FAISS vector store for fast retrieval. This setup is well-suited for processing the plain-text file types listed in the UI, such as .txt, .md, and .py.

However, the system's effectiveness is constrained by its simplistic "shallow parsing" strategy. The backend logic reads all supported files as raw text, discarding any inherent structure. For complex, semi-structured documents common in enterprise settings, such as PDFs with tables and columns or DOCX files with headers and sections, this approach is insufficient. It will fail to correctly interpret document layout, leading to garbled context being fed into the vector store and, consequently, irrelevant or nonsensical search results. This inability to deeply understand document structure is the single biggest bottleneck preventing the RAG feature from being truly production-grade.

#### **3.2 Security and Input Validation**

The inclusion of the InputValidator class is a commendable and critical feature that demonstrates a production-oriented mindset.1 The class implements methods to validate user messages against excessive length, suspicious patterns (e.g.,

\<script\> tags to prevent XSS), and unusual character distributions. It also includes a sanitization method to normalize whitespace and remove control characters. This proactive approach to security is often overlooked in prototype applications and represents a significant strength, mitigating common vectors for abuse and denial-of-service attacks.

### **Section 4: User Interface and Backend Integration**

The Gradio-based user interface is a standout feature, though its integration with the backend reveals a critical inconsistency between documentation and implementation.

#### **4.1 Professional UI Design**

The user interface, defined in app/ui.py, is a major strength. It leverages gr.Blocks to create a highly professional and usable front-end that goes far beyond a basic proof-of-concept.1 The use of custom CSS, collapsible accordions for advanced features like RAG and system prompt configuration, and a real-time performance monitoring panel provides a rich user experience. This design effectively exposes the application's advanced capabilities in an intuitive and accessible manner, aligning with best practices for building custom Gradio applications.5

#### **4.2 Critical Data Format Inconsistency**

A significant issue exists in how chat history is managed and documented. The developer guide, CLAUDE.md, explicitly warns against using a list of dictionaries (\[{"role": "user",...}\]) for chat history, stating that the correct format for gr.ChatInterface is a list of lists (\[\["user\_message", "bot\_response"\]\]).1 However, the legacy

gradio\_qwen\_enhanced.py script uses the dictionary-based format, while the modern app/chat.py correctly uses the list-of-lists format.1

This contradiction between the developer guide, the legacy code, and the modern implementation points to a flawed refactoring and documentation process. Outdated or incorrect documentation is often more damaging than no documentation, as it can actively mislead developers and waste significant time on debugging. This inconsistency highlights a need to treat documentation as a first-class citizen of the codebase, ensuring it is updated and validated as part of the standard development and review cycle.

---

## **Part II: Strategic Development Roadmap**

This part provides a phased, prioritized plan to address the issues identified in Part I, pay down technical debt, and mature the application into a production-ready system.

### **Section 5: Phase 1 \- Foundational Refactoring and Consolidation**

This initial phase is critical for establishing a stable and maintainable foundation. Its primary goal is to eliminate technical debt and introduce professional software development practices.

#### **5.1 Unifying the Architecture**

The first and most important action is to formally deprecate and remove the gradio\_qwen\_enhanced.py script. All development, testing, and documentation (including README.md and SETUP.md) must be standardized on the main.py entry point and the app/ modular architecture. This single action will resolve the architectural duality, eliminate code duplication, and halve the ongoing maintenance burden, creating a single source of truth for the application's logic.

#### **5.2 Model-Agnostic Refactoring**

To align the codebase with its current functionality and prepare it for future model integrations, a repository-wide refactoring of legacy "Qwen3" naming is required. Components should be renamed to be model-agnostic or specific to the current Phi-3 model. Key changes should include:

* EnhancedQwen3Streamer \-\> EnhancedLLMStreamer  
* deploy\_qwen3\_pipeline \-\> deploy\_llm\_pipeline  
* context/qwen3\_model\_context \-\> context/llm\_optimization\_patterns  
* QWEN3\_MODEL\_PATH (environment variable) \-\> MODEL\_PATH

This refactoring resolves the configuration risk identified previously and makes the codebase more intuitive. It paves the way for easier integration of other OpenVINO-supported models like Llama, Mistral, or Gemma in the future.7

#### **5.3 Implementing a Testing and CI Framework**

The absence of automated testing is a major production readiness gap. A formal testing suite and a Continuous Integration (CI) pipeline must be established.

* **Testing Suite:**  
  * **Linting:** Integrate flake8 to enforce a consistent code style and catch syntax errors.  
  * **Unit Tests:** Use pytest to create unit tests for core, non-UI logic. High-priority targets for initial tests include the InputValidator in app/chat.py and the ConfigurationLoader in app/config.py, as their logic is critical and easily testable.  
* **Continuous Integration:**  
  * Create a CI pipeline using GitHub Actions by adding a workflow file (e.g., .github/workflows/ci.yml). This workflow should automatically run the linter and the pytest suite on every push and pull request.

Implementing a CI pipeline represents a fundamental shift from slow, error-prone manual validation to an automated assurance process. It provides a quality gate that offers rapid feedback to developers, prevents regressions, and builds confidence in the codebase. This automation is the bedrock of modern, agile development and is essential for increasing development velocity safely.10

### **Section 6: Phase 2 \- Productionization and Scalability**

With a stable foundation, this phase focuses on packaging the application for reliable, repeatable deployment and enhancing its utility for end-users.

#### **6.1 Containerization with Docker**

A Dockerfile must be created to containerize the application, which is a critical step for production deployment. The Dockerfile will define a reproducible environment by:

1. Starting from an official OpenVINO development image, such as openvino/ubuntu22\_dev, which includes necessary libraries.13  
2. Ensuring all required system dependencies and NPU drivers are installed within the container.15  
3. Copying the requirements.txt file and installing all Python packages.  
4. Copying the application source code and configuration files.  
5. Exposing the Gradio port (e.g., 7860\) and configuring the server to listen on all network interfaces (0.0.0.0) to be accessible from outside the container.16  
6. Defining the CMD instruction to launch the application via python main.py.

Containerization provides a consistent, isolated environment that eliminates "it works on my machine" issues, ensuring the application runs identically across development, staging, and production environments. It also greatly simplifies deployment and scaling.16

#### **6.2 Continuous Deployment (CD) Pipeline**

The CI pipeline established in Phase 1 should be extended into a full Continuous Deployment (CD) pipeline. After the linting and testing stages pass on the main branch, a new job in the GitHub Actions workflow should automatically:

1. Build the Docker image using the Dockerfile.  
2. Tag the image with a version number or commit hash for traceability.  
3. Push the tagged image to a container registry, such as Docker Hub or GitHub Container Registry.

This automates the release process, ensuring that every change merged into the main branch produces a tested, versioned, and deployable artifact. This completes the CI/CD loop, a cornerstone of modern MLOps practices.

#### **6.3 Enhancing UI Interactivity**

To increase the application's utility, key LLM generation parameters should be exposed as interactive controls in the Gradio UI. An "Advanced Generation Settings" accordion can be added to house components like gr.Slider for temperature and top\_p, and gr.Number for max\_new\_tokens. These components would pass their values as additional inputs to the main chat function.

The current generation configuration is static, limiting experimentation. Allowing users to modify these parameters in real-time transforms the application from a simple chatbot into a powerful tool for prompt engineering and exploring the model's creative capabilities. Gradio's gr.Blocks architecture fully supports this dynamic interaction.6

### **Section 7: Phase 3 \- Advanced Feature Expansion**

This phase focuses on evolving the application's core AI capabilities to match the state-of-the-art in generative AI.

#### **7.1 Evolving the RAG Pipeline: Advanced Parsing**

To overcome the "Shallow Parsing" bottleneck, the RAG system's document ingestion logic must be upgraded. The basic open(file).read() approach should be replaced with a sophisticated, pipeline-based document parsing library capable of handling complex, structured formats like PDF and DOCX. This will enable the system to extract text while preserving layout, identifying tables, and handling figures, resulting in significantly higher-quality context for the LLM. The choice of library should be guided by factors such as format support, extraction quality, and ease of integration.

**Table 7.1: Comparison of Advanced Document Parsing Libraries**

| Library | Supported Formats | Table Extraction | Layout Awareness | Integration | Execution Model |
| :---- | :---- | :---- | :---- | :---- | :---- |
| unstructured | PDF, DOCX, PPTX, HTML, etc. 21 | Good (via different strategies) 21 | High (hi\_res strategy) 21 | langchain native 21 | Local / API 21 |
| LlamaParse | PDF, DOCX, PPTX, etc. 22 | Excellent (LLM-based) 23 | High 22 | LlamaIndex native 22 | API-first 23 |
| PyMuPDF | PDF 24 | Basic (text extraction only) 24 | Low (extracts raw text stream) 24 | Manual | Local 24 |
| Docling | PDF, DOCX, images, audio 25 | Advanced 25 | High 25 | langchain native 25 | Local / Actor 25 |

#### **7.2 Evolving the RAG Pipeline: Cross-Encoder Reranking**

The accuracy of the RAG pipeline can be further improved by adding a second-stage reranker. The current single-stage process relies on vector search (a bi-encoder approach), which is fast but can retrieve documents that are only superficially related to the query. A two-stage process would first retrieve a larger set of candidate documents (e.g., top 20\) using the fast vector search, and then use a more computationally intensive but far more accurate cross-encoder model (e.g., BAAI/bge-reranker-base) to re-score and rank these candidates.

Cross-encoders achieve higher accuracy by performing full attention across the query and each document, capturing semantic nuance that bi-encoders miss.26 By passing only the top 3-5 reranked documents to the LLM, the system provides much more relevant context, leading to more accurate and helpful answers. This pattern is well-supported by LangChain's

ContextualCompressionRetriever, making it straightforward to integrate.28

#### **7.3 Transitioning to an Agentic Architecture**

The final step in this phase is to evolve the application from a chatbot into an AI agent by introducing function-calling capabilities. This involves defining a set of "tools"—Python functions that can perform actions like making calculations or searching the web—and giving the LLM the ability to invoke them. The chat loop would be modified to detect when the model outputs a structured request to call a function, execute that function with the provided arguments, and then feed the result back to the model to generate a final, tool-informed response.

This represents a paradigm shift from passive text generation to active problem-solving. OpenVINO supports models that are fine-tuned for function calling, and frameworks like LangChain provide robust abstractions for building agents that use local OpenVINO models as their reasoning engine.29 This is the next logical step in building more powerful and useful AI applications.

---

## **Part III: Long-Term Vision and Strategic Opportunities**

This part looks beyond the immediate roadmap to consider future directions that can provide a significant and sustainable competitive advantage.

### **Section 8: Exploring Model Customization and Fine-Tuning**

While RAG is highly effective for injecting factual, up-to-date knowledge into an LLM's responses, it is less effective at changing the model's inherent style, tone, or understanding of specialized domain vocabulary. For example, a RAG system can provide a legal document as context, but the LLM might still explain its contents in overly casual language.

For true domain adaptation, fine-tuning is necessary. This process adjusts the model's weights on a smaller, curated dataset, adapting its behavior to a specific task or communication style.32 While the OpenVINO™ Training Extensions project exists 33, the more common path for LLMs involves using standard frameworks like Hugging Face Transformers with Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA, followed by conversion and quantization with Optimum Intel.35

The long-term strategic vision should include creating a feedback loop where high-quality user interactions and key domain-specific documents are used to build a proprietary fine-tuning dataset. This would enable the creation of a custom-tuned version of Phi-3 that not only has access to an organization's knowledge via RAG but also "speaks the language" of the organization, a powerful and defensible differentiator.

### **Section 9: Alternative Deployment Models with OpenVINO™ Model Server (OVMS)**

The current architecture embeds the OpenVINO LLMPipeline directly within the Gradio application. This monolithic deployment model is simple but suffers from tight coupling: the UI and the model cannot be scaled independently, and the application can only serve a single model at a time.

The long-term architectural goal should be to migrate the inference logic to the OpenVINO™ Model Server (OVMS). OVMS is a high-performance, C++ based serving solution designed for production environments. It exposes models via standard gRPC and REST APIs and supports advanced LLM-specific features like continuous batching and prefix caching for improved throughput and latency.18

This migration would transform the system into a modern microservices architecture. The Gradio application would become a lightweight client that makes API calls to a dedicated, scalable OVMS endpoint. This decoupling allows the UI and the AI model to be developed, deployed, and scaled independently. A cluster of OVMS instances, managed by an orchestrator like Kubernetes, could handle heavy inference loads for multiple applications, while only a few instances of the Gradio UI are needed. This architecture is far more robust, scalable, and aligned with modern enterprise IT practices than the current embedded model.18

### **Conclusion and Prioritized Recommendations**

This report has detailed the current state of the Enhanced Phi-3 Chat System, identifying its strengths in NPU optimization and UI design, as well as critical weaknesses related to technical debt and production readiness. To guide its evolution, a clear, prioritized roadmap is essential.

The following actions are recommended:

1. **Immediate Priority (Phase 1):** The highest priority is to stabilize the project's foundation. This requires **unifying the architecture** by removing the legacy monolithic script, **refactoring all legacy naming conventions** to be model-agnostic, and **implementing a CI pipeline with automated testing**. These actions are non-negotiable for ensuring the health, maintainability, and future of the project.  
2. **Mid-Term Priority (Phase 2):** Once the foundation is stable, the focus should shift to production readiness. This involves **containerizing the application with Docker**, establishing a **Continuous Deployment (CD) pipeline** to automate releases, and **enhancing the UI with interactive controls** for generation parameters. These steps will make the application robust, deployable, and more useful.  
3. **Long-Term Priority (Phase 3):** With a production-ready application, the focus can turn to state-of-the-art AI capabilities. This includes **evolving the RAG system with advanced document parsing and cross-encoder reranking** to dramatically improve its accuracy, and beginning the **transition to an agentic architecture** with function-calling capabilities.  
4. **Strategic Vision:** Concurrently with the phased roadmap, the project should plan for two major strategic initiatives: investigating **model fine-tuning** for deep domain adaptation and architecting a future migration to the **OpenVINO™ Model Server (OVMS)** to achieve enterprise-grade scalability and a true microservices architecture.

By systematically executing this roadmap, the project can successfully transition from an impressive but fragile prototype into a robust, scalable, and powerful AI application poised for long-term success.

#### **Referenzen**

1. combined\_openvinodev\_folder.txt  
2. Announcing OpenVINO™ 2025.2: New Models, Generative AI Pipelines, and Performance Improvements \- Medium, Zugriff am August 11, 2025, [https://medium.com/openvino-toolkit/announcing-openvino-2025-2-new-models-generative-ai-pipelines-and-performance-improvements-9e4e46335db3](https://medium.com/openvino-toolkit/announcing-openvino-2025-2-new-models-generative-ai-pipelines-and-performance-improvements-9e4e46335db3)  
3. Releases · openvinotoolkit/openvino \- GitHub, Zugriff am August 11, 2025, [https://github.com/openvinotoolkit/openvino/releases](https://github.com/openvinotoolkit/openvino/releases)  
4. Release Notes for Intel Distribution of OpenVINO Toolkit 2025.2, Zugriff am August 11, 2025, [https://www.intel.com/content/www/us/en/developer/articles/release-notes/openvino/2025-2.html](https://www.intel.com/content/www/us/en/developer/articles/release-notes/openvino/2025-2.html)  
5. Quickstart \- Gradio, Zugriff am August 11, 2025, [https://www.gradio.app/guides/quickstart](https://www.gradio.app/guides/quickstart)  
6. Introduction to Gradio Blocks \- Hugging Face LLM Course, Zugriff am August 11, 2025, [https://huggingface.co/learn/llm-course/chapter9/7](https://huggingface.co/learn/llm-course/chapter9/7)  
7. OpenVINO GenAI, Zugriff am August 11, 2025, [https://openvinotoolkit.github.io/openvino.genai/](https://openvinotoolkit.github.io/openvino.genai/)  
8. OpenVINO Release Notes — OpenVINO™ documentation — Version(2024), Zugriff am August 11, 2025, [https://docs.openvino.ai/2024/about-openvino/release-notes-openvino.html](https://docs.openvino.ai/2024/about-openvino/release-notes-openvino.html)  
9. Release Notes for Intel Distribution of OpenVINO Toolkit 2024.5, Zugriff am August 11, 2025, [https://www.intel.com/content/www/us/en/developer/articles/release-notes/openvino/2024-5.html](https://www.intel.com/content/www/us/en/developer/articles/release-notes/openvino/2024-5.html)  
10. Pytest Coverage Comment · Actions · GitHub Marketplace, Zugriff am August 11, 2025, [https://github.com/marketplace/actions/pytest-coverage-comment](https://github.com/marketplace/actions/pytest-coverage-comment)  
11. Setting Up Continuous Integration with GitHub Actions for a Python Project \- Vuyisile Ndlovu, Zugriff am August 11, 2025, [https://vuyisile.com/setting-up-continuous-integration-with-github-actions-for-a-python-project/](https://vuyisile.com/setting-up-continuous-integration-with-github-actions-for-a-python-project/)  
12. Testing your Python Project with GitHub Actions | by Marc Wouts | TDS Archive | Medium, Zugriff am August 11, 2025, [https://marc-wouts.medium.com/testing-your-python-project-with-github-actions-ec9bf82b20dc](https://marc-wouts.medium.com/testing-your-python-project-with-github-actions-ec9bf82b20dc)  
13. openvino/ubuntu22\_dev \- Docker Image, Zugriff am August 11, 2025, [https://hub.docker.com/r/openvino/ubuntu22\_dev](https://hub.docker.com/r/openvino/ubuntu22_dev)  
14. Install Intel® Distribution of OpenVINO™ Toolkit From a Docker Image, Zugriff am August 11, 2025, [https://docs.openvino.ai/2025/get-started/install-openvino/install-openvino-docker-linux.html](https://docs.openvino.ai/2025/get-started/install-openvino/install-openvino-docker-linux.html)  
15. OpenVINO on Intel Ultra NPU \- Dockerfile \- HackMD, Zugriff am August 11, 2025, [https://hackmd.io/@Otj6UinLQLiYTva4YngKuw/SyVFNztj6](https://hackmd.io/@Otj6UinLQLiYTva4YngKuw/SyVFNztj6)  
16. Deploying Gradio With Docker, Zugriff am August 11, 2025, [https://www.gradio.app/guides/deploying-gradio-with-docker](https://www.gradio.app/guides/deploying-gradio-with-docker)  
17. Deploy a custom Docker image for Data Science project \- Gradio sketch recognition app (Part 1\) \- OVHcloud Blog, Zugriff am August 11, 2025, [https://blog.ovhcloud.com/deploy-a-custom-docker-image-for-data-science-project-gradio-sketch-recognition-app-part-1/](https://blog.ovhcloud.com/deploy-a-custom-docker-image-for-data-science-project-gradio-sketch-recognition-app-part-1/)  
18. openvinotoolkit/model\_server: A scalable inference server for models optimized with OpenVINO \- GitHub, Zugriff am August 11, 2025, [https://github.com/openvinotoolkit/model\_server](https://github.com/openvinotoolkit/model_server)  
19. ChatInterface \- Gradio Docs, Zugriff am August 11, 2025, [https://www.gradio.app/docs/gradio/chatinterface](https://www.gradio.app/docs/gradio/chatinterface)  
20. Blocks And Event Listeners \- Gradio, Zugriff am August 11, 2025, [https://www.gradio.app/guides/blocks-and-event-listeners](https://www.gradio.app/guides/blocks-and-event-listeners)  
21. RAG — Three Python libraries for Pipeline-based PDF parsing \- AI Bites, Zugriff am August 11, 2025, [https://www.ai-bites.net/rag-three-python-libraries-for-pipeline-based-pdf-parsing/](https://www.ai-bites.net/rag-three-python-libraries-for-pipeline-based-pdf-parsing/)  
22. RAG \+ LlamaParse: Advanced PDF Parsing for Retrieval | by Ryan Siegler | KX Systems, Zugriff am August 11, 2025, [https://medium.com/kx-systems/rag-llamaparse-advanced-pdf-parsing-for-retrieval-c393ab29891b](https://medium.com/kx-systems/rag-llamaparse-advanced-pdf-parsing-for-retrieval-c393ab29891b)  
23. Simple Ways to Parse PDFs for Better RAG Systems | by kirouane Ayoub | GoPenAI, Zugriff am August 11, 2025, [https://blog.gopenai.com/simple-ways-to-parse-pdfs-for-better-rag-systems-82ec68c9d8cd](https://blog.gopenai.com/simple-ways-to-parse-pdfs-for-better-rag-systems-82ec68c9d8cd)  
24. How to Efficiently Parse Large PDF and DOCX Files (in GBs) for Embeddings Without Loading Fully in Memory? : r/Rag \- Reddit, Zugriff am August 11, 2025, [https://www.reddit.com/r/Rag/comments/1gjz2dj/how\_to\_efficiently\_parse\_large\_pdf\_and\_docx\_files/](https://www.reddit.com/r/Rag/comments/1gjz2dj/how_to_efficiently_parse_large_pdf_and_docx_files/)  
25. Docling \- GitHub Pages, Zugriff am August 11, 2025, [https://docling-project.github.io/docling/](https://docling-project.github.io/docling/)  
26. The aRt of RAG Part 3: Reranking with Cross Encoders | by Ross Ashman (PhD) | Medium, Zugriff am August 11, 2025, [https://medium.com/@rossashman/the-art-of-rag-part-3-reranking-with-cross-encoders-688a16b64669](https://medium.com/@rossashman/the-art-of-rag-part-3-reranking-with-cross-encoders-688a16b64669)  
27. Rerankers and Two-Stage Retrieval \- Pinecone, Zugriff am August 11, 2025, [https://www.pinecone.io/learn/series/rag/rerankers/](https://www.pinecone.io/learn/series/rag/rerankers/)  
28. Cross Encoder Reranker | 🦜️🔗 LangChain, Zugriff am August 11, 2025, [https://python.langchain.com/docs/integrations/document\_transformers/cross\_encoder\_reranker/](https://python.langchain.com/docs/integrations/document_transformers/cross_encoder_reranker/)  
29. Create ReAct Agent using OpenVINO and LangChain, Zugriff am August 11, 2025, [https://docs.openvino.ai/2024/notebooks/llm-agent-react-langchain-with-output.html](https://docs.openvino.ai/2024/notebooks/llm-agent-react-langchain-with-output.html)  
30. OpenVINO | 🦜️ LangChain, Zugriff am August 11, 2025, [https://python.langchain.com/docs/integrations/llms/openvino/](https://python.langchain.com/docs/integrations/llms/openvino/)  
31. Create Function-calling Agent using OpenVINO and Qwen-Agent, Zugriff am August 11, 2025, [https://docs.openvino.ai/2024/notebooks/llm-agent-functioncall-qwen-with-output.html](https://docs.openvino.ai/2024/notebooks/llm-agent-functioncall-qwen-with-output.html)  
32. Optimizing Large Language Models with the OpenVINO™ Toolkit \- Intel® Network Builders, Zugriff am August 11, 2025, [https://builders.intel.com/docs/networkbuilders/optimizing-large-language-models-with-the-openvino-toolkit-1742810892.pdf](https://builders.intel.com/docs/networkbuilders/optimizing-large-language-models-with-the-openvino-toolkit-1742810892.pdf)  
33. OpenVINO Training Extensions download | SourceForge.net, Zugriff am August 11, 2025, [https://sourceforge.net/projects/openvino-train-ext.mirror/](https://sourceforge.net/projects/openvino-train-ext.mirror/)  
34. open-edge-platform/training\_extensions: Train, Evaluate, Optimize, Deploy Computer Vision Models via OpenVINO \- GitHub, Zugriff am August 11, 2025, [https://github.com/open-edge-platform/training\_extensions](https://github.com/open-edge-platform/training_extensions)  
35. LLM Instruction-following pipeline with OpenVINO, Zugriff am August 11, 2025, [https://docs.openvino.ai/2024/notebooks/llm-question-answering-with-output.html](https://docs.openvino.ai/2024/notebooks/llm-question-answering-with-output.html)  
36. Fine-Tuning LLMs: A Guide With Examples \- DataCamp, Zugriff am August 11, 2025, [https://www.datacamp.com/tutorial/fine-tuning-large-language-models](https://www.datacamp.com/tutorial/fine-tuning-large-language-models)  
37. Efficient LLM Serving \- OpenVINO™ documentation, Zugriff am August 11, 2025, [https://docs.openvino.ai/2025/model-server/ovms\_docs\_llm\_reference.html](https://docs.openvino.ai/2025/model-server/ovms_docs_llm_reference.html)  
38. Efficiently Serve Large Language Models (LLMs) with OpenVINO™ Model Server \- Intel, Zugriff am August 11, 2025, [https://www.intel.com/content/dam/develop/public/us/en/documents/llm-with-model-server-white-paper.pdf](https://www.intel.com/content/dam/develop/public/us/en/documents/llm-with-model-server-white-paper.pdf)  
39. Manage deep learning models with OpenVINO Model Server | Red Hat Developer, Zugriff am August 11, 2025, [https://developers.redhat.com/articles/2024/07/03/manage-deep-learning-models-openvino-model-server](https://developers.redhat.com/articles/2024/07/03/manage-deep-learning-models-openvino-model-server)

================================================================================

FILE: context\README.md
----------------------------------------
# OpenVINO GenAI Context for LLM Copilot

**Generated**: January 8, 2025  
**Version**: 1.0.0  
**Purpose**: Curated essential files for LLM understanding of OpenVINO GenAI

## 🎯 Overview

This context folder contains the most critical OpenVINO GenAI files for building robust, performance-optimized Gradio chat applications. Each file has been carefully selected based on its importance for understanding API patterns, avoiding common pitfalls, and implementing advanced features.

---

## 📁 Complete Directory Structure

```
context/
├── python_samples/          # 🟢 Python API examples (4 files)
│   ├── chat_sample.py ⭐⭐⭐⭐⭐          # Session management API
│   ├── benchmark_genai.py ⭐⭐⭐⭐⭐      # Performance patterns  
│   ├── greedy_causal_lm.py ⭐⭐⭐⭐       # Basic generation patterns
│   └── multinomial_causal_lm.py ⭐⭐⭐⭐   # Advanced sampling techniques
├── test_configs/           # 🔧 Configuration patterns (3 files)
│   ├── generation_config.py ⭐⭐⭐⭐⭐     # Complete config reference
│   ├── ov_genai_pipelines.py ⭐⭐⭐⭐      # Pipeline initialization
│   └── hugging_face.py ⭐⭐⭐             # Tokenizer integration
├── core_cpp/              # ⚙️ C++ implementation (3 files)
│   ├── utils.cpp ⭐⭐⭐⭐                 # NPU detection & NPUW
│   ├── pipeline_stateful.cpp ⭐⭐⭐       # Stateful pipeline logic
│   └── generation_config.cpp ⭐⭐⭐       # Config internals
├── documentation/         # 📚 Architecture guides (2 files)
│   ├── HOW_IT_WORKS.md ⭐⭐⭐⭐           # Architecture guide
│   └── DEBUG_LOG.md ⭐⭐⭐               # Troubleshooting guide
├── python_bindings/       # 🔗 C++ to Python bindings (3 files)
│   ├── py_llm_pipeline.cpp ⭐⭐⭐⭐        # LLMPipeline binding
│   ├── py_generation_config.cpp ⭐⭐⭐⭐   # GenerationConfig binding
│   └── py_streamers.cpp ⭐⭐⭐           # Streaming implementation
├── gradio_patterns/        # 🎨 Official Gradio integration (5 files)
│   ├── chatbot_streaming.py ⭐⭐⭐⭐⭐     # Official streaming patterns
│   ├── chatinterface_advanced.py ⭐⭐⭐⭐⭐ # Advanced chat interfaces
│   ├── performance_dashboard.py ⭐⭐⭐⭐   # Monitoring & analytics
│   ├── concurrency_queue.py ⭐⭐⭐⭐      # Resource management
│   └── llm_hf_integration.py ⭐⭐⭐       # HuggingFace integration
├── gradio_testing/         # 🧪 Professional testing (2 files)
│   ├── chat_interface_tests.py ⭐⭐⭐⭐    # Interface validation
│   └── streaming_tests.py ⭐⭐⭐⭐        # Streaming performance
├── qwen3_model_context/    # 🎯 Qwen3-specific optimization (4 files)
│   ├── model_architecture.py ⭐⭐⭐⭐⭐    # Complete Qwen3 specs
│   ├── special_tokens.py ⭐⭐⭐⭐⭐       # Token handling & templates
│   ├── npu_optimization.py ⭐⭐⭐⭐⭐     # NPU deployment guide
│   └── README.md ⭐⭐⭐⭐                # Model-specific guide
└── README.md             # 📋 This comprehensive guide

Total: 24 files across 7 directories
```

---

## 🥇 Priority 5 Files (Critical)

### `python_samples/chat_sample.py`
**What it is**: The official chat implementation pattern  
**Why critical**: Shows the **correct** way to handle chat sessions  
**Key insights**: 
- `pipe.start_chat()` / `pipe.finish_chat()` session management
- Proper streaming with `StreamingStatus.RUNNING`
- Minimal, working configuration patterns

**How to use**: Study this before building any chat interface. Your current Gradio scripts are missing the session management!

### `python_samples/benchmark_genai.py`
**What it is**: Performance optimization and metrics collection  
**Why critical**: Professional-grade performance patterns  
**Key insights**:
- Advanced configuration with `SchedulerConfig`
- Performance metrics collection (`perf_metrics`)
- Batch processing and warmup strategies
- Proper tokenizer integration

**How to use**: Copy the performance measurement patterns into your Gradio apps.

### `test_configs/generation_config.py`
**What it is**: Complete catalog of all `GenerationConfig` parameters  
**Why critical**: Prevents C++ binding errors (like your `pad_token_id` issue!)  
**Key insights**:
- Every supported parameter with working examples
- Proper construction patterns (empty constructor + attribute setting)
- Beam search, sampling, and penalty configurations
- Safe parameter combinations

**How to use**: Reference this whenever setting generation parameters. Never guess - always verify here first!

---

## 🥈 Priority 4 Files (High Value)

### `core_cpp/utils.cpp`
**What it is**: Core C++ utilities including NPU device handling  
**Why important**: Explains NPU auto-configuration and NPUW settings  
**Key insights**: 
- `update_npu_config()` function (lines 76-95)
- Automatic NPUW property setting
- Device detection and validation logic
- Cache management implementation

**How to use**: Understand why your NPU configurations succeed or fail.

### `documentation/HOW_IT_WORKS.md`
**What it is**: Core architecture documentation  
**Why important**: Deep understanding of stateful models and KV-cache  
**Key insights**:
- Stateful vs stateless model differences  
- KV-cache mechanics and memory management
- Beam search implementation details
- Token limit implications

**How to use**: Read this to understand why certain token limits exist and how to optimize memory usage.

---

## 🟢 Gradio Integration Patterns (New)

### `gradio_patterns/` ⭐⭐⭐⭐⭐
**What it contains**: Official Gradio patterns extracted and optimized for OpenVINO GenAI  
**Why critical**: Bridges gap between official Gradio best practices and OpenVINO implementation  
**Key components**:
- `chatbot_streaming.py` - Official streaming patterns with ChatInterface
- `chatinterface_advanced.py` - System prompts, parameter controls, multi-modal
- `performance_dashboard.py` - Professional monitoring and analytics
- `concurrency_queue.py` - NPU resource management and queue handling
- `llm_hf_integration.py` - HuggingFace tokenizer integration patterns

**How to use**: Copy these patterns directly into your Gradio applications for production-quality interfaces.

### `gradio_testing/` ⭐⭐⭐⭐
**What it contains**: Comprehensive testing patterns for chat interfaces  
**Why important**: Professional testing methodologies for streaming and chat functionality  
**Key components**:
- `chat_interface_tests.py` - Chat interface validation and testing
- `streaming_tests.py` - Performance and reliability testing for streaming

**How to use**: Use these test patterns to validate your OpenVINO GenAI applications.

### `qwen3_model_context/` ⭐⭐⭐⭐⭐
**What it contains**: Model-specific knowledge for Qwen3-8B optimization  
**Why critical**: Addresses model-specific requirements and NPU optimization  
**Key components**:
- `model_architecture.py` - Complete Qwen3 specs and configuration
- `special_tokens.py` - 26+ special tokens, chat templates, filtering
- `npu_optimization.py` - NPUW configuration, deployment, troubleshooting

**How to use**: Essential for Qwen3 deployments - provides missing model-specific context.

---

## 🥈 Priority 4 Files (High Value)

### `python_samples/greedy_causal_lm.py`
**What it is**: Basic text generation without sampling  
**Why important**: Simplest, most reliable generation pattern  
**Key insights**: Greedy decoding, minimal configuration, deterministic output  
**How to use**: Start with this pattern before adding sampling complexity.

### `test_configs/ov_genai_pipelines.py`
**What it is**: Pipeline initialization and configuration patterns  
**Why important**: Shows proper pipeline setup with various devices and configs  
**Key insights**: Device handling, scheduler configuration, error handling  
**How to use**: Reference for robust pipeline initialization.

### `python_bindings/py_generation_config.cpp`
**What it is**: C++ binding for GenerationConfig class  
**Why important**: Explains why certain parameters work/fail  
**Key insights**: Supported attributes, type validation, binding implementation  
**How to use**: Debug C++ binding errors and understand parameter limitations.

### `python_bindings/py_streamers.cpp`
**What it is**: C++ implementation of streaming classes  
**Why important**: Shows proper streaming implementation patterns  
**Key insights**: Token-level streaming, cleanup, thread safety  
**How to use**: Understand streaming internals and build custom streamers.

### `core_cpp/pipeline_stateful.cpp`
**What it is**: Core stateful pipeline implementation  
**Why important**: Token limit handling, state management, memory optimization  
**Key insights**: KV-cache handling, conversation management, NPU constraints  
**How to use**: Understand why token limits exist and how to work within them.

### `core_cpp/generation_config.cpp`
**What it is**: Core GenerationConfig implementation  
**Why important**: Parameter validation, defaults, internal logic  
**Key insights**: Default values, parameter interactions, validation rules  
**How to use**: Understand config parameter behavior and defaults.

### `documentation/DEBUG_LOG.md`
**What it is**: Debug logging and troubleshooting guide  
**Why important**: Professional debugging techniques  
**Key insights**: Log levels, performance debugging, error diagnosis  
**How to use**: Enable detailed logging for troubleshooting complex issues.

---

## 🥉 Priority 3 Files (Supporting)

### `python_samples/multinomial_causal_lm.py`
**Advanced sampling techniques for creative and diverse text generation**

### `test_configs/hugging_face.py`  
**Integration patterns with Hugging Face tokenizers and model compatibility**

### `python_bindings/py_llm_pipeline.cpp`
**C++ binding implementation details for advanced troubleshooting and customization**

---

## 🚀 How to Use This Context

### For Your Current Gradio Scripts:

1. **Fix Session Management** (HIGH PRIORITY)
   ```python
   # WRONG (your current approach):
   pipe.generate(full_conversation_prompt, config, streamer)
   
   # RIGHT (from chat_sample.py):
   pipe.start_chat()
   pipe.generate(user_message, config, streamer)  # Only current message!
   pipe.finish_chat()
   ```

2. **Implement Qwen3-Specific Optimizations** (CRITICAL)
   ```python
   # Use complete NPUW configuration from qwen3_model_context/
   from qwen3_model_context.npu_optimization import Qwen3NPUConfigBuilder
   
   builder = Qwen3NPUConfigBuilder("balanced")
   npu_config = builder.build_complete_config()
   pipe = ov_genai.LLMPipeline(model_path, "NPU", **npu_config)
   ```

3. **Add Proper Token Filtering** (HIGH PRIORITY)
   ```python
   # Filter Qwen3's 26+ special tokens in streaming
   from qwen3_model_context.special_tokens import Qwen3StreamingFilter
   
   class EnhancedGradioStreamer(ov_genai.StreamerBase):
       def __init__(self, tokenizer):
           self.filter = Qwen3StreamingFilter()
       
       def put(self, token_id):
           display_text = self.filter.process_token(token_id, token_text)
           if display_text:  # Only show filtered tokens
               self.text_queue.put(display_text)
   ```

4. **Use Official Gradio Patterns**
   - Copy streaming patterns from `gradio_patterns/chatbot_streaming.py`
   - Implement advanced features from `gradio_patterns/chatinterface_advanced.py`
   - Add monitoring from `gradio_patterns/performance_dashboard.py`

5. **Verify GenerationConfig Parameters**
   - Before setting any config parameter, check `test_configs/generation_config.py`
   - Never set unsupported attributes like `pad_token_id`

### For New Development:

1. **Start with** `gradio_patterns/chatbot_streaming.py` for Gradio chat interfaces
2. **Use** `qwen3_model_context/model_architecture.py` for Qwen3-specific configurations
3. **Reference** `generation_config.py` for all configuration options
4. **Study** `HOW_IT_WORKS.md` for architecture understanding  
5. **Benchmark with** `benchmark_genai.py` patterns
6. **Test with** patterns from `gradio_testing/` directory

---

## 🔍 Key Discoveries for Your Project

### 1. Missing Session Management
Your Gradio scripts don't use `start_chat()/finish_chat()` - this could explain token management issues!

### 2. Incomplete NPUW Configuration  
Your NPU config is missing critical Qwen3-specific NPUW settings required for compilation success.

### 3. No Special Token Filtering
Qwen3 has 26+ special tokens that appear in streaming output - you need proper filtering.

### 4. Missing Official Gradio Patterns
Official Gradio demos show more robust patterns than basic streaming implementations.

### 5. Model-Specific Requirements
Qwen3 has unique chat templates, token handling, and memory constraints not covered in general OpenVINO docs.

---

## 📋 Next Steps

### Immediate Actions (Critical):
1. **Implement complete NPUW configuration** from `qwen3_model_context/npu_optimization.py`
2. **Add Qwen3 token filtering** from `qwen3_model_context/special_tokens.py`
3. **Update to proper chat templates** using Qwen3ChatTemplate class
4. **Add session management** with `start_chat()/finish_chat()`

### Integration Improvements:
5. **Copy official Gradio patterns** from `gradio_patterns/` directory
6. **Implement professional monitoring** from `gradio_patterns/performance_dashboard.py`
7. **Add comprehensive testing** using `gradio_testing/` patterns
8. **Study model architecture** in `qwen3_model_context/model_architecture.py`

### Advanced Features:
9. **Add tool calling support** using Qwen3 tool tokens
10. **Implement thinking mode** with Qwen3 reasoning tokens
11. **Create multi-modal interfaces** using vision tokens
12. **Add conversation memory management** within NPU token limits

This enhanced context now provides complete coverage:
- **OpenVINO GenAI implementation** (original context)
- **Official Gradio best practices** (new gradio_patterns/)
- **Professional testing methodologies** (new gradio_testing/)
- **Qwen3-specific optimizations** (new qwen3_model_context/)

🎯 **Everything needed for production-quality Qwen3 + OpenVINO GenAI + Gradio applications!**

================================================================================

FILE: context\documentation\DEBUG_LOG.md
----------------------------------------
## 1. Using Debug Log

There are six levels of logs, which can be called explicitly or set via the ``OPENVINO_LOG_LEVEL`` environment variable:

0 - ``ov::log::Level::NO``
1 - ``ov::log::Level::ERR``
2 - ``ov::log::Level::WARNING``
3 - ``ov::log::Level::INFO``
4 - ``ov::log::Level::DEBUG``
5 - ``ov::log::Level::TRACE``

When setting the environment variable OPENVINO_LOG_LEVEL > ov::log::Level::WARNING, the properties of the compiled model can be printed.

For example:

Linux - export OPENVINO_LOG_LEVEL=3
Windows - set OPENVINO_LOG_LEVEL=3

the properties of the compiled model are printed as follows:
```sh
    NETWORK_NAME: Model0
    OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1
    NUM_STREAMS: 1
    INFERENCE_NUM_THREADS: 48
    PERF_COUNT: NO
    INFERENCE_PRECISION_HINT: bf16
    PERFORMANCE_HINT: LATENCY
    EXECUTION_MODE_HINT: PERFORMANCE
    PERFORMANCE_HINT_NUM_REQUESTS: 0
    ENABLE_CPU_PINNING: YES
    SCHEDULING_CORE_TYPE: ANY_CORE
    MODEL_DISTRIBUTION_POLICY:
    ENABLE_HYPER_THREADING: NO
    EXECUTION_DEVICES: CPU
    CPU_DENORMALS_OPTIMIZATION: NO
    LOG_LEVEL: LOG_NONE
    CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1
    DYNAMIC_QUANTIZATION_GROUP_SIZE: 32
    KV_CACHE_PRECISION: f16
    AFFINITY: CORE
    EXECUTION_DEVICES:
    CPU: Intel(R) Xeon(R) Platinum 8468
```

When Speculative Decoding or Prompt Lookup pipeline is executed, performance metrics will be also printed.

For example:

```
===============================
Total duration, sec: 26.6217
Draft model duration, sec: 1.60329
Main model duration, sec: 25.0184
Draft model duration, %: 6.02248
Main model duration, %: 93.9775
AVG acceptance rate, %: 21.6809
===============================
REQUEST_ID: 0
Main model iterations: 47
Token per sec: 3.75633
AVG acceptance rate, %: 21.6809
Accepted tokens by draft model: 51
Generated tokens: 100
Accepted token rate, %: 51
===============================
Request_id: 0 ||| 40 0 40 20 0 0 40 40 0 20 20 20 0 40 0 0 20 80 0 80 20 0 0 0 40 80 0 40 60 40 80 0 0 0 0 40 20 20 0 40 20 40 0 20 0 0 0
```


When GGUF model passed to LLMPipeline, the details debug info will be also printed.

For example:
```
[GGUF Reader]: Loading and unpacking model from: gguf_models/qwen2.5-0.5b-instruct-q4_0.gguf
[GGUF Reader]: Loading and unpacking model done. Time: 196ms
[GGUF Reader]: Start generating OpenVINO model...
[GGUF Reader]: Save generated OpenVINO model to: gguf_models/openvino_model.xml done. Time: 466 ms
[GGUF Reader]: Model generation done. Time: 757ms
```


================================================================================

FILE: context\documentation\HOW_IT_WORKS.md
----------------------------------------
# OpenVINO™ GenAI: How it works

## Stateful LLM

A common optimization for LLM inference is using a past KV (key/value)-cache. This cache is represented by the corresponding inputs and outputs in a model originally implemented in a DL framework (e.g. PyTorch models from Hugging Face). For further optimization and easier use, the model is transformed to a stateful form. This transformation improves inference performance and decreases the allocated runtime memory in long-running text generation scenarios. It is achieved by hiding inputs and outputs of the model that represent past KV-cache tensors and handling them inside the model in a more efficient way. Although the cache is still accessible with state API. It is opposed to stateless model approach requiring manipulating these inputs and outputs explicitly. An introduction to the stateful models can be found in the [Stateful Models article](https://docs.openvino.ai/2025/openvino-workflow/running-inference/stateful-models.html).

Hiding KV-cache introduces a peculiarity for beam search algorithm. Beam search suggests batched inference of multiple beams. The design described here so far would result in generating multiple independent sequences of tokens. Beam search algorithm, on the other hand, requires removing some of the ongoing beams and splitting other beams to multiple branches. Beam removal requires deleting corresponding KV-cache entry and beam splitting requires copying corresponding KV-cache values.

To provide the possibility to implement beam search without accessing model's internal state, a stateful LLM converted with `optimum-intel` or [llm_bench](../../tools/llm_bench) introduces an additional 1-dimentional `beam_idx` input. `beam_idx` must contain indexes of elements in a batch which are intended to be selected and will evolve during the next beam search iteration. There's only one beam when the generation starts. That beam corresponds to the initial prompt. `beam_idx` must have values: `[0, 0]` to keep the initial beam and introduce its copy. The dynamic batch size enables to change the number of beams dynamically. `beam_idx` must have `[1]` as the value to remove zeroth sequence and keep the second beam only.

Assume there are two running beams. To proceed with generating both beams at the next iteration, `beam_idx` values must be `[0, 1]`, pointing to batch elements `0` and `1`. To drop the last beam and split the other beam in two, `beam_idx` must be set to `[0, 0]`. This results in utilizing only the part of KV cache corresponding to the zeroth element in the batch. The process of selecting proper entries in cache is called Cache Reorder.

![](beam_idx-fork.gif)
![](beam_idx-drop.gif)

The images below represent stateless and stateful LLM pipelines. The model has 4 inputs:
1. `input_ids` contains the next selected token
2. `attention_mask` is filled with `1`
3. `position_ids` encodes a position of currently generating token in the sequence
4. `beam_idx` selects beams

The model has 1 output `logits` describing the predicted distribution over the next tokens. And there's KV cache state.

![](stateless.jpg)
![](stateful.jpg)


================================================================================

FILE: context\gradio_patterns\chatbot_streaming.py
----------------------------------------
# Gradio Chatbot Streaming Patterns
# ===================================
# 
# PRIORITY: ⭐⭐⭐⭐⭐ (Essential for OpenVINO GenAI streaming)
# 
# This file contains official Gradio patterns for implementing streaming chatbots.
# These patterns are directly applicable to OpenVINO GenAI applications and provide
# the "official" way to handle streaming responses in Gradio.
#
# Key Learning Points:
# - Character-by-character streaming with proper yielding
# - Clean separation of user input handling and bot response generation
# - Proper use of gr.Blocks with message-type chatbot
# - Queue management for non-blocking UI updates
# - Simple but effective streaming architecture

import gradio as gr
import random
import time

# ================================
# PATTERN 1: Basic Streaming Chat
# ================================
# Source: demo/chatbot_streaming/run.py
# This is the simplest and most reliable streaming pattern

def create_basic_streaming_chat():
    """
    Official Gradio streaming pattern - minimal but complete implementation.
    Perfect template for OpenVINO GenAI integration.
    """
    with gr.Blocks() as demo:
        chatbot = gr.Chatbot(type="messages")
        msg = gr.Textbox()
        clear = gr.Button("Clear")

        def user(user_message, history: list):
            """Handle user input and add to history"""
            return "", history + [{"role": "user", "content": user_message}]

        def bot(history: list):
            """Generate streaming bot response"""
            # This would be replaced with OpenVINO GenAI generation
            bot_message = random.choice(["How are you?", "I love you", "I'm very hungry"])
            history.append({"role": "assistant", "content": ""})
            
            # Character-by-character streaming (replace with token-by-token for LLMs)
            for character in bot_message:
                history[-1]['content'] += character
                time.sleep(0.05)  # Simulate generation time
                yield history

        # Event chain: user input -> bot response
        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
            bot, chatbot, chatbot
        )
        clear.click(lambda: None, None, chatbot, queue=False)

    return demo

# ======================================
# PATTERN 2: ChatInterface Streaming  
# ======================================
# Source: demo/chatinterface_streaming_echo/run.py
# Higher-level abstraction - easier to implement but less control

def create_chatinterface_streaming():
    """
    ChatInterface with built-in streaming support.
    Good for rapid prototyping of OpenVINO GenAI applications.
    """
    
    def slow_echo(message, history):
        """Streaming function compatible with ChatInterface"""
        for i in range(len(message)):
            time.sleep(0.05)
            yield "You typed: " + message[: i + 1]

    demo = gr.ChatInterface(
        slow_echo,
        type="messages",
        flagging_mode="manual",
        flagging_options=["Like", "Spam", "Inappropriate", "Other"], 
        save_history=True,
    )
    
    return demo

# ====================================
# PATTERN 3: Advanced Streaming Chat
# ====================================
# Enhanced pattern with better error handling and performance monitoring

def create_advanced_streaming_chat():
    """
    Advanced streaming pattern with error handling, performance monitoring,
    and OpenVINO GenAI-specific optimizations.
    """
    
    class StreamingMetrics:
        def __init__(self):
            self.total_requests = 0
            self.avg_response_time = 0
            self.streaming_errors = 0
    
    metrics = StreamingMetrics()
    
    with gr.Blocks() as demo:
        # Enhanced chatbot with better configuration
        chatbot = gr.Chatbot(
            type="messages",
            height=600,
            show_copy_button=True,
            avatar_images=("👤", "🤖")
        )
        
        with gr.Row():
            msg = gr.Textbox(
                placeholder="Enter your message...",
                scale=4,
                max_lines=3
            )
            send_btn = gr.Button("Send", scale=1, variant="primary")
        
        with gr.Row():
            clear_btn = gr.Button("Clear", variant="secondary")
            metrics_btn = gr.Button("Metrics", variant="secondary")
        
        # Metrics display (collapsible)
        with gr.Row(visible=False) as metrics_row:
            metrics_display = gr.JSON(label="Streaming Performance")

        def user_input_handler(user_message, history: list):
            """Enhanced user input handling with validation"""
            if not user_message.strip():
                return "", history
            
            metrics.total_requests += 1
            return "", history + [{"role": "user", "content": user_message}]

        def streaming_bot_response(history: list):
            """Enhanced bot response with error handling and metrics"""
            if not history:
                return history
            
            start_time = time.time()
            
            try:
                # Simulate OpenVINO GenAI streaming generation
                response = "This is a simulated OpenVINO GenAI response with proper streaming."
                history.append({"role": "assistant", "content": ""})
                
                # Token-by-token streaming (more realistic for LLMs)
                words = response.split()
                for i, word in enumerate(words):
                    if i == 0:
                        history[-1]['content'] = word
                    else:
                        history[-1]['content'] += " " + word
                    
                    time.sleep(0.1)  # Simulate token generation time
                    yield history
                
                # Update metrics
                elapsed = time.time() - start_time
                metrics.avg_response_time = (
                    (metrics.avg_response_time * (metrics.total_requests - 1) + elapsed) 
                    / metrics.total_requests
                )
                
            except Exception as e:
                metrics.streaming_errors += 1
                error_msg = f"❌ Streaming error: {str(e)[:100]}..."
                if history and history[-1]["role"] == "assistant":
                    history[-1]["content"] = error_msg
                else:
                    history.append({"role": "assistant", "content": error_msg})
                yield history

        def get_metrics():
            """Return current streaming metrics"""
            return {
                "total_requests": metrics.total_requests,
                "avg_response_time": round(metrics.avg_response_time, 3),
                "streaming_errors": metrics.streaming_errors,
                "error_rate": round(metrics.streaming_errors / max(metrics.total_requests, 1) * 100, 2)
            }

        def toggle_metrics():
            """Toggle metrics display visibility"""
            return gr.update(visible=True), get_metrics()

        # Event handlers
        msg.submit(
            user_input_handler, [msg, chatbot], [msg, chatbot], queue=False
        ).then(
            streaming_bot_response, chatbot, chatbot
        )
        
        send_btn.click(
            user_input_handler, [msg, chatbot], [msg, chatbot], queue=False  
        ).then(
            streaming_bot_response, chatbot, chatbot
        )
        
        clear_btn.click(lambda: [], None, chatbot, queue=False)
        
        metrics_btn.click(
            toggle_metrics, None, [metrics_row, metrics_display]
        )

    return demo

# =====================================
# INTEGRATION GUIDELINES FOR OPENVINO
# =====================================

"""
OPENVINO GENAI INTEGRATION NOTES:
================================

1. REPLACE SIMULATION WITH REAL GENERATION:
   Replace the simulated streaming in `streaming_bot_response()` with:
   
   ```python
   streamer = YourOpenVINOStreamer(tokenizer)
   pipe.generate(prompt, generation_config, streamer)
   
   for token_text in streamer:
       history[-1]['content'] += token_text
       yield history
   ```

2. TOKEN-LEVEL STREAMING:
   - Use character-by-character for demos/testing
   - Use token-by-token for production LLM applications
   - Consider word-by-word for balanced performance

3. ERROR HANDLING:
   - Wrap OpenVINO generation in try-catch blocks
   - Handle NPU compilation errors gracefully
   - Provide fallback messages for failures

4. PERFORMANCE MONITORING:
   - Track token generation speed (tokens/second)
   - Monitor NPU utilization if available
   - Log compilation and inference times

5. MEMORY MANAGEMENT:
   - Clear conversation history when memory limits reached
   - Implement token counting for NPU constraints
   - Use session management for stateful pipelines

6. UI CONSIDERATIONS:
   - Add generation mode selection (greedy, sampling)
   - Include temperature/top-p controls
   - Show real-time token counts
   - Display device information (NPU/CPU)

USAGE IN YOUR PROJECT:
=====================

The advanced streaming pattern is most suitable for production OpenVINO GenAI
applications. Key benefits:

- Professional error handling
- Built-in performance metrics
- Easy to extend with OpenVINO-specific features
- Compatible with both stateful and stateless pipelines
- Responsive UI with proper queue management

Simply replace the simulated generation with your OpenVINO GenAI streaming
implementation and add NPU-specific configurations.
"""

# Example usage:
if __name__ == "__main__":
    # Choose the pattern most suitable for your needs
    demo = create_advanced_streaming_chat()  # Recommended for production
    demo.launch()

================================================================================

FILE: context\gradio_patterns\chatinterface_advanced.py
----------------------------------------
# Gradio ChatInterface Advanced Patterns
# ======================================
#
# PRIORITY: ⭐⭐⭐⭐⭐ (Essential for system prompts and advanced controls)
#
# This file contains advanced ChatInterface patterns including system prompts,
# additional inputs, and configuration controls. These patterns are crucial for
# production OpenVINO GenAI applications that need fine-grained control.
#
# Key Learning Points:
# - System prompt integration with ChatInterface
# - Additional inputs for generation parameters
# - Multi-parameter streaming functions
# - Advanced ChatInterface configuration options
# - Production-ready parameter controls

import gradio as gr
import time

# =======================================
# PATTERN 1: System Prompt Integration
# =======================================
# Source: demo/chatinterface_system_prompt/run.py
# Essential for controlling LLM behavior

def create_system_prompt_chat():
    """
    ChatInterface with system prompt and token limit controls.
    Perfect for OpenVINO GenAI applications requiring behavior control.
    """
    
    def echo_with_system_prompt(message, history, system_prompt, tokens):
        """
        Streaming function with system prompt and token limit.
        Replace this with OpenVINO GenAI generation.
        """
        response = f"System prompt: {system_prompt}\nMessage: {message}."
        
        # Respect token limit
        max_length = min(len(response), int(tokens))
        
        for i in range(max_length):
            time.sleep(0.05)
            yield response[: i + 1]

    demo = gr.ChatInterface(
        echo_with_system_prompt,
        type="messages",
        additional_inputs=[
            gr.Textbox(
                value="You are a helpful AI assistant powered by OpenVINO.", 
                label="System Prompt",
                info="Controls the AI's behavior and personality"
            ),
            gr.Slider(
                minimum=10, 
                maximum=1000, 
                value=200,
                label="Max Response Tokens",
                info="Limit response length for NPU efficiency"
            ),
        ],
    )
    
    return demo

# =======================================
# PATTERN 2: Advanced Parameter Control
# =======================================
# Enhanced ChatInterface with comprehensive generation controls

def create_advanced_parameter_chat():
    """
    Advanced ChatInterface with full generation parameter control.
    Ideal for OpenVINO GenAI applications requiring fine-tuning.
    """
    
    def generate_with_params(message, history, system_prompt, temperature, top_p, top_k, max_tokens, repetition_penalty):
        """
        Streaming function with comprehensive parameter control.
        This signature matches OpenVINO GenAI GenerationConfig parameters.
        """
        
        # Simulate parameter-aware generation
        # In practice, these parameters would be passed to OpenVINO GenerationConfig
        params_info = f"[T={temperature}, top_p={top_p}, top_k={top_k}, max_tokens={max_tokens}, rep_penalty={repetition_penalty}]"
        response = f"{system_prompt}\n\nUser: {message}\n\nAssistant (with params {params_info}): This is a simulated response that would be generated using the specified parameters."
        
        # Stream with respect to max_tokens
        words = response.split()
        current_response = ""
        
        for i, word in enumerate(words):
            if i >= max_tokens:  # Respect token limit
                break
                
            if i == 0:
                current_response = word
            else:
                current_response += " " + word
            
            time.sleep(0.08)  # Simulate generation with different speeds based on temperature
            yield current_response

    demo = gr.ChatInterface(
        generate_with_params,
        type="messages",
        title="🤖 Advanced OpenVINO GenAI Chat",
        description="Full parameter control for OpenVINO GenAI models",
        additional_inputs=[
            gr.Textbox(
                value="You are an intelligent AI assistant optimized for efficient NPU inference. Provide accurate, helpful responses while being mindful of computational constraints.",
                label="System Prompt",
                lines=3,
                info="Define the AI's role and behavior"
            ),
            gr.Slider(
                minimum=0.1, 
                maximum=2.0, 
                value=0.7,
                step=0.1,
                label="Temperature",
                info="Control randomness (0.1=focused, 2.0=creative)"
            ),
            gr.Slider(
                minimum=0.1, 
                maximum=1.0, 
                value=0.9,
                step=0.05,
                label="Top-p",
                info="Nucleus sampling threshold"
            ),
            gr.Slider(
                minimum=1, 
                maximum=100, 
                value=50,
                step=1,
                label="Top-k", 
                info="Top-k sampling limit"
            ),
            gr.Slider(
                minimum=50, 
                maximum=2048, 
                value=512,
                step=50,
                label="Max Response Tokens",
                info="Maximum tokens to generate (NPU optimized)"
            ),
            gr.Slider(
                minimum=1.0, 
                maximum=2.0, 
                value=1.1,
                step=0.05,
                label="Repetition Penalty",
                info="Penalty for repeating tokens"
            ),
        ],
        additional_inputs_accordion=gr.Accordion("🎛️ Generation Parameters", open=False),
        examples=[
            "Explain quantum computing in simple terms",
            "Write Python code for binary search",
            "Compare NPU vs GPU for AI inference",
            "What are the benefits of model quantization?"
        ],
        cache_examples=False,
    )
    
    return demo

# =====================================
# PATTERN 3: Multi-Modal ChatInterface
# =====================================
# Advanced pattern with file uploads and multi-modal inputs

def create_multimodal_chat():
    """
    Multi-modal ChatInterface supporting text and file inputs.
    Useful for OpenVINO GenAI applications with vision capabilities.
    """
    
    def handle_multimodal_input(message, history, system_prompt, enable_vision):
        """
        Handle both text and potential file inputs.
        Extends to vision-language models with OpenVINO GenAI.
        """
        
        response_prefix = "Vision enabled: " if enable_vision else "Text only: "
        
        # Check if message contains files (multimodal)
        if isinstance(message, dict) and "files" in message:
            file_info = f"[Received {len(message['files'])} file(s)] "
            text_content = message.get("text", "")
            full_response = f"{response_prefix}{file_info}{text_content}"
        else:
            full_response = f"{response_prefix}{message}"
        
        # Stream the response
        for i in range(len(full_response)):
            time.sleep(0.03)
            yield full_response[:i + 1]

    demo = gr.ChatInterface(
        handle_multimodal_input,
        multimodal=True,  # Enable file uploads
        type="messages",
        title="🖼️ Multi-Modal OpenVINO GenAI Chat",
        additional_inputs=[
            gr.Textbox(
                value="You are a multi-modal AI assistant capable of understanding both text and images.",
                label="System Prompt",
                lines=2
            ),
            gr.Checkbox(
                value=False,
                label="Enable Vision Processing",
                info="Use vision-language model for image understanding"
            )
        ],
        examples=[
            {"text": "Hello! How can you help me?"},
            {"text": "Analyze this image for me", "files": ["example.jpg"]},
            {"text": "What can you tell me about AI hardware?"}
        ]
    )
    
    return demo

# =====================================
# PATTERN 4: Session Management Chat
# =====================================
# Advanced session management for stateful OpenVINO GenAI pipelines

def create_session_managed_chat():
    """
    ChatInterface with explicit session management.
    Essential for OpenVINO GenAI stateful pipelines with start_chat/finish_chat.
    """
    
    class SessionState:
        def __init__(self):
            self.session_active = False
            self.session_id = 0
            self.total_tokens = 0
            self.conversation_count = 0
    
    session = SessionState()
    
    def managed_generation(message, history, auto_manage_session, max_conversation_tokens):
        """
        Generation with automatic session management.
        Mimics OpenVINO GenAI stateful pipeline behavior.
        """
        
        if auto_manage_session:
            # Simulate token counting
            estimated_tokens = len(str(history)) + len(message)
            session.total_tokens = estimated_tokens
            
            # Start new session if needed
            if not session.session_active or session.total_tokens > max_conversation_tokens:
                session.session_active = True
                session.session_id += 1
                session.conversation_count = 0
                yield "🔄 New session started (previous session exceeded token limit)\n\n"
            
            session.conversation_count += 1
        
        # Generate response
        response = f"[Session {session.session_id}, Turn {session.conversation_count}] Your message: {message}"
        
        if auto_manage_session:
            response += f"\n\nSession info: {session.total_tokens} tokens used, limit: {max_conversation_tokens}"
        
        # Stream response
        for i in range(len(response)):
            time.sleep(0.02)
            yield response[:i + 1]
    
    def clear_session():
        """Clear session state - equivalent to finish_chat() in OpenVINO GenAI"""
        session.session_active = False
        session.total_tokens = 0
        session.conversation_count = 0
        return None

    demo = gr.ChatInterface(
        managed_generation,
        type="messages",
        title="🔗 Session-Managed OpenVINO GenAI Chat",
        description="Automatic session management for stateful pipelines",
        additional_inputs=[
            gr.Checkbox(
                value=True,
                label="Auto-Manage Sessions",
                info="Automatically start new sessions when token limits exceeded"
            ),
            gr.Slider(
                minimum=500,
                maximum=2048,
                value=1024,
                step=128,
                label="Max Conversation Tokens",
                info="Token limit before starting new session (NPU optimized)"
            )
        ],
        additional_inputs_accordion=gr.Accordion("Session Management", open=True)
    )
    
    # Add custom clear button behavior
    demo.clear_btn.click(clear_session, None, None)
    
    return demo

# =====================================
# INTEGRATION GUIDELINES FOR OPENVINO  
# =====================================

"""
OPENVINO GENAI INTEGRATION GUIDELINES:
=====================================

1. SYSTEM PROMPT INTEGRATION:
   ```python
   # In your streaming function:
   def openvino_generate(message, history, system_prompt, temperature, top_p, max_tokens):
       # Configure generation
       config = ov_genai.GenerationConfig()
       config.temperature = temperature
       config.top_p = top_p  
       config.max_new_tokens = max_tokens
       
       # Build conversation with system prompt
       conversation = [{"role": "system", "content": system_prompt}] + history
       conversation.append({"role": "user", "content": message})
       
       # Generate with OpenVINO
       streamer = YourStreamer(tokenizer)
       pipe.generate(conversation, config, streamer)
       
       for chunk in streamer:
           yield chunk
   ```

2. PARAMETER MAPPING:
   ChatInterface additional_inputs → OpenVINO GenerationConfig:
   - temperature → config.temperature
   - top_p → config.top_p
   - top_k → config.top_k
   - max_tokens → config.max_new_tokens
   - repetition_penalty → config.repetition_penalty

3. SESSION MANAGEMENT:
   For stateful pipelines:
   ```python
   # Start session
   pipe.start_chat(system_prompt)
   
   # Generate (only send current message, not full history)
   pipe.generate(message, config, streamer)
   
   # End session when clearing chat
   pipe.finish_chat()
   ```

4. MULTI-MODAL SUPPORT:
   For vision-language models:
   ```python
   def handle_multimodal(message, history, system_prompt, enable_vision):
       if isinstance(message, dict) and message.get("files"):
           # Handle image files with VLM pipeline
           vlm_pipe.generate(message["text"], message["files"], config, streamer)
       else:
           # Handle text-only with LLM pipeline
           llm_pipe.generate(message, config, streamer)
   ```

RECOMMENDED USAGE:
=================

1. Use PATTERN 1 (System Prompt) for basic OpenVINO GenAI integration
2. Use PATTERN 2 (Advanced Parameters) for production applications 
3. Use PATTERN 3 (Multi-Modal) for vision-language models
4. Use PATTERN 4 (Session Management) for stateful pipelines

The advanced parameter pattern (PATTERN 2) is most suitable for production
OpenVINO GenAI applications as it provides comprehensive control while
maintaining clean separation of concerns.
"""

# Example usage:
if __name__ == "__main__":
    # Choose the pattern most suitable for your OpenVINO GenAI application
    demo = create_advanced_parameter_chat()  # Recommended for production
    demo.launch()

================================================================================

FILE: context\gradio_patterns\concurrency_queue.py
----------------------------------------
# Gradio Concurrency and Queue Management Patterns
# ================================================
#
# PRIORITY: ⭐⭐⭐⭐⭐ (Essential for production OpenVINO GenAI deployments)
#
# This file contains essential patterns for handling concurrency, queue management,
# and multiple simultaneous requests in Gradio applications. Critical for OpenVINO
# GenAI applications where NPU resources are limited and requests must be managed.
#
# Key Learning Points:
# - Proper queue configuration for blocking/non-blocking operations
# - Concurrency limits for resource management
# - Request prioritization and load balancing
# - Error handling for concurrent requests
# - NPU-specific resource management patterns

import gradio as gr
import time
import asyncio
import queue
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
from typing import List, Dict, Any
from dataclasses import dataclass
from enum import Enum

# =======================================
# PATTERN 1: Basic Queue Management
# =======================================
# Source: demo/concurrency_with_queue/run.py (enhanced for OpenVINO)

def create_basic_queue_demo():
    """
    Basic queue management for handling multiple OpenVINO GenAI requests.
    Essential pattern for managing NPU resources efficiently.
    """
    
    def slow_openvino_generation(message: str):
        """Simulate OpenVINO GenAI generation with realistic timing"""
        # Simulate NPU model loading/compilation time
        if random.random() < 0.1:  # 10% chance of compilation
            print(f"🔧 Compiling model for request: {message[:20]}...")
            time.sleep(3)  # NPU compilation overhead
        
        # Simulate actual inference time
        inference_time = random.uniform(2, 6)  # 2-6 seconds for generation
        time.sleep(inference_time)
        
        return f"OpenVINO GenAI Response (took {inference_time:.1f}s): {message}"
    
    with gr.Blocks() as demo:
        gr.Markdown("# 🚀 OpenVINO GenAI Queue Management Demo")
        gr.Markdown("Multiple requests are queued and processed sequentially to manage NPU resources")
        
        input_text = gr.Textbox(label="Your Message", placeholder="Enter message for OpenVINO GenAI...")
        output_text = gr.Textbox(label="Response")
        submit_btn = gr.Button("Generate Response", variant="primary")
        
        # Queue status display
        queue_status = gr.Textbox(label="Queue Status", value="Ready", interactive=False)
        
        def update_queue_status():
            """Update queue status display"""
            return "🟡 Processing request..."
        
        def reset_queue_status():
            """Reset queue status after completion"""
            return "✅ Ready for next request"
        
        # Event with queue management
        submit_btn.click(
            update_queue_status,
            outputs=queue_status,
            queue=False  # Status update should be immediate
        ).then(
            slow_openvino_generation,
            inputs=input_text,
            outputs=output_text
            # queue=True by default - requests will be queued
        ).then(
            reset_queue_status,
            outputs=queue_status,
            queue=False
        )
    
    return demo

# =========================================
# PATTERN 2: Advanced Concurrency Control
# =========================================
# Sophisticated concurrency management for production deployments

class RequestPriority(Enum):
    HIGH = 1
    NORMAL = 2  
    LOW = 3

@dataclass
class OpenVINORequest:
    message: str
    priority: RequestPriority
    user_id: str
    timestamp: float
    max_tokens: int = 512

class OpenVINORequestManager:
    """Advanced request manager for OpenVINO GenAI applications"""
    
    def __init__(self, max_concurrent: int = 2, npu_available: bool = True):
        self.max_concurrent = max_concurrent
        self.npu_available = npu_available
        self.active_requests = 0
        self.request_queue = queue.PriorityQueue()
        self.executor = ThreadPoolExecutor(max_workers=max_concurrent)
        self.request_history: List[Dict] = []
        
    def add_request(self, request: OpenVINORequest):
        """Add request to priority queue"""
        priority_value = request.priority.value
        self.request_queue.put((priority_value, request.timestamp, request))
        
    def process_request(self, request: OpenVINORequest) -> str:
        """Process individual OpenVINO GenAI request"""
        start_time = time.time()
        
        try:
            self.active_requests += 1
            
            # Simulate device selection
            device = "NPU" if self.npu_available and random.random() > 0.3 else "CPU"
            
            # Simulate OpenVINO GenAI processing
            if device == "NPU":
                # NPU processing - faster but limited concurrency
                processing_time = random.uniform(1.5, 3.0)
                compile_time = random.uniform(0.5, 2.0) if random.random() < 0.2 else 0
            else:
                # CPU processing - slower but more concurrent
                processing_time = random.uniform(3.0, 6.0)
                compile_time = 0
            
            time.sleep(compile_time + processing_time)
            
            # Record metrics
            total_time = time.time() - start_time
            self.request_history.append({
                'user_id': request.user_id,
                'priority': request.priority.name,
                'device': device,
                'processing_time': processing_time,
                'compile_time': compile_time,
                'total_time': total_time,
                'max_tokens': request.max_tokens,
                'timestamp': start_time
            })
            
            tokens_per_sec = request.max_tokens / processing_time if processing_time > 0 else 0
            
            result = f"[{device}] Generated response for '{request.message[:30]}...' "
            result += f"({total_time:.1f}s, {tokens_per_sec:.1f} tok/s, Priority: {request.priority.name})"
            
            return result
            
        except Exception as e:
            return f"❌ Error processing request: {str(e)}"
        finally:
            self.active_requests -= 1
    
    def get_queue_status(self) -> Dict[str, Any]:
        """Get current queue status"""
        return {
            'active_requests': self.active_requests,
            'queued_requests': self.request_queue.qsize(),
            'max_concurrent': self.max_concurrent,
            'npu_available': self.npu_available,
            'total_processed': len(self.request_history)
        }

def create_advanced_concurrency_demo():
    """
    Advanced concurrency control demo for OpenVINO GenAI applications.
    Shows proper resource management and request prioritization.
    """
    
    manager = OpenVINORequestManager(max_concurrent=2, npu_available=True)
    
    def submit_request(message: str, priority: str, user_id: str, max_tokens: int):
        """Submit request to the manager"""
        if not message.strip():
            return "⚠️ Please enter a message", manager.get_queue_status()
        
        priority_enum = RequestPriority[priority.upper()]
        request = OpenVINORequest(
            message=message,
            priority=priority_enum,
            user_id=user_id,
            timestamp=time.time(),
            max_tokens=max_tokens
        )
        
        # Process request
        result = manager.process_request(request)
        status = manager.get_queue_status()
        
        return result, status
    
    def get_performance_stats():
        """Get performance statistics"""
        if not manager.request_history:
            return "No requests processed yet"
        
        history = manager.request_history
        avg_total_time = sum(r['total_time'] for r in history) / len(history)
        avg_processing_time = sum(r['processing_time'] for r in history) / len(history)
        
        device_usage = {}
        for r in history:
            device = r['device']
            device_usage[device] = device_usage.get(device, 0) + 1
        
        stats = {
            "total_requests": len(history),
            "avg_total_time": round(avg_total_time, 2),
            "avg_processing_time": round(avg_processing_time, 2),
            "device_usage": device_usage,
            "recent_requests": history[-5:]  # Last 5 requests
        }
        
        return stats
    
    with gr.Blocks() as demo:
        gr.Markdown("# ⚡ Advanced OpenVINO GenAI Concurrency Management")
        gr.Markdown("Production-grade request handling with priority queues and resource management")
        
        with gr.Row():
            with gr.Column(scale=2):
                message_input = gr.Textbox(
                    label="Message",
                    placeholder="Enter your message for OpenVINO GenAI...",
                    lines=2
                )
                with gr.Row():
                    priority_select = gr.Dropdown(
                        choices=["HIGH", "NORMAL", "LOW"],
                        value="NORMAL",
                        label="Priority"
                    )
                    user_id_input = gr.Textbox(
                        value="user1",
                        label="User ID",
                        scale=1
                    )
                    max_tokens_slider = gr.Slider(
                        minimum=100,
                        maximum=1000,
                        value=512,
                        label="Max Tokens"
                    )
                
                submit_btn = gr.Button("🚀 Submit Request", variant="primary", size="lg")
                
            with gr.Column(scale=1):
                queue_status_json = gr.JSON(label="Queue Status")
                performance_btn = gr.Button("📊 Performance Stats")
        
        with gr.Row():
            result_output = gr.Textbox(
                label="Response",
                lines=4,
                max_lines=10
            )
        
        with gr.Row(visible=False) as stats_row:
            performance_stats = gr.JSON(label="Performance Statistics")
        
        # Event handlers
        submit_btn.click(
            submit_request,
            inputs=[message_input, priority_select, user_id_input, max_tokens_slider],
            outputs=[result_output, queue_status_json]
        )
        
        performance_btn.click(
            lambda: (gr.update(visible=True), get_performance_stats()),
            outputs=[stats_row, performance_stats]
        )
        
        # Initialize with current status
        demo.load(
            lambda: manager.get_queue_status(),
            outputs=queue_status_json
        )
    
    return demo

# =========================================
# PATTERN 3: Async Streaming with Queues
# =========================================
# Advanced async pattern for streaming responses with queue management

class AsyncOpenVINOManager:
    """Async OpenVINO GenAI manager for streaming responses"""
    
    def __init__(self, max_concurrent: int = 3):
        self.max_concurrent = max_concurrent
        self.active_streams = {}
        self.stream_counter = 0
        
    async def stream_generation(self, message: str, user_id: str) -> str:
        """Async streaming generation"""
        stream_id = f"stream_{self.stream_counter}_{user_id}"
        self.stream_counter += 1
        
        try:
            self.active_streams[stream_id] = {
                'message': message,
                'user_id': user_id,
                'start_time': time.time(),
                'status': 'processing'
            }
            
            # Simulate async OpenVINO GenAI streaming
            response_parts = [
                f"Processing '{message[:20]}...' with OpenVINO GenAI",
                "Initializing NPU resources...",
                "Compiling model graph...",
                "Starting inference...",
                "Generating tokens...",
                f"Response: This is a simulated streaming response to '{message}'"
            ]
            
            for i, part in enumerate(response_parts):
                await asyncio.sleep(0.8)  # Simulate processing time
                progress = f"[{i+1}/{len(response_parts)}] {part}"
                yield progress
            
            self.active_streams[stream_id]['status'] = 'completed'
            
        except Exception as e:
            self.active_streams[stream_id]['status'] = 'error'
            yield f"❌ Error: {str(e)}"
        
        finally:
            # Keep completed streams for a while for monitoring
            await asyncio.sleep(5)
            self.active_streams.pop(stream_id, None)
    
    def get_active_streams(self) -> Dict[str, Any]:
        """Get information about active streams"""
        return {
            'count': len(self.active_streams),
            'streams': list(self.active_streams.values()),
            'max_concurrent': self.max_concurrent
        }

def create_async_streaming_demo():
    """
    Async streaming demo with queue management.
    Shows how to handle multiple concurrent streaming requests.
    """
    
    manager = AsyncOpenVINOManager(max_concurrent=3)
    
    def sync_stream_wrapper(message: str, user_id: str, history: list):
        """Wrapper to make async streaming work with Gradio"""
        # This is a simplified version - in practice, use proper async integration
        
        # Simulate streaming response
        responses = [
            f"🔄 Starting generation for user {user_id}...",
            f"🧠 Processing: '{message[:30]}...'",
            "⚡ Using NPU for acceleration...",
            "🔧 Optimizing with OpenVINO...",
            f"✅ Generated response: This is a simulated streaming response for '{message}'. The system processed your request using advanced NPU acceleration and OpenVINO optimization."
        ]
        
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ""})
        
        for i, response_part in enumerate(responses):
            if i == 0:
                history[-1]["content"] = response_part
            else:
                history[-1]["content"] = response_part
            
            time.sleep(1.0)  # Simulate streaming delay
            yield history
    
    with gr.Blocks() as demo:
        gr.Markdown("# 🌊 Async Streaming Queue Management")
        gr.Markdown("Concurrent streaming responses with proper queue management")
        
        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    type="messages",
                    height=400,
                    label="Streaming Chat"
                )
                
                with gr.Row():
                    message_input = gr.Textbox(
                        placeholder="Enter message for streaming generation...",
                        scale=4
                    )
                    user_id = gr.Textbox(
                        value="user1",
                        label="User ID",
                        scale=1
                    )
                
                stream_btn = gr.Button("🌊 Start Streaming", variant="primary")
                
            with gr.Column(scale=1):
                gr.Markdown("### Queue Monitor")
                active_streams = gr.JSON(label="Active Streams")
                refresh_btn = gr.Button("🔄 Refresh Status")
        
        # Event handlers
        stream_btn.click(
            sync_stream_wrapper,
            inputs=[message_input, user_id, chatbot],
            outputs=chatbot
        ).then(
            lambda: "",
            outputs=message_input
        )
        
        refresh_btn.click(
            lambda: manager.get_active_streams(),
            outputs=active_streams
        )
        
        # Initialize
        demo.load(
            lambda: manager.get_active_streams(),
            outputs=active_streams
        )
    
    return demo

# =====================================
# INTEGRATION GUIDELINES FOR OPENVINO
# =====================================

"""
OPENVINO GENAI INTEGRATION GUIDELINES:
=====================================

1. QUEUE CONFIGURATION:
   ```python
   # Configure Gradio queue for OpenVINO GenAI
   demo.queue(
       default_concurrency_limit=2,  # Limit concurrent NPU requests
       max_size=10,                   # Maximum queue size
       api_open=False                 # Disable API access if not needed
   )
   
   # Launch with queue enabled
   demo.launch(enable_queue=True)
   ```

2. NPU RESOURCE MANAGEMENT:
   ```python
   class NPUResourceManager:
       def __init__(self):
           self.npu_busy = False
           self.npu_lock = threading.Lock()
       
       def acquire_npu(self):
           with self.npu_lock:
               if self.npu_busy:
                   return False
               self.npu_busy = True
               return True
       
       def release_npu(self):
           with self.npu_lock:
               self.npu_busy = False
   ```

3. REQUEST PRIORITIZATION:
   ```python
   # In your generation function:
   def openvino_generate_with_priority(message, history, priority="NORMAL"):
       if priority == "HIGH":
           # Use NPU if available
           device = "NPU" if npu_manager.acquire_npu() else "CPU"
       else:
           # Use CPU for lower priority requests
           device = "CPU"
       
       try:
           # Your OpenVINO generation logic
           result = pipe.generate(message, device=device)
           return result
       finally:
           if device == "NPU":
               npu_manager.release_npu()
   ```

4. CONCURRENT STREAMING:
   ```python
   # For streaming with multiple users:
   def concurrent_streaming_generate(message, history, user_id):
       # Create user-specific streamer
       streamer = UserSpecificStreamer(user_id, tokenizer)
       
       # Generate with proper resource management
       with resource_manager.acquire_device() as device:
           pipe.generate(message, generation_config, streamer)
           
           for chunk in streamer:
               yield chunk
   ```

5. ERROR HANDLING AND FALLBACKS:
   ```python
   def robust_openvino_generate(message, history):
       try:
           # Try NPU first
           return generate_with_npu(message, history)
       except NPUCompilationError:
           # Fallback to CPU
           return generate_with_cpu(message, history)
       except Exception as e:
           # Return error message to user
           return f"❌ Generation failed: {str(e)}"
   ```

PRODUCTION DEPLOYMENT RECOMMENDATIONS:
=====================================

1. SET APPROPRIATE CONCURRENCY LIMITS:
   - NPU: 1-2 concurrent requests max
   - CPU: 2-4 concurrent requests  
   - GPU: 4-8 concurrent requests

2. IMPLEMENT REQUEST QUEUING:
   - Use priority queues for different user types
   - Implement timeout mechanisms
   - Monitor queue length and processing times

3. RESOURCE MONITORING:
   - Track device utilization
   - Monitor memory usage
   - Log request patterns and response times

4. GRACEFUL DEGRADATION:
   - NPU → CPU fallback for compilation failures
   - Reduced quality modes under high load
   - Circuit breaker patterns for failing requests

USAGE IN YOUR PROJECT:
=====================

The advanced concurrency pattern (PATTERN 2) is most suitable for production
OpenVINO GenAI applications. It provides:

- Proper NPU resource management
- Request prioritization
- Performance monitoring
- Graceful error handling

Simply integrate the RequestManager with your existing OpenVINO GenAI
generation logic to enable robust concurrent request handling.
"""

# Example usage:
if __name__ == "__main__":
    # Choose the pattern most suitable for your deployment
    demo = create_advanced_concurrency_demo()  # Recommended for production
    demo.launch()

================================================================================

FILE: context\gradio_patterns\llm_hf_integration.py
----------------------------------------
# HuggingFace + Gradio Integration Patterns for OpenVINO GenAI
# =============================================================
#
# PRIORITY: ⭐⭐⭐⭐ (Important for model integration best practices)
#
# This file contains official HuggingFace integration patterns that demonstrate
# proper model loading, tokenizer usage, and chat template application. These
# patterns are directly applicable to OpenVINO GenAI applications.
#
# Key Learning Points:
# - Proper tokenizer integration and configuration
# - Chat template application for conversation formatting
# - Model loading and device management patterns
# - Token decoding and post-processing
# - Error handling for model operations

import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
import time

# =======================================
# PATTERN 1: Basic HuggingFace Integration
# =======================================
# Source: demo/llm_hf_transformers/run.py
# Shows the standard pattern for integrating transformers models

def create_basic_hf_integration():
    """
    Basic HuggingFace transformers integration pattern.
    Demonstrates concepts directly applicable to OpenVINO GenAI.
    """
    
    # Model loading pattern (replace with OpenVINO GenAI equivalent)
    checkpoint = "HuggingFaceTB/SmolLM2-135M-Instruct"
    device = "cpu"  # "cuda" or "cpu" - equivalent to OpenVINO device selection
    
    print(f"Loading model: {checkpoint}")
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)
    
    def predict(message, history):
        """
        Core prediction function showing HuggingFace patterns.
        This maps directly to OpenVINO GenAI workflows.
        """
        
        # 1. Build conversation history (same for OpenVINO GenAI)
        history.append({"role": "user", "content": message})
        
        # 2. Apply chat template (OpenVINO GenAI uses same tokenizer.apply_chat_template)
        input_text = tokenizer.apply_chat_template(history, tokenize=False)
        
        # 3. Tokenize input (OpenVINO GenAI handles this internally)
        inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)
        
        # 4. Generate response (equivalent to pipe.generate in OpenVINO GenAI)
        outputs = model.generate(
            inputs, 
            max_new_tokens=100,
            temperature=0.2,
            top_p=0.9,
            do_sample=True
        )
        
        # 5. Decode response (similar post-processing needed for OpenVINO GenAI)
        decoded = tokenizer.decode(outputs[0])
        
        # 6. Extract assistant response from full conversation
        response = decoded.split("<|im_start|>assistant\n")[-1].split("<|im_end|>")[0]
        
        return response

    demo = gr.ChatInterface(predict, type="messages")
    return demo

# =======================================
# PATTERN 2: Advanced Tokenizer Usage
# =======================================
# Enhanced tokenizer patterns for OpenVINO GenAI integration

def create_advanced_tokenizer_demo():
    """
    Advanced tokenizer usage patterns applicable to OpenVINO GenAI.
    Shows proper handling of special tokens, padding, and conversation formatting.
    """
    
    # Simulate OpenVINO GenAI model setup
    model_path = "microsoft/DialoGPT-medium"  # Example model
    
    # Load tokenizer with proper configuration
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # Configure tokenizer for chat applications (same for OpenVINO GenAI)
    if not hasattr(tokenizer, 'pad_token_id') or tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # Add chat template if not present (common need for OpenVINO GenAI)
    if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:
        tokenizer.chat_template = """{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'system' %}{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}"""
    
    def advanced_predict(message, history, system_prompt, max_tokens, temperature):
        """
        Advanced prediction with comprehensive tokenizer usage.
        Shows patterns essential for OpenVINO GenAI integration.
        """
        
        # Build full conversation with system prompt
        conversation = []
        if system_prompt.strip():
            conversation.append({"role": "system", "content": system_prompt})
        
        # Add history and current message
        conversation.extend(history)
        conversation.append({"role": "user", "content": message})
        
        try:
            # Apply chat template with proper settings
            formatted_prompt = tokenizer.apply_chat_template(
                conversation,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # Tokenize with attention to special tokens
            inputs = tokenizer.encode(
                formatted_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2048  # Adjust based on model limits
            )
            
            # Simulate generation (replace with OpenVINO GenAI pipe.generate)
            # This shows the parameters that map to OpenVINO GenerationConfig
            simulated_response = f"[Simulated] Response to '{message}' with temp={temperature}, max_tokens={max_tokens}"
            
            # Token counting for monitoring (important for OpenVINO GenAI)
            input_tokens = len(inputs[0])
            output_tokens = len(tokenizer.encode(simulated_response))
            
            response_with_stats = f"{simulated_response}\n\n📊 Tokens: {input_tokens} input → {output_tokens} output"
            
            return response_with_stats
            
        except Exception as e:
            return f"❌ Error during generation: {str(e)}"
    
    demo = gr.ChatInterface(
        advanced_predict,
        type="messages",
        title="🔧 Advanced Tokenizer Integration",
        additional_inputs=[
            gr.Textbox(
                value="You are a helpful AI assistant.",
                label="System Prompt",
                lines=2
            ),
            gr.Slider(
                minimum=50,
                maximum=512,
                value=150,
                label="Max Tokens"
            ),
            gr.Slider(
                minimum=0.1,
                maximum=2.0,
                value=0.7,
                step=0.1,
                label="Temperature"
            )
        ]
    )
    
    return demo

# =======================================
# PATTERN 3: Streaming Integration
# =======================================
# Streaming patterns compatible with both HuggingFace and OpenVINO GenAI

def create_streaming_integration_demo():
    """
    Streaming integration pattern showing token-by-token generation.
    Demonstrates streaming concepts applicable to OpenVINO GenAI.
    """
    
    model_name = "microsoft/DialoGPT-small"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Configure for proper streaming
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    class StreamingTokenizer:
        """
        Token-level streaming class.
        Similar concept to OpenVINO GenAI StreamerBase.
        """
        
        def __init__(self, tokenizer):
            self.tokenizer = tokenizer
            self.accumulated_tokens = []
            self.accumulated_text = ""
        
        def add_token(self, token_id):
            """Add token and return incremental decoded text"""
            self.accumulated_tokens.append(token_id)
            
            # Decode accumulated tokens
            try:
                new_text = self.tokenizer.decode(
                    self.accumulated_tokens,
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=True
                )
                
                # Return only the new portion
                if len(new_text) > len(self.accumulated_text):
                    new_portion = new_text[len(self.accumulated_text):]
                    self.accumulated_text = new_text
                    return new_portion
                
            except Exception:
                # Fallback for partial tokens
                return ""
            
            return ""
        
        def reset(self):
            """Reset accumulator"""
            self.accumulated_tokens = []
            self.accumulated_text = ""
    
    def streaming_predict(message, history):
        """
        Streaming prediction showing token-by-token processing.
        This pattern translates directly to OpenVINO GenAI streaming.
        """
        
        # Prepare conversation
        conversation = history + [{"role": "user", "content": message}]
        
        # Apply chat template
        prompt = tokenizer.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # Simulate streaming token generation
        # In OpenVINO GenAI, this would be: pipe.generate(prompt, config, streamer)
        simulated_response = f"This is a streaming response to your message: '{message}'. Each word appears progressively to demonstrate token-level streaming capabilities."
        
        words = simulated_response.split()
        current_response = ""
        
        for i, word in enumerate(words):
            if i == 0:
                current_response = word
            else:
                current_response += " " + word
            
            time.sleep(0.1)  # Simulate token generation time
            yield current_response
    
    def streaming_chat(message, history):
        """
        Chat interface compatible streaming function.
        Maps to OpenVINO GenAI streaming patterns.
        """
        response_generator = streaming_predict(message, history)
        
        for partial_response in response_generator:
            yield partial_response
    
    demo = gr.ChatInterface(
        streaming_chat,
        type="messages",
        title="🌊 Streaming Integration Demo",
        description="Token-by-token streaming compatible with OpenVINO GenAI patterns"
    )
    
    return demo

# =======================================
# PATTERN 4: Error Handling & Fallbacks
# =======================================
# Robust error handling patterns for production use

def create_robust_integration_demo():
    """
    Robust integration with comprehensive error handling.
    Essential patterns for production OpenVINO GenAI applications.
    """
    
    class RobustModelHandler:
        """Model handler with comprehensive error handling"""
        
        def __init__(self, model_path: str):
            self.model_path = model_path
            self.tokenizer = None
            self.model = None
            self.loaded = False
            self.load_errors = []
        
        def load_model(self):
            """Load model with proper error handling"""
            try:
                print(f"🔄 Loading tokenizer from {self.model_path}")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                
                # Configure tokenizer
                if self.tokenizer.pad_token_id is None:
                    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
                
                print(f"🔄 Loading model from {self.model_path}")
                # In OpenVINO GenAI: pipe = ov_genai.LLMPipeline(model_path, device)
                self.model = "simulated_openvino_pipeline"  # Placeholder
                
                self.loaded = True
                print("✅ Model loaded successfully")
                
            except Exception as e:
                error_msg = f"Failed to load model: {str(e)}"
                self.load_errors.append(error_msg)
                print(f"❌ {error_msg}")
                self.loaded = False
        
        def generate_with_fallback(self, message: str, history: list, **kwargs):
            """Generate response with fallback strategies"""
            
            if not self.loaded:
                return "❌ Model not loaded. Please check the model path and try again."
            
            try:
                # Build conversation
                conversation = history + [{"role": "user", "content": message}]
                
                # Apply chat template with error handling
                try:
                    prompt = self.tokenizer.apply_chat_template(
                        conversation,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                except Exception as e:
                    # Fallback: manual template
                    prompt = f"User: {message}\nAssistant:"
                    print(f"⚠️ Chat template failed, using fallback: {e}")
                
                # Simulate generation with error handling
                # In OpenVINO GenAI: result = pipe.generate(prompt, generation_config, streamer)
                try:
                    response = f"Generated response for: '{message}' (using robust error handling)"
                    
                    # Simulate potential generation errors
                    if len(message) > 1000:  # Simulate token limit error
                        raise ValueError("Input too long")
                    
                    return response
                    
                except ValueError as e:
                    if "too long" in str(e):
                        # Truncate and retry
                        truncated_message = message[:500] + "... [truncated]"
                        return f"⚠️ Input truncated due to length. Response to: '{truncated_message}'"
                    else:
                        raise e
                
            except Exception as e:
                # Final fallback
                error_response = f"❌ Generation failed: {str(e)[:100]}..."
                
                # Suggest actions based on error type
                if "memory" in str(e).lower():
                    error_response += "\n💡 Try a shorter message or restart the application."
                elif "timeout" in str(e).lower():
                    error_response += "\n💡 Request timed out. Please try again."
                elif "compilation" in str(e).lower():
                    error_response += "\n💡 Model compilation failed. Check device compatibility."
                
                return error_response
    
    # Initialize robust handler
    handler = RobustModelHandler("microsoft/DialoGPT-small")
    handler.load_model()
    
    def robust_predict(message, history, enable_fallback, max_retries):
        """Prediction with configurable error handling"""
        
        retries = 0
        while retries < max_retries:
            try:
                result = handler.generate_with_fallback(message, history)
                
                if result.startswith("❌") and enable_fallback and retries < max_retries - 1:
                    retries += 1
                    time.sleep(1)  # Brief delay before retry
                    continue
                
                return result
                
            except Exception as e:
                retries += 1
                if retries >= max_retries:
                    return f"❌ Failed after {max_retries} retries: {str(e)}"
                time.sleep(1)
        
        return "❌ Unexpected error in retry logic"
    
    demo = gr.ChatInterface(
        robust_predict,
        type="messages",
        title="🛡️ Robust Integration with Error Handling",
        description="Production-ready error handling and fallback strategies",
        additional_inputs=[
            gr.Checkbox(
                value=True,
                label="Enable Automatic Fallbacks",
                info="Automatically handle common errors"
            ),
            gr.Slider(
                minimum=1,
                maximum=5,
                value=2,
                step=1,
                label="Max Retries",
                info="Number of retry attempts on failure"
            )
        ]
    )
    
    return demo

# =====================================
# INTEGRATION GUIDELINES FOR OPENVINO
# =====================================

"""
OPENVINO GENAI INTEGRATION GUIDELINES:
=====================================

1. TOKENIZER INTEGRATION:
   ```python
   # Load tokenizer (same as HuggingFace)
   tokenizer = AutoTokenizer.from_pretrained(model_path)
   
   # Configure for OpenVINO GenAI
   if tokenizer.pad_token_id is None:
       tokenizer.pad_token_id = tokenizer.eos_token_id
   
   # Load OpenVINO GenAI pipeline
   pipe = ov_genai.LLMPipeline(model_path, device)
   ```

2. CHAT TEMPLATE APPLICATION:
   ```python
   # Build conversation (same as HuggingFace)
   conversation = [{"role": "system", "content": system_prompt}] + history
   conversation.append({"role": "user", "content": message})
   
   # Apply chat template (same as HuggingFace)
   prompt = tokenizer.apply_chat_template(
       conversation,
       tokenize=False,
       add_generation_prompt=True
   )
   
   # Generate with OpenVINO GenAI
   response = pipe.generate(prompt, generation_config)
   ```

3. STREAMING INTEGRATION:
   ```python
   # Create custom streamer (extends ov_genai.StreamerBase)
   class OpenVINOStreamer(ov_genai.StreamerBase):
       def __init__(self, tokenizer):
           super().__init__()
           self.tokenizer = tokenizer
           self.tokens = []
       
       def put(self, token_id):
           self.tokens.append(token_id)
           # Decode incrementally
           text = self.tokenizer.decode(self.tokens, skip_special_tokens=True)
           return False  # Continue generation
   ```

4. ERROR HANDLING PATTERNS:
   ```python
   def openvino_generate_with_fallback(message, history):
       try:
           # Try main generation
           return pipe.generate(prompt, generation_config)
       except ov_genai.CompilationError:
           # Handle NPU compilation issues
           return generate_with_cpu_fallback(prompt)
       except ov_genai.TokenLimitError:
           # Handle token limit exceeded
           truncated_prompt = truncate_conversation(prompt)
           return pipe.generate(truncated_prompt, generation_config)
   ```

5. DEVICE MANAGEMENT:
   ```python
   # Device selection (similar to HuggingFace device handling)
   devices = ["NPU", "CPU", "GPU"]
   
   for device in devices:
       try:
           pipe = ov_genai.LLMPipeline(model_path, device)
           print(f"✅ Loaded on {device}")
           break
       except Exception as e:
           print(f"❌ {device} failed: {e}")
           continue
   ```

KEY DIFFERENCES FROM HUGGINGFACE:
===============================

1. MODEL LOADING:
   - HuggingFace: AutoModelForCausalLM.from_pretrained()
   - OpenVINO GenAI: ov_genai.LLMPipeline(model_path, device)

2. GENERATION:
   - HuggingFace: model.generate(inputs, **generation_kwargs)
   - OpenVINO GenAI: pipe.generate(prompt, generation_config, streamer)

3. STREAMING:
   - HuggingFace: Custom implementation required
   - OpenVINO GenAI: Built-in StreamerBase class

4. DEVICE HANDLING:
   - HuggingFace: .to(device)
   - OpenVINO GenAI: Device specified at pipeline creation

RECOMMENDED USAGE:
=================

1. Use PATTERN 1 for understanding basic integration concepts
2. Use PATTERN 2 for advanced tokenizer handling
3. Use PATTERN 3 for streaming implementations
4. Use PATTERN 4 for production error handling

The robust integration pattern (PATTERN 4) is essential for production
OpenVINO GenAI applications where reliability is critical.
"""

# Example usage:
if __name__ == "__main__":
    # Choose the pattern most suitable for your integration needs
    demo = create_robust_integration_demo()  # Recommended for production
    demo.launch()

================================================================================

FILE: context\gradio_patterns\performance_dashboard.py
----------------------------------------
# Gradio Performance Dashboard Patterns
# ====================================
#
# PRIORITY: ⭐⭐⭐⭐⭐ (Essential for OpenVINO GenAI monitoring)
#
# This file contains professional dashboard patterns for monitoring OpenVINO GenAI
# performance, metrics collection, and real-time visualization. These patterns are
# crucial for production deployments where performance monitoring is essential.
#
# Key Learning Points:
# - Real-time metrics collection and visualization
# - Professional dashboard layouts with multiple panels
# - Dynamic plot updates and data management
# - Performance monitoring best practices
# - Integration with ML model metrics

import gradio as gr
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import time
import random
from datetime import datetime, timedelta
import json

# =======================================
# PATTERN 1: Basic Performance Monitor
# =======================================
# Source: demo/dashboard/run.py (adapted for OpenVINO GenAI)

def create_basic_performance_dashboard():
    """
    Basic performance monitoring dashboard for OpenVINO GenAI applications.
    Tracks essential metrics like response time, token throughput, and device utilization.
    """
    
    # Simulate OpenVINO GenAI metrics storage
    class OpenVINOMetrics:
        def __init__(self):
            self.response_times = []
            self.token_rates = []
            self.device_utilization = []
            self.error_counts = []
            self.timestamps = []
            
        def add_metric(self, response_time, tokens_per_sec, device_util, errors=0):
            now = datetime.now()
            self.response_times.append(response_time)
            self.token_rates.append(tokens_per_sec)
            self.device_utilization.append(device_util)
            self.error_counts.append(errors)
            self.timestamps.append(now)
            
            # Keep only last 100 measurements
            if len(self.timestamps) > 100:
                self.response_times.pop(0)
                self.token_rates.pop(0)
                self.device_utilization.pop(0)
                self.error_counts.pop(0)
                self.timestamps.pop(0)
    
    metrics = OpenVINOMetrics()
    
    def generate_response_time_plot():
        """Generate response time visualization"""
        if not metrics.timestamps:
            return gr.Plot(visible=False)
            
        df = pd.DataFrame({
            'timestamp': metrics.timestamps,
            'response_time': metrics.response_times
        })
        
        plot = px.line(df, x='timestamp', y='response_time',
                      title='OpenVINO GenAI Response Times',
                      labels={'response_time': 'Response Time (seconds)'})
        plot.update_layout(title_x=0.5)
        return gr.Plot(value=plot, visible=True)
    
    def generate_throughput_plot():
        """Generate token throughput visualization"""
        if not metrics.timestamps:
            return gr.Plot(visible=False)
            
        df = pd.DataFrame({
            'timestamp': metrics.timestamps,
            'tokens_per_sec': metrics.token_rates
        })
        
        plot = px.bar(df.tail(20), x='timestamp', y='tokens_per_sec',
                     title='Token Generation Throughput',
                     labels={'tokens_per_sec': 'Tokens/Second'})
        plot.update_layout(title_x=0.5)
        return gr.Plot(value=plot, visible=True)
    
    def generate_device_utilization_plot():
        """Generate device utilization visualization"""
        if not metrics.timestamps:
            return gr.Plot(visible=False)
            
        df = pd.DataFrame({
            'timestamp': metrics.timestamps,
            'utilization': metrics.device_utilization
        })
        
        plot = px.area(df, x='timestamp', y='utilization',
                      title='NPU/CPU Device Utilization',
                      labels={'utilization': 'Utilization %'})
        plot.update_layout(title_x=0.5, yaxis_range=[0, 100])
        return gr.Plot(value=plot, visible=True)
    
    def simulate_metrics_update():
        """Simulate adding new metrics (replace with real OpenVINO data)"""
        # Simulate realistic OpenVINO GenAI metrics
        response_time = random.uniform(0.5, 3.0)  # 0.5-3 seconds
        tokens_per_sec = random.uniform(15, 45)   # 15-45 tokens/second
        device_util = random.uniform(60, 95)      # 60-95% utilization
        errors = random.randint(0, 1)             # Occasional errors
        
        metrics.add_metric(response_time, tokens_per_sec, device_util, errors)
        
        return (
            generate_response_time_plot(),
            generate_throughput_plot(), 
            generate_device_utilization_plot(),
            get_summary_stats()
        )
    
    def get_summary_stats():
        """Get summary statistics"""
        if not metrics.response_times:
            return {"status": "No data available"}
            
        avg_response = sum(metrics.response_times) / len(metrics.response_times)
        avg_throughput = sum(metrics.token_rates) / len(metrics.token_rates)
        avg_utilization = sum(metrics.device_utilization) / len(metrics.device_utilization)
        total_errors = sum(metrics.error_counts)
        
        return {
            "total_requests": len(metrics.response_times),
            "avg_response_time": round(avg_response, 3),
            "avg_throughput": round(avg_throughput, 1),
            "avg_device_utilization": round(avg_utilization, 1),
            "total_errors": total_errors,
            "uptime": f"{len(metrics.timestamps) * 2} seconds"  # Assuming 2s intervals
        }
    
    with gr.Blocks(title="OpenVINO GenAI Performance Dashboard") as demo:
        gr.Markdown("# 🚀 OpenVINO GenAI Performance Dashboard")
        gr.Markdown("Real-time monitoring of NPU/CPU performance, token throughput, and system metrics")
        
        with gr.Row():
            update_btn = gr.Button("📊 Update Metrics", variant="primary")
            auto_refresh = gr.Checkbox(label="🔄 Auto-refresh (every 5s)", value=False)
        
        with gr.Row():
            with gr.Column():
                response_plot = gr.Plot(label="Response Times")
                device_plot = gr.Plot(label="Device Utilization") 
            with gr.Column():
                throughput_plot = gr.Plot(label="Token Throughput")
                summary_stats = gr.JSON(label="Summary Statistics")
        
        # Manual update
        update_btn.click(
            simulate_metrics_update,
            outputs=[response_plot, throughput_plot, device_plot, summary_stats]
        )
        
        # Auto-refresh setup (simplified - in practice use gr.Timer)
        def auto_update_handler():
            return simulate_metrics_update()
        
        # Initial load
        demo.load(
            simulate_metrics_update,
            outputs=[response_plot, throughput_plot, device_plot, summary_stats]
        )
    
    return demo

# =========================================
# PATTERN 2: Advanced Metrics Dashboard
# =========================================
# Comprehensive dashboard with multiple metrics and real-time updates

def create_advanced_metrics_dashboard():
    """
    Advanced dashboard with comprehensive OpenVINO GenAI metrics,
    model comparison, and performance analysis tools.
    """
    
    class AdvancedMetrics:
        def __init__(self):
            self.data = []
            
        def add_request(self, model_name, device, input_tokens, output_tokens, 
                       response_time, temperature, compilation_time=None):
            entry = {
                'timestamp': datetime.now(),
                'model': model_name,
                'device': device,
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'total_tokens': input_tokens + output_tokens,
                'response_time': response_time,
                'tokens_per_second': output_tokens / response_time if response_time > 0 else 0,
                'temperature': temperature,
                'compilation_time': compilation_time
            }
            self.data.append(entry)
            
            # Keep last 500 entries
            if len(self.data) > 500:
                self.data.pop(0)
    
    metrics = AdvancedMetrics()
    
    def create_performance_comparison_plot(models_to_compare, device_filter):
        """Create model performance comparison"""
        if not metrics.data:
            return gr.Plot(visible=False)
        
        df = pd.DataFrame(metrics.data)
        
        # Apply filters
        if device_filter != "All":
            df = df[df['device'] == device_filter]
        if models_to_compare:
            df = df[df['model'].isin(models_to_compare)]
        
        if df.empty:
            return gr.Plot(visible=False)
        
        # Group by model and calculate averages
        comparison = df.groupby('model').agg({
            'tokens_per_second': 'mean',
            'response_time': 'mean',
            'total_tokens': 'mean'
        }).reset_index()
        
        plot = px.scatter(comparison, x='response_time', y='tokens_per_second',
                         size='total_tokens', color='model',
                         title='Model Performance Comparison',
                         labels={
                             'response_time': 'Avg Response Time (s)',
                             'tokens_per_second': 'Avg Tokens/Second'
                         })
        plot.update_layout(title_x=0.5)
        return gr.Plot(value=plot, visible=True)
    
    def create_device_comparison_plot():
        """Create device performance comparison"""
        if not metrics.data:
            return gr.Plot(visible=False)
        
        df = pd.DataFrame(metrics.data)
        device_stats = df.groupby('device').agg({
            'tokens_per_second': ['mean', 'std'],
            'response_time': ['mean', 'std']
        }).round(2)
        
        # Flatten column names
        device_stats.columns = ['_'.join(col).strip() for col in device_stats.columns]
        device_stats = device_stats.reset_index()
        
        plot = px.bar(device_stats, x='device', y='tokens_per_second_mean',
                     error_y='tokens_per_second_std',
                     title='Device Performance Comparison',
                     labels={'tokens_per_second_mean': 'Average Tokens/Second'})
        plot.update_layout(title_x=0.5)
        return gr.Plot(value=plot, visible=True)
    
    def create_timeline_analysis():
        """Create timeline performance analysis"""
        if not metrics.data:
            return gr.Plot(visible=False)
        
        df = pd.DataFrame(metrics.data)
        df['hour'] = df['timestamp'].dt.floor('H')
        
        hourly_stats = df.groupby(['hour', 'device']).agg({
            'tokens_per_second': 'mean',
            'response_time': 'mean'
        }).reset_index()
        
        plot = px.line(hourly_stats, x='hour', y='tokens_per_second',
                      color='device', title='Performance Over Time',
                      labels={'tokens_per_second': 'Tokens/Second'})
        plot.update_layout(title_x=0.5)
        return gr.Plot(value=plot, visible=True)
    
    def generate_performance_report():
        """Generate comprehensive performance report"""
        if not metrics.data:
            return "No data available for report generation."
        
        df = pd.DataFrame(metrics.data)
        
        report = {
            "summary": {
                "total_requests": len(df),
                "avg_tokens_per_second": round(df['tokens_per_second'].mean(), 2),
                "avg_response_time": round(df['response_time'].mean(), 2),
                "total_tokens_processed": int(df['total_tokens'].sum())
            },
            "by_device": df.groupby('device').agg({
                'tokens_per_second': ['mean', 'max', 'min'],
                'response_time': ['mean', 'max', 'min']
            }).round(2).to_dict(),
            "by_model": df.groupby('model').agg({
                'tokens_per_second': 'mean',
                'response_time': 'mean'
            }).round(2).to_dict(),
            "recent_performance": df.tail(10)[['timestamp', 'model', 'device', 
                                           'tokens_per_second', 'response_time']].to_dict('records')
        }
        
        return json.dumps(report, indent=2, default=str)
    
    def simulate_advanced_metrics():
        """Simulate adding advanced metrics"""
        models = ["qwen3-8b-int4", "llama2-7b-int8", "mistral-7b-fp16"]
        devices = ["NPU", "CPU", "GPU"]
        
        for _ in range(5):  # Add 5 random entries
            model = random.choice(models)
            device = random.choice(devices)
            input_tokens = random.randint(50, 500)
            output_tokens = random.randint(100, 300)
            response_time = random.uniform(1.0, 5.0)
            temperature = random.uniform(0.3, 1.2)
            
            # NPU has compilation overhead sometimes
            compilation_time = random.uniform(2.0, 8.0) if device == "NPU" and random.random() < 0.3 else None
            
            metrics.add_request(model, device, input_tokens, output_tokens,
                              response_time, temperature, compilation_time)
        
        return "✅ Added 5 new metric entries"
    
    with gr.Blocks(title="Advanced OpenVINO GenAI Analytics") as demo:
        gr.Markdown("# 📊 Advanced OpenVINO GenAI Performance Analytics")
        gr.Markdown("Comprehensive monitoring, comparison, and analysis of OpenVINO GenAI models")
        
        with gr.Row():
            with gr.Column(scale=2):
                model_selector = gr.CheckboxGroup(
                    choices=["qwen3-8b-int4", "llama2-7b-int8", "mistral-7b-fp16"],
                    label="Models to Compare",
                    value=["qwen3-8b-int4"]
                )
            with gr.Column(scale=1):
                device_filter = gr.Dropdown(
                    choices=["All", "NPU", "CPU", "GPU"],
                    value="All",
                    label="Device Filter"
                )
            with gr.Column(scale=1):
                simulate_btn = gr.Button("🎲 Add Sample Data", variant="secondary")
                update_btn = gr.Button("📊 Update Dashboard", variant="primary")
        
        with gr.Tabs():
            with gr.Tab("Performance Comparison"):
                comparison_plot = gr.Plot(label="Model Performance Comparison")
                device_comparison_plot = gr.Plot(label="Device Comparison")
            
            with gr.Tab("Timeline Analysis"):
                timeline_plot = gr.Plot(label="Performance Over Time")
            
            with gr.Tab("Detailed Report"):
                report_output = gr.Code(label="Performance Report (JSON)", language="json")
                generate_report_btn = gr.Button("📋 Generate Report")
        
        # Event handlers
        simulate_btn.click(simulate_advanced_metrics, outputs=gr.Textbox(visible=False))
        
        update_btn.click(
            lambda models, device: (
                create_performance_comparison_plot(models, device),
                create_device_comparison_plot(),
                create_timeline_analysis()
            ),
            inputs=[model_selector, device_filter],
            outputs=[comparison_plot, device_comparison_plot, timeline_plot]
        )
        
        generate_report_btn.click(generate_performance_report, outputs=report_output)
        
        # Initial load with sample data
        demo.load(
            lambda: simulate_advanced_metrics() + update_btn.click(),
            outputs=gr.Textbox(visible=False)
        )
    
    return demo

# =========================================
# PATTERN 3: Real-Time Monitoring System  
# =========================================
# Live monitoring system with alerts and thresholds

def create_realtime_monitoring_system():
    """
    Real-time monitoring system with alerts and performance thresholds.
    Essential for production OpenVINO GenAI deployments.
    """
    
    class MonitoringSystem:
        def __init__(self):
            self.alerts = []
            self.thresholds = {
                'max_response_time': 5.0,
                'min_tokens_per_second': 10.0,
                'max_error_rate': 0.05,
                'max_memory_usage': 85.0
            }
            self.current_metrics = {}
            
        def check_alerts(self, metrics):
            alerts = []
            
            if metrics.get('response_time', 0) > self.thresholds['max_response_time']:
                alerts.append(f"⚠️ High response time: {metrics['response_time']:.2f}s")
            
            if metrics.get('tokens_per_second', 0) < self.thresholds['min_tokens_per_second']:
                alerts.append(f"⚠️ Low throughput: {metrics['tokens_per_second']:.1f} tok/s")
            
            if metrics.get('error_rate', 0) > self.thresholds['max_error_rate']:
                alerts.append(f"🚨 High error rate: {metrics['error_rate']:.2%}")
            
            return alerts
    
    monitor = MonitoringSystem()
    
    def update_realtime_metrics():
        """Update real-time metrics with alert checking"""
        # Simulate current system state
        current = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'response_time': random.uniform(0.8, 6.0),
            'tokens_per_second': random.uniform(8, 35),
            'active_requests': random.randint(0, 10),
            'queue_length': random.randint(0, 5),
            'memory_usage': random.uniform(60, 95),
            'error_rate': random.uniform(0, 0.08),
            'device_temperature': random.uniform(45, 75)
        }
        
        monitor.current_metrics = current
        alerts = monitor.check_alerts(current)
        
        # Create status indicator
        status = "🟢 Healthy"
        if alerts:
            if any("🚨" in alert for alert in alerts):
                status = "🔴 Critical"
            else:
                status = "🟡 Warning"
        
        # Format metrics for display
        metrics_display = {
            "System Status": status,
            "Response Time": f"{current['response_time']:.2f}s",
            "Token Throughput": f"{current['tokens_per_second']:.1f} tok/s",
            "Active Requests": current['active_requests'],
            "Queue Length": current['queue_length'],
            "Memory Usage": f"{current['memory_usage']:.1f}%",
            "Error Rate": f"{current['error_rate']:.2%}",
            "Device Temperature": f"{current['device_temperature']:.1f}°C"
        }
        
        alerts_text = "\n".join(alerts) if alerts else "✅ No alerts"
        
        return metrics_display, alerts_text, status
    
    def update_threshold(threshold_name, new_value):
        """Update monitoring thresholds"""
        if threshold_name in monitor.thresholds:
            monitor.thresholds[threshold_name] = new_value
            return f"✅ Updated {threshold_name} threshold to {new_value}"
        return f"❌ Unknown threshold: {threshold_name}"
    
    with gr.Blocks(title="OpenVINO GenAI Real-Time Monitor") as demo:
        gr.Markdown("# 🔴 OpenVINO GenAI Real-Time Monitoring System")
        gr.Markdown("Live performance monitoring with automated alerts and threshold management")
        
        with gr.Row():
            with gr.Column(scale=2):
                status_display = gr.HTML("<h2>🟢 System Status: Initializing...</h2>")
                metrics_json = gr.JSON(label="Current Metrics")
            with gr.Column(scale=1):
                alerts_display = gr.Textbox(
                    label="🚨 Active Alerts",
                    lines=5,
                    placeholder="No alerts"
                )
        
        with gr.Row():
            refresh_btn = gr.Button("🔄 Refresh Metrics", variant="primary")
            auto_refresh_checkbox = gr.Checkbox(label="Auto-refresh (5s intervals)", value=True)
        
        with gr.Accordion("⚙️ Alert Thresholds", open=False):
            with gr.Row():
                with gr.Column():
                    max_response_threshold = gr.Slider(
                        minimum=1.0, maximum=10.0, value=5.0, step=0.1,
                        label="Max Response Time (seconds)"
                    )
                    min_throughput_threshold = gr.Slider(
                        minimum=5.0, maximum=50.0, value=10.0, step=1.0,
                        label="Min Tokens/Second"
                    )
                with gr.Column():
                    max_error_threshold = gr.Slider(
                        minimum=0.01, maximum=0.20, value=0.05, step=0.01,
                        label="Max Error Rate"
                    )
                    max_memory_threshold = gr.Slider(
                        minimum=70.0, maximum=95.0, value=85.0, step=1.0,
                        label="Max Memory Usage (%)"
                    )
            
            update_thresholds_btn = gr.Button("💾 Update Thresholds")
        
        # Event handlers
        def refresh_with_status():
            metrics, alerts, status = update_realtime_metrics()
            status_html = f"<h2>{status}</h2>"
            return metrics, alerts, status_html
        
        refresh_btn.click(
            refresh_with_status,
            outputs=[metrics_json, alerts_display, status_display]
        )
        
        def update_all_thresholds(max_response, min_throughput, max_error, max_memory):
            monitor.thresholds['max_response_time'] = max_response
            monitor.thresholds['min_tokens_per_second'] = min_throughput  
            monitor.thresholds['max_error_rate'] = max_error
            monitor.thresholds['max_memory_usage'] = max_memory
            return "✅ All thresholds updated successfully"
        
        update_thresholds_btn.click(
            update_all_thresholds,
            inputs=[max_response_threshold, min_throughput_threshold, 
                   max_error_threshold, max_memory_threshold],
            outputs=gr.Textbox(label="Update Status", visible=True)
        )
        
        # Initial load
        demo.load(refresh_with_status, outputs=[metrics_json, alerts_display, status_display])
    
    return demo

# =====================================
# INTEGRATION GUIDELINES FOR OPENVINO
# =====================================

"""
OPENVINO GENAI INTEGRATION GUIDELINES:
=====================================

1. METRICS COLLECTION:
   ```python
   # In your OpenVINO GenAI application:
   class OpenVINOMetricsCollector:
       def track_request(self, start_time, end_time, input_tokens, output_tokens, device, errors=0):
           response_time = end_time - start_time
           tokens_per_sec = output_tokens / response_time if response_time > 0 else 0
           
           # Store in your metrics system
           self.add_metric(response_time, tokens_per_sec, device, input_tokens, output_tokens, errors)
   ```

2. REAL-TIME MONITORING:
   ```python
   # Integrate with your streaming generation:
   def openvino_generate_with_monitoring(message, history):
       start_time = time.time()
       
       try:
           # Your OpenVINO generation
           streamer = YourStreamer(tokenizer)
           pipe.generate(prompt, config, streamer)
           
           for chunk in streamer:
               yield chunk
           
           # Record successful metrics
           end_time = time.time()
           collector.track_request(start_time, end_time, input_tokens, output_tokens, device)
           
       except Exception as e:
           # Record error metrics
           end_time = time.time()
           collector.track_request(start_time, end_time, input_tokens, 0, device, errors=1)
           raise
   ```

3. DEVICE-SPECIFIC MONITORING:
   ```python
   # NPU-specific monitoring
   def monitor_npu_performance():
       return {
           'compilation_time': get_npu_compilation_time(),
           'inference_time': get_npu_inference_time(),
           'memory_usage': get_npu_memory_usage(),
           'temperature': get_npu_temperature()
       }
   ```

RECOMMENDED USAGE:
=================

1. Use PATTERN 1 for basic performance monitoring during development
2. Use PATTERN 2 for comprehensive model comparison and analysis
3. Use PATTERN 3 for production monitoring with alerts

The real-time monitoring system (PATTERN 3) is essential for production
OpenVINO GenAI deployments where uptime and performance are critical.

DASHBOARD DEPLOYMENT:
====================

Deploy alongside your main application:
```python
# Main chat interface
chat_demo = your_openvino_chat_interface()

# Performance dashboard  
dashboard_demo = create_advanced_metrics_dashboard()

# Combine or deploy separately
gr.TabbedInterface([chat_demo, dashboard_demo], 
                  ["💬 Chat", "📊 Dashboard"]).launch()
```
"""

# Example usage:
if __name__ == "__main__":
    # Choose the dashboard pattern most suitable for your needs
    demo = create_advanced_metrics_dashboard()  # Recommended for production
    demo.launch()

================================================================================

FILE: context\gradio_testing\chat_interface_tests.py
----------------------------------------
# Gradio ChatInterface Testing Patterns
# ====================================
#
# PRIORITY: ⭐⭐⭐ (Important for quality assurance)
#
# This file contains testing patterns and methodologies for Gradio chat interfaces,
# extracted from the official Gradio test suite. These patterns are essential for
# ensuring reliability in OpenVINO GenAI applications.
#
# Key Learning Points:
# - Proper test structure for chat interfaces
# - Async and sync testing patterns
# - Streaming response validation
# - Error condition testing
# - Performance and concurrency testing

import pytest
import asyncio
import time
from concurrent.futures import wait
from unittest.mock import patch, MagicMock
import gradio as gr

# =======================================
# PATTERN 1: Basic Chat Function Tests
# =======================================
# Source: test/test_chat_interface.py (adapted)

class TestChatFunctions:
    """Test patterns for chat function validation"""
    
    def test_invalid_function_signature(self):
        """Test that invalid function signatures are rejected"""
        def invalid_fn(message):  # Missing history parameter
            return message
        
        with pytest.raises(TypeError):
            gr.ChatInterface(invalid_fn)
    
    def test_valid_sync_function(self):
        """Test valid synchronous chat function"""
        def valid_double(message, history):
            return message + " " + message
        
        # Should not raise exception
        chat_interface = gr.ChatInterface(valid_double)
        assert chat_interface is not None
    
    def test_valid_async_function(self):
        """Test valid asynchronous chat function"""
        async def async_greet(message, history):
            return "hi, " + message
        
        # Should not raise exception
        chat_interface = gr.ChatInterface(async_greet)
        assert chat_interface is not None
    
    def test_streaming_function(self):
        """Test streaming chat function"""
        def stream_response(message, history):
            for i in range(len(message)):
                yield message[:i + 1]
        
        chat_interface = gr.ChatInterface(stream_response)
        assert chat_interface is not None
    
    def test_async_streaming_function(self):
        """Test async streaming chat function"""
        async def async_stream(message, history):
            for i in range(len(message)):
                yield message[:i + 1]
        
        chat_interface = gr.ChatInterface(async_stream)
        assert chat_interface is not None

# =======================================
# PATTERN 2: OpenVINO GenAI Test Patterns
# =======================================
# Specific test patterns for OpenVINO GenAI integration

class TestOpenVINOGenAIIntegration:
    """Test patterns specific to OpenVINO GenAI applications"""
    
    @pytest.fixture
    def mock_openvino_pipeline(self):
        """Mock OpenVINO GenAI pipeline for testing"""
        mock_pipe = MagicMock()
        mock_pipe.generate.return_value = "Mocked OpenVINO response"
        return mock_pipe
    
    @pytest.fixture
    def mock_tokenizer(self):
        """Mock tokenizer for testing"""
        mock_tokenizer = MagicMock()
        mock_tokenizer.apply_chat_template.return_value = "Formatted prompt"
        mock_tokenizer.encode.return_value = [1, 2, 3, 4, 5]
        return mock_tokenizer
    
    def test_openvino_chat_function(self, mock_openvino_pipeline, mock_tokenizer):
        """Test OpenVINO GenAI chat function"""
        
        def openvino_chat(message, history):
            # Simulate OpenVINO GenAI generation
            conversation = history + [{"role": "user", "content": message}]
            prompt = mock_tokenizer.apply_chat_template(conversation, tokenize=False)
            response = mock_openvino_pipeline.generate(prompt)
            return response
        
        # Test the function
        result = openvino_chat("Hello", [])
        assert result == "Mocked OpenVINO response"
        
        # Verify tokenizer was called
        mock_tokenizer.apply_chat_template.assert_called_once()
        
        # Verify pipeline was called
        mock_openvino_pipeline.generate.assert_called_once()
    
    def test_streaming_openvino_chat(self, mock_openvino_pipeline, mock_tokenizer):
        """Test streaming OpenVINO GenAI chat function"""
        
        def streaming_openvino_chat(message, history):
            # Simulate streaming response
            response = f"Streaming response to: {message}"
            for i in range(len(response)):
                yield response[:i + 1]
        
        # Test streaming
        generator = streaming_openvino_chat("Hello", [])
        chunks = list(generator)
        
        # Verify streaming behavior
        assert len(chunks) > 1
        assert chunks[-1] == "Streaming response to: Hello"
        assert all(chunks[i].startswith(chunks[i-1]) for i in range(1, len(chunks)))
    
    def test_error_handling_openvino_chat(self, mock_openvino_pipeline, mock_tokenizer):
        """Test error handling in OpenVINO GenAI chat"""
        
        # Configure mock to raise exception
        mock_openvino_pipeline.generate.side_effect = Exception("NPU compilation failed")
        
        def robust_openvino_chat(message, history):
            try:
                conversation = history + [{"role": "user", "content": message}]
                prompt = mock_tokenizer.apply_chat_template(conversation, tokenize=False)
                response = mock_openvino_pipeline.generate(prompt)
                return response
            except Exception as e:
                return f"❌ Error: {str(e)}"
        
        # Test error handling
        result = robust_openvino_chat("Hello", [])
        assert result.startswith("❌ Error:")
        assert "NPU compilation failed" in result
    
    def test_token_limit_handling(self, mock_tokenizer):
        """Test token limit handling for NPU constraints"""
        
        def token_aware_chat(message, history, max_tokens=1024):
            # Simulate token counting
            conversation = history + [{"role": "user", "content": message}]
            mock_tokenizer.encode.return_value = list(range(max_tokens + 100))  # Over limit
            
            token_count = len(mock_tokenizer.encode("dummy"))
            
            if token_count > max_tokens:
                return f"⚠️ Token limit exceeded: {token_count}/{max_tokens}"
            
            return "Response within token limit"
        
        # Test token limit enforcement
        result = token_aware_chat("Hello", [])
        assert result.startswith("⚠️ Token limit exceeded:")

# =======================================
# PATTERN 3: Performance Testing
# =======================================
# Performance and concurrency test patterns

class TestPerformancePatterns:
    """Performance testing patterns for chat interfaces"""
    
    def test_response_time_measurement(self):
        """Test response time measurement"""
        
        def timed_chat_function(message, history):
            # Simulate processing time
            time.sleep(0.5)
            return f"Response to: {message}"
        
        start_time = time.time()
        result = timed_chat_function("Hello", [])
        end_time = time.time()
        
        response_time = end_time - start_time
        
        assert result == "Response to: Hello"
        assert 0.4 < response_time < 0.6  # Allow for small timing variations
    
    def test_concurrent_requests(self):
        """Test concurrent request handling"""
        import threading
        
        results = []
        errors = []
        
        def concurrent_chat_function(message, history):
            # Simulate concurrent processing
            time.sleep(0.1)
            return f"Response to: {message}"
        
        def make_request(message_id):
            try:
                result = concurrent_chat_function(f"Message {message_id}", [])
                results.append(result)
            except Exception as e:
                errors.append(e)
        
        # Create multiple threads
        threads = []
        for i in range(5):
            thread = threading.Thread(target=make_request, args=(i,))
            threads.append(thread)
        
        # Start all threads
        for thread in threads:
            thread.start()
        
        # Wait for completion
        for thread in threads:
            thread.join()
        
        # Verify results
        assert len(results) == 5
        assert len(errors) == 0
        assert all("Response to: Message" in result for result in results)
    
    def test_memory_usage_streaming(self):
        """Test memory usage in streaming scenarios"""
        
        def memory_efficient_streaming(message, history):
            # Simulate large response generation
            response = "Word " * 1000  # Large response
            
            # Stream in chunks to test memory efficiency
            words = response.split()
            current = ""
            
            for word in words:
                current += word + " "
                yield current.strip()
                
                # In real implementation, you'd measure memory here
                # For testing, we just verify the streaming works
        
        generator = memory_efficient_streaming("Hello", [])
        chunks = []
        
        for chunk in generator:
            chunks.append(chunk)
            # Simulate memory constraint - don't keep all chunks
            if len(chunks) > 10:
                chunks = chunks[-5:]  # Keep only recent chunks
        
        # Verify streaming completed
        assert len(chunks) > 0
        assert chunks[-1].endswith("Word")

# =======================================
# PATTERN 4: Integration Testing
# =======================================
# End-to-end integration test patterns

class TestIntegrationPatterns:
    """Integration testing patterns for complete chat systems"""
    
    @pytest.fixture
    def sample_chat_interface(self):
        """Create sample chat interface for testing"""
        
        def sample_chat_function(message, history):
            return f"Echo: {message}"
        
        return gr.ChatInterface(sample_chat_function, type="messages")
    
    def test_chat_interface_creation(self, sample_chat_interface):
        """Test chat interface creation"""
        assert sample_chat_interface is not None
        assert hasattr(sample_chat_interface, 'chatbot')
        assert hasattr(sample_chat_interface, 'textbox')
    
    def test_system_prompt_integration(self):
        """Test system prompt integration"""
        
        def system_prompt_chat(message, history, system_prompt, temperature):
            # Simulate system prompt usage
            full_prompt = f"System: {system_prompt}\nUser: {message}\nAssistant:"
            return f"[T={temperature}] {full_prompt}"
        
        chat_interface = gr.ChatInterface(
            system_prompt_chat,
            type="messages",
            additional_inputs=[
                gr.Textbox("You are helpful", label="System Prompt"),
                gr.Slider(0.1, 2.0, 0.7, label="Temperature")
            ]
        )
        
        assert chat_interface is not None
        # In a real test, you would simulate user interaction
    
    def test_error_recovery_integration(self):
        """Test error recovery in integrated system"""
        
        error_count = 0
        
        def unreliable_chat_function(message, history):
            nonlocal error_count
            error_count += 1
            
            if error_count <= 2:  # Fail first 2 times
                raise Exception("Simulated failure")
            
            return f"Success after {error_count} attempts: {message}"
        
        def resilient_wrapper(message, history, max_retries=3):
            for attempt in range(max_retries):
                try:
                    return unreliable_chat_function(message, history)
                except Exception as e:
                    if attempt == max_retries - 1:
                        return f"Failed after {max_retries} attempts: {str(e)}"
                    continue
        
        # Test resilience
        result = resilient_wrapper("Hello", [])
        assert result.startswith("Success after")

# =======================================
# PATTERN 5: Async Testing Patterns
# =======================================
# Async testing patterns for advanced scenarios

class TestAsyncPatterns:
    """Async testing patterns for chat interfaces"""
    
    @pytest.mark.asyncio
    async def test_async_chat_function(self):
        """Test async chat function"""
        
        async def async_chat(message, history):
            await asyncio.sleep(0.1)  # Simulate async processing
            return f"Async response: {message}"
        
        result = await async_chat("Hello", [])
        assert result == "Async response: Hello"
    
    @pytest.mark.asyncio
    async def test_async_streaming_chat(self):
        """Test async streaming chat function"""
        
        async def async_streaming_chat(message, history):
            response = f"Async streaming: {message}"
            for i in range(len(response)):
                await asyncio.sleep(0.01)  # Simulate async processing
                yield response[:i + 1]
        
        chunks = []
        async for chunk in async_streaming_chat("Hello", []):
            chunks.append(chunk)
        
        assert len(chunks) > 1
        assert chunks[-1] == "Async streaming: Hello"
    
    @pytest.mark.asyncio
    async def test_concurrent_async_requests(self):
        """Test concurrent async requests"""
        
        async def async_chat_with_delay(message, history):
            await asyncio.sleep(0.1)
            return f"Async: {message}"
        
        # Create multiple concurrent requests
        tasks = []
        for i in range(5):
            task = async_chat_with_delay(f"Message {i}", [])
            tasks.append(task)
        
        # Wait for all to complete
        results = await asyncio.gather(*tasks)
        
        assert len(results) == 5
        assert all("Async: Message" in result for result in results)

# =====================================
# USAGE GUIDELINES FOR OPENVINO GENAI
# =====================================

"""
OPENVINO GENAI TESTING GUIDELINES:
=================================

1. BASIC FUNCTION TESTING:
   ```python
   def test_openvino_chat_basic():
       def openvino_chat(message, history):
           # Your OpenVINO GenAI implementation
           pipe = ov_genai.LLMPipeline(model_path, "NPU")
           return pipe.generate(message)
       
       result = openvino_chat("Hello", [])
       assert result is not None
       assert len(result) > 0
   ```

2. STREAMING TESTING:
   ```python
   def test_openvino_streaming():
       def streaming_chat(message, history):
           streamer = YourStreamer(tokenizer)
           pipe.generate(message, config, streamer)
           for chunk in streamer:
               yield chunk
       
       chunks = list(streaming_chat("Hello", []))
       assert len(chunks) > 0
   ```

3. ERROR HANDLING TESTING:
   ```python
   def test_npu_compilation_error():
       with pytest.raises(ov_genai.CompilationError):
           pipe = ov_genai.LLMPipeline(invalid_model_path, "NPU")
   ```

4. PERFORMANCE TESTING:
   ```python
   def test_response_time():
       start_time = time.time()
       result = openvino_chat("Hello", [])
       response_time = time.time() - start_time
       
       assert response_time < 5.0  # Max 5 seconds
   ```

5. CONCURRENCY TESTING:
   ```python
   def test_npu_concurrency():
       # Test that NPU handles concurrent requests properly
       with ThreadPoolExecutor(max_workers=2) as executor:
           futures = [executor.submit(openvino_chat, f"Message {i}", []) 
                     for i in range(3)]
           results = [f.result() for f in futures]
       
       assert len(results) == 3
   ```

RECOMMENDED TEST STRUCTURE:
=========================

1. Unit tests for individual functions
2. Integration tests for complete workflows  
3. Performance tests for response times
4. Concurrency tests for resource management
5. Error handling tests for robustness

CONTINUOUS INTEGRATION:
=====================

```python
# conftest.py
@pytest.fixture
def openvino_pipeline():
    return ov_genai.LLMPipeline(test_model_path, "CPU")  # Use CPU for CI

# test_openvino_chat.py  
def test_basic_generation(openvino_pipeline):
    result = openvino_pipeline.generate("Hello")
    assert result is not None
```

Run tests with: pytest test_openvino_chat.py -v
"""

# Example usage:
if __name__ == "__main__":
    # Run specific test patterns
    pytest.main([__file__, "-v"])

================================================================================

FILE: context\gradio_testing\streaming_tests.py
----------------------------------------
# Gradio Streaming Testing Patterns
# =================================
#
# PRIORITY: ⭐⭐⭐ (Important for streaming reliability)
#
# This file contains comprehensive testing patterns for streaming functionality
# in Gradio applications. Essential for ensuring reliable streaming behavior
# in OpenVINO GenAI applications.
#
# Key Learning Points:
# - Streaming response validation
# - Performance testing for streaming
# - Error handling in streaming scenarios
# - Memory usage testing
# - Concurrency testing for streaming

import pytest
import time
import asyncio
import threading
import queue
from unittest.mock import MagicMock, patch
from typing import Iterator, AsyncIterator
import gradio as gr

# =======================================
# PATTERN 1: Basic Streaming Tests
# =======================================

class TestBasicStreaming:
    """Basic streaming functionality tests"""
    
    def test_simple_streaming_function(self):
        """Test basic streaming function"""
        
        def simple_stream(message, history):
            response = f"Response to: {message}"
            for char in response:
                yield char
        
        # Collect streaming output
        chunks = list(simple_stream("Hello", []))
        
        # Verify streaming behavior
        assert len(chunks) > 1
        assert "".join(chunks) == "Response to: Hello"
        
        # Verify incremental build-up
        full_text = ""
        for chunk in chunks:
            full_text += chunk
            assert full_text.endswith(chunk)
    
    def test_word_level_streaming(self):
        """Test word-level streaming (more realistic for LLMs)"""
        
        def word_stream(message, history):
            response = f"This is a streaming response to {message}"
            words = response.split()
            
            current_text = ""
            for i, word in enumerate(words):
                if i == 0:
                    current_text = word
                else:
                    current_text += " " + word
                yield current_text
        
        chunks = list(word_stream("test", []))
        
        # Verify progressive building
        assert chunks[0] == "This"
        assert chunks[1] == "This is"
        assert chunks[-1] == "This is a streaming response to test"
        
        # Verify each chunk builds on previous
        for i in range(1, len(chunks)):
            assert chunks[i].startswith(chunks[i-1])
    
    def test_token_level_streaming_simulation(self):
        """Test token-level streaming (simulates OpenVINO GenAI behavior)"""
        
        def token_stream(message, history):
            # Simulate tokenizer behavior
            tokens = ["Hello", " there", ",", " how", " are", " you", "?"]
            
            accumulated = ""
            for token in tokens:
                accumulated += token
                yield accumulated
                time.sleep(0.05)  # Simulate generation delay
        
        start_time = time.time()
        chunks = list(token_stream("test", []))
        end_time = time.time()
        
        # Verify content
        assert chunks[-1] == "Hello there, how are you?"
        
        # Verify timing (should take at least 0.3 seconds for 7 tokens)
        assert end_time - start_time >= 0.3
        
        # Verify incremental nature
        assert all(chunks[i+1].startswith(chunks[i]) for i in range(len(chunks)-1))

# =======================================
# PATTERN 2: OpenVINO Streaming Tests
# =======================================

class TestOpenVINOStreaming:
    """OpenVINO GenAI specific streaming tests"""
    
    @pytest.fixture
    def mock_openvino_streamer(self):
        """Mock OpenVINO GenAI streamer"""
        
        class MockOpenVINOStreamer:
            def __init__(self, tokenizer):
                self.tokenizer = tokenizer
                self.tokens = []
                self.queue = queue.Queue()
            
            def put(self, token_id):
                self.tokens.append(token_id)
                # Simulate token decoding
                decoded = f"token_{token_id}"
                self.queue.put(decoded)
                return False  # Continue generation
            
            def end(self):
                self.queue.put(None)
            
            def __iter__(self):
                return self
            
            def __next__(self):
                item = self.queue.get()
                if item is None:
                    raise StopIteration
                return item
        
        return MockOpenVINOStreamer
    
    def test_openvino_streamer_integration(self, mock_openvino_streamer):
        """Test OpenVINO GenAI streamer integration"""
        
        def openvino_streaming_chat(message, history):
            # Simulate OpenVINO GenAI streaming
            mock_tokenizer = MagicMock()
            streamer = mock_openvino_streamer(mock_tokenizer)
            
            # Simulate token generation
            for token_id in range(5):
                streamer.put(token_id)
                time.sleep(0.1)
            
            streamer.end()
            
            # Stream tokens to UI
            accumulated = ""
            for token_text in streamer:
                accumulated += token_text + " "
                yield accumulated.strip()
        
        chunks = list(openvino_streaming_chat("Hello", []))
        
        # Verify streaming behavior
        assert len(chunks) == 5
        assert chunks[0] == "token_0"
        assert chunks[-1] == "token_0 token_1 token_2 token_3 token_4"
    
    def test_streaming_with_special_tokens(self):
        """Test streaming with special token handling"""
        
        def streaming_with_special_tokens(message, history):
            # Simulate tokens including special ones
            raw_tokens = ["<|im_start|>", "Hello", " world", "<|im_end|>", "!"]
            special_tokens = {"<|im_start|>", "<|im_end|>"}
            
            accumulated = ""
            for token in raw_tokens:
                if token not in special_tokens:
                    accumulated += token
                    yield accumulated
                else:
                    # Special tokens don't appear in output but might affect processing
                    pass
        
        chunks = list(streaming_with_special_tokens("test", []))
        
        # Verify special tokens are filtered out
        assert len(chunks) == 3  # Only non-special tokens
        assert chunks[-1] == "Hello world!"
        assert "<|im_start|>" not in chunks[-1]
        assert "<|im_end|>" not in chunks[-1]
    
    def test_streaming_error_recovery(self):
        """Test error recovery in streaming scenarios"""
        
        def streaming_with_errors(message, history):
            tokens = ["Hello", " world", "ERROR", " recovered", "!"]
            
            accumulated = ""
            for i, token in enumerate(tokens):
                if token == "ERROR":
                    # Simulate error during streaming
                    yield accumulated + " [Error occurred, recovering...]"
                    continue
                
                accumulated += token
                yield accumulated
        
        chunks = list(streaming_with_errors("test", []))
        
        # Verify error handling
        assert any("Error occurred" in chunk for chunk in chunks)
        assert chunks[-1] == "Hello world recovered!"

# =======================================
# PATTERN 3: Performance Testing
# =======================================

class TestStreamingPerformance:
    """Performance testing for streaming functionality"""
    
    def test_streaming_latency(self):
        """Test streaming response latency"""
        
        def measured_streaming(message, history):
            response = "This is a test response for latency measurement"
            words = response.split()
            
            for i, word in enumerate(words):
                start_time = time.time()
                
                if i == 0:
                    current = word
                else:
                    current = " ".join(words[:i+1])
                
                yield current
                
                # Measure time to yield
                yield_time = time.time() - start_time
                assert yield_time < 0.1, f"Yield took too long: {yield_time}s"
        
        chunks = list(measured_streaming("test", []))
        assert len(chunks) > 1
    
    def test_streaming_throughput(self):
        """Test streaming throughput (tokens per second)"""
        
        def throughput_streaming(message, history):
            # Generate many tokens
            tokens = [f"token_{i}" for i in range(100)]
            
            start_time = time.time()
            accumulated = ""
            
            for token in tokens:
                accumulated += token + " "
                yield accumulated.strip()
                time.sleep(0.01)  # Simulate generation time
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # Calculate throughput
            throughput = len(tokens) / total_time
            yield f"{accumulated.strip()} [Throughput: {throughput:.1f} tokens/sec]"
        
        chunks = list(throughput_streaming("test", []))
        
        # Verify throughput information is included
        assert "Throughput:" in chunks[-1]
        
        # Extract and verify reasonable throughput
        throughput_str = chunks[-1].split("Throughput: ")[1].split(" ")[0]
        throughput = float(throughput_str)
        assert 50 <= throughput <= 150, f"Unexpected throughput: {throughput}"
    
    def test_memory_usage_streaming(self):
        """Test memory usage during streaming"""
        
        def memory_aware_streaming(message, history):
            # Generate large response
            large_response = "word " * 10000  # 10k words
            words = large_response.split()
            
            # Stream in chunks to avoid memory buildup
            chunk_size = 100
            for i in range(0, len(words), chunk_size):
                chunk_words = words[i:i + chunk_size]
                chunk_text = " ".join(chunk_words)
                
                # Only yield the chunk, not accumulated text
                yield chunk_text
                
                # Simulate processing and cleanup
                del chunk_words
                del chunk_text
        
        chunks = list(memory_aware_streaming("test", []))
        
        # Verify chunked streaming
        assert len(chunks) == 100  # 10000 words / 100 words per chunk
        assert all(len(chunk.split()) <= 100 for chunk in chunks)

# =======================================
# PATTERN 4: Concurrency Testing
# =======================================

class TestStreamingConcurrency:
    """Concurrency testing for streaming functionality"""
    
    def test_concurrent_streaming_requests(self):
        """Test multiple concurrent streaming requests"""
        
        def concurrent_stream(message, history, user_id):
            response = f"User {user_id}: {message}"
            for char in response:
                yield char
                time.sleep(0.01)
        
        results = {}
        threads = []
        
        def stream_for_user(user_id):
            chunks = list(concurrent_stream("Hello", [], user_id))
            results[user_id] = "".join(chunks)
        
        # Start multiple concurrent streams
        for user_id in range(5):
            thread = threading.Thread(target=stream_for_user, args=(user_id,))
            threads.append(thread)
            thread.start()
        
        # Wait for all to complete
        for thread in threads:
            thread.join()
        
        # Verify all streams completed correctly
        assert len(results) == 5
        for user_id in range(5):
            expected = f"User {user_id}: Hello"
            assert results[user_id] == expected
    
    def test_streaming_queue_management(self):
        """Test queue management for streaming requests"""
        
        class StreamingQueueManager:
            def __init__(self, max_concurrent=2):
                self.max_concurrent = max_concurrent
                self.active_streams = 0
                self.queue_lock = threading.Lock()
            
            def can_start_stream(self):
                with self.queue_lock:
                    return self.active_streams < self.max_concurrent
            
            def start_stream(self):
                with self.queue_lock:
                    if self.active_streams < self.max_concurrent:
                        self.active_streams += 1
                        return True
                    return False
            
            def end_stream(self):
                with self.queue_lock:
                    self.active_streams = max(0, self.active_streams - 1)
        
        manager = StreamingQueueManager(max_concurrent=2)
        
        def managed_stream(message, history, manager):
            if not manager.start_stream():
                yield "❌ Server busy, please try again"
                return
            
            try:
                response = f"Managed response: {message}"
                for char in response:
                    yield char
                    time.sleep(0.02)
            finally:
                manager.end_stream()
        
        # Test queue management
        assert manager.can_start_stream()
        
        # Start maximum concurrent streams
        for _ in range(2):
            assert manager.start_stream()
        
        # Should reject additional streams
        assert not manager.can_start_stream()
        assert not manager.start_stream()
        
        # Clean up
        manager.end_stream()
        manager.end_stream()

# =======================================
# PATTERN 5: Error Handling Tests
# =======================================

class TestStreamingErrorHandling:
    """Error handling in streaming scenarios"""
    
    def test_streaming_interruption(self):
        """Test handling of interrupted streaming"""
        
        def interruptible_stream(message, history):
            response = "This is a long response that might be interrupted"
            words = response.split()
            
            for i, word in enumerate(words):
                if i == 5:  # Simulate interruption
                    raise KeyboardInterrupt("User interrupted")
                
                current = " ".join(words[:i+1])
                yield current
        
        chunks = []
        try:
            for chunk in interruptible_stream("test", []):
                chunks.append(chunk)
        except KeyboardInterrupt:
            # Handle interruption gracefully
            chunks.append(" [Interrupted]")
        
        # Verify partial response was captured
        assert len(chunks) > 1
        assert chunks[-1].endswith("[Interrupted]")
    
    def test_streaming_with_generation_errors(self):
        """Test streaming with generation errors"""
        
        def error_prone_stream(message, history):
            tokens = ["Hello", "world", "ERROR_TOKEN", "recovered", "response"]
            
            accumulated = ""
            for token in tokens:
                if token == "ERROR_TOKEN":
                    # Simulate generation error
                    yield accumulated + " ❌ [Generation error, retrying...]"
                    time.sleep(0.1)  # Simulate retry delay
                    continue
                
                accumulated += " " + token if accumulated else token
                yield accumulated
        
        chunks = list(error_prone_stream("test", []))
        
        # Verify error was handled
        error_chunk = next((chunk for chunk in chunks if "Generation error" in chunk), None)
        assert error_chunk is not None
        
        # Verify recovery
        assert chunks[-1] == "Hello world recovered response"
        assert "ERROR_TOKEN" not in chunks[-1]
    
    def test_streaming_timeout_handling(self):
        """Test timeout handling in streaming"""
        
        def timeout_stream(message, history, timeout=1.0):
            start_time = time.time()
            response = "This is a response that processes slowly"
            words = response.split()
            
            accumulated = ""
            for word in words:
                if time.time() - start_time > timeout:
                    yield accumulated + " [Timeout - response truncated]"
                    return
                
                accumulated += " " + word if accumulated else word
                yield accumulated
                time.sleep(0.2)  # Slow processing
        
        chunks = list(timeout_stream("test", []))
        
        # Verify timeout handling
        assert any("Timeout" in chunk for chunk in chunks)
        assert not chunks[-1].endswith("slowly")  # Should be truncated

# =====================================
# USAGE GUIDELINES FOR OPENVINO GENAI
# =====================================

"""
OPENVINO GENAI STREAMING TESTING GUIDELINES:
===========================================

1. BASIC STREAMING VALIDATION:
   ```python
   def test_openvino_streaming():
       def openvino_stream(message, history):
           streamer = YourOpenVINOStreamer(tokenizer)
           pipe.generate(message, config, streamer)
           
           for chunk in streamer:
               yield chunk
       
       chunks = list(openvino_stream("Hello", []))
       assert len(chunks) > 0
       assert all(isinstance(chunk, str) for chunk in chunks)
   ```

2. PERFORMANCE VALIDATION:
   ```python
   def test_streaming_performance():
       start_time = time.time()
       chunks = list(openvino_stream("test message", []))
       total_time = time.time() - start_time
       
       tokens_per_second = len(chunks) / total_time
       assert tokens_per_second >= 10  # Minimum acceptable rate
   ```

3. ERROR HANDLING VALIDATION:
   ```python
   def test_npu_compilation_streaming_error():
       def streaming_with_fallback(message, history):
           try:
               # Try NPU streaming
               return openvino_npu_stream(message, history)
           except CompilationError:
               # Fallback to CPU streaming
               return openvino_cpu_stream(message, history)
       
       chunks = list(streaming_with_fallback("test", []))
       assert len(chunks) > 0
   ```

4. CONCURRENCY VALIDATION:
   ```python
   def test_concurrent_streaming():
       with ThreadPoolExecutor(max_workers=2) as executor:
           futures = []
           for i in range(3):  # More requests than workers
               future = executor.submit(
                   lambda: list(openvino_stream(f"Message {i}", []))
               )
               futures.append(future)
           
           results = [f.result() for f in futures]
           assert all(len(result) > 0 for result in results)
   ```

5. MEMORY USAGE VALIDATION:
   ```python
   def test_streaming_memory_usage():
       import psutil
       process = psutil.Process()
       
       initial_memory = process.memory_info().rss
       
       # Run streaming test
       chunks = list(openvino_stream("long message" * 100, []))
       
       final_memory = process.memory_info().rss
       memory_increase = final_memory - initial_memory
       
       # Memory increase should be reasonable
       assert memory_increase < 100 * 1024 * 1024  # Less than 100MB
   ```

CONTINUOUS INTEGRATION SETUP:
============================

```yaml
# .github/workflows/streaming_tests.yml
name: Streaming Tests
on: [push, pull_request]

jobs:
  test-streaming:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: |
          pip install pytest gradio openvino-genai
      - name: Run streaming tests
        run: |
          pytest context/gradio_testing/streaming_tests.py -v
```

RECOMMENDED TEST SUITE STRUCTURE:
===============================

1. Basic functionality tests (smoke tests)
2. Performance benchmark tests  
3. Error handling and recovery tests
4. Concurrency and resource management tests
5. Memory usage and leak detection tests

These patterns ensure robust streaming functionality in production
OpenVINO GenAI applications.
"""

# Example usage:
if __name__ == "__main__":
    # Run streaming tests
    pytest.main([__file__, "-v", "--tb=short"])

================================================================================

FILE: context\python_samples\benchmark_genai.py
----------------------------------------
# Copyright (C) 2023-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

import sys
import argparse
import openvino_genai as ov_genai
from openvino import get_version

def main():
    parser = argparse.ArgumentParser(description="Help command")
    parser.add_argument("-m", "--model", type=str, required=True, help="Path to model and tokenizers base directory")
    parser.add_argument("-p", "--prompt", type=str, default=None, help="Prompt")
    parser.add_argument("-pf", "--prompt_file", type=str, help="Read prompt from file")
    parser.add_argument("-nw", "--num_warmup", type=int, default=1, help="Number of warmup iterations")
    parser.add_argument("-n", "--num_iter", type=int, default=2, help="Number of iterations")
    parser.add_argument("-mt", "--max_new_tokens", type=int, default=20, help="Maximal number of new tokens")
    parser.add_argument("-d", "--device", type=str, default="CPU", help="Device")
    
    args = parser.parse_args()

    if args.prompt is not None and args.prompt_file is not None:
        raise RuntimeError(f'Prompt and prompt file should not exist together!')
    else:
        if args.prompt_file is not None:
            with open(args.prompt_file, 'r', encoding='utf-8') as f:
                prompt = [f.read()]
        else:
            prompt = ['The Sky is blue because'] if args.prompt is None else [args.prompt]
    if len(prompt) == 0:
        raise RuntimeError(f'Prompt is empty!')

    print(f'openvino runtime version: {get_version()}')

    # Perf metrics is stored in DecodedResults. 
    # In order to get DecodedResults instead of a string input should be a list.
    models_path = args.model
    device = args.device
    num_warmup = args.num_warmup
    num_iter = args.num_iter
    
    config = ov_genai.GenerationConfig()
    config.max_new_tokens = args.max_new_tokens

    scheduler_config = ov_genai.SchedulerConfig()
    scheduler_config.enable_prefix_caching = False
    scheduler_config.max_num_batched_tokens = sys.maxsize

    pipe = ov_genai.LLMPipeline(models_path, device, scheduler_config=scheduler_config)
    
    input_data = pipe.get_tokenizer().encode(prompt)
    prompt_token_size = input_data.input_ids.get_shape()[1]
    print(f"Prompt token size: {prompt_token_size}")

    for _ in range(num_warmup):
        pipe.generate(prompt, config)
    
    res = pipe.generate(prompt, config)
    perf_metrics = res.perf_metrics
    for _ in range(num_iter - 1):
        res = pipe.generate(prompt, config)
        perf_metrics += res.perf_metrics
    
    print(f"Output token size: {res.perf_metrics.get_num_generated_tokens()}")
    print(f"Load time: {perf_metrics.get_load_time():.2f} ms")
    print(f"Generate time: {perf_metrics.get_generate_duration().mean:.2f} ± {perf_metrics.get_generate_duration().std:.2f} ms")
    print(f"Tokenization time: {perf_metrics.get_tokenization_duration().mean:.2f} ± {perf_metrics.get_tokenization_duration().std:.2f} ms")
    print(f"Detokenization time: {perf_metrics.get_detokenization_duration().mean:.2f} ± {perf_metrics.get_detokenization_duration().std:.2f} ms")
    print(f"TTFT: {perf_metrics.get_ttft().mean:.2f} ± {perf_metrics.get_ttft().std:.2f} ms")
    print(f"TPOT: {perf_metrics.get_tpot().mean:.2f} ± {perf_metrics.get_tpot().std:.2f} ms")
    print(f"Throughput : {perf_metrics.get_throughput().mean:.2f} ± {perf_metrics.get_throughput().std:.2f} tokens/s")

if __name__ == "__main__":
    main()


================================================================================

FILE: context\python_samples\chat_sample.py
----------------------------------------
#!/usr/bin/env python3
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

import argparse
import openvino_genai


def streamer(subword):
    print(subword, end='', flush=True)
    # Return flag corresponds whether generation should be stopped.
    return openvino_genai.StreamingStatus.RUNNING

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('model_dir', help='Path to the model directory')
    parser.add_argument('device', nargs='?', default='CPU', help='Device to run the model on (default: CPU)')
    args = parser.parse_args()

    device = args.device
    pipe = openvino_genai.LLMPipeline(args.model_dir, device)

    config = openvino_genai.GenerationConfig()
    config.max_new_tokens = 100

    pipe.start_chat()
    while True:
        try:
            prompt = input('question:\n')
        except EOFError:
            break
        pipe.generate(prompt, config, streamer)
        print('\n----------')
    pipe.finish_chat()


if '__main__' == __name__:
    main()


================================================================================

FILE: context\python_samples\greedy_causal_lm.py
----------------------------------------
#!/usr/bin/env python3
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

import argparse
import openvino_genai


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('model_dir')
    parser.add_argument('prompt')
    args = parser.parse_args()

    device = 'CPU'  # GPU can be used as well
    pipe = openvino_genai.LLMPipeline(args.model_dir, device)

    config = openvino_genai.GenerationConfig()
    config.max_new_tokens = 100

    print(pipe.generate(args.prompt, config))


if '__main__' == __name__:
    main()


================================================================================

FILE: context\python_samples\multinomial_causal_lm.py
----------------------------------------
#!/usr/bin/env python3
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

import argparse
import openvino_genai
import queue
import threading
from typing import Union


class IterableStreamer(openvino_genai.StreamerBase):
    """
    A custom streamer class for handling token streaming and detokenization with buffering.

    Attributes:
        tokenizer (Tokenizer): The tokenizer used for encoding and decoding tokens.
        tokens_cache (list): A buffer to accumulate tokens for detokenization.
        text_queue (Queue): A synchronized queue for storing decoded text chunks.
        print_len (int): The length of the printed text to manage incremental decoding.
    """

    def __init__(self, tokenizer):
        """
        Initializes the IterableStreamer with the given tokenizer.

        Args:
            tokenizer (Tokenizer): The tokenizer to use for encoding and decoding tokens.
        """
        super().__init__()
        self.tokenizer = tokenizer
        self.tokens_cache = []
        self.text_queue = queue.Queue()
        self.print_len = 0
        self.decoded_lengths = []

    def __iter__(self):
        """
        Returns the iterator object itself.
        """
        return self

    def __next__(self):
        """
        Returns the next value from the text queue.

        Returns:
            str: The next decoded text chunk.

        Raises:
            StopIteration: If there are no more elements in the queue.
        """
        # get() will be blocked until a token is available.
        value = self.text_queue.get()
        if value is None:
            raise StopIteration
        return value

    def get_stop_flag(self):
        """
        Checks whether the generation process should be stopped or cancelled.

        Returns:
            openvino_genai.StreamingStatus: Always returns RUNNING in this implementation.
        """
        return openvino_genai.StreamingStatus.RUNNING

    def write_word(self, word: str):
        """
        Puts a word into the text queue.

        Args:
            word (str): The word to put into the queue.
        """
        self.text_queue.put(word)

    def write(self, token: Union[int, list[int]]) -> openvino_genai.StreamingStatus:
        """
        Processes a token and manages the decoding buffer. Adds decoded text to the queue.

        Args:
            token (Union[int, list[int]]): The token(s) to process.

        Returns:
            bool: True if generation should be stopped, False otherwise.
        """
        if type(token) is list:
            self.tokens_cache += token
            self.decoded_lengths += [-2 for _ in range(len(token) - 1)]
        else:
            self.tokens_cache.append(token)

        text = self.tokenizer.decode(self.tokens_cache)
        self.decoded_lengths.append(len(text))

        word = ""
        delay_n_tokens = 3
        if len(text) > self.print_len and "\n" == text[-1]:
            # Flush the cache after the new line symbol.
            word = text[self.print_len :]
            self.tokens_cache = []
            self.decoded_lengths = []
            self.print_len = 0
        elif len(text) > 0 and text[-1] == chr(65533):
            # Don't print incomplete text.
            self.decoded_lengths[-1] = -1
        elif len(self.tokens_cache) >= delay_n_tokens:
            self.compute_decoded_length_for_position(
                len(self.decoded_lengths) - delay_n_tokens
            )
            print_until = self.decoded_lengths[-delay_n_tokens]
            if print_until != -1 and print_until > self.print_len:
                # It is possible to have a shorter text after adding new token.
                # Print to output only if text length is increased and text is complete (print_until != -1).
                word = text[self.print_len : print_until]
                self.print_len = print_until
        self.write_word(word)

        stop_flag = self.get_stop_flag()
        if stop_flag != openvino_genai.StreamingStatus.RUNNING:
            # When generation is stopped from streamer then end is not called, need to call it here manually.
            self.end()

        return stop_flag

    def compute_decoded_length_for_position(self, cache_position: int):
        # decode was performed for this position, skippping
        if self.decoded_lengths[cache_position] != -2:
            return

        cache_for_position = self.tokens_cache[: cache_position + 1]
        text_for_position = self.tokenizer.decode(cache_for_position)

        if len(text_for_position) > 0 and text_for_position[-1] == chr(65533):
            # Mark text as incomplete
            self.decoded_lengths[cache_position] = -1
        else:
            self.decoded_lengths[cache_position] = len(text_for_position)

    def end(self):
        """
        Flushes residual tokens from the buffer and puts a None value in the queue to signal the end.
        """
        text = self.tokenizer.decode(self.tokens_cache)
        if len(text) > self.print_len:
            word = text[self.print_len :]
            self.write_word(word)
            self.tokens_cache = []
            self.print_len = 0
        self.text_queue.put(None)


class ChunkStreamer(IterableStreamer):

    def __init__(self, tokenizer, tokens_len):
        super().__init__(tokenizer)
        self.tokens_len = tokens_len

    def write(self, token: Union[int, list[int]]) -> openvino_genai.StreamingStatus:
        if (len(self.tokens_cache) + 1) % self.tokens_len == 0:
            return super().write(token)

        if type(token) is list:
            self.tokens_cache += token
            # -2 means no decode was done for this token position
            self.decoded_lengths += [-2 for _ in range(len(token))]
        else:
            self.tokens_cache.append(token)
            self.decoded_lengths.append(-2)

        return openvino_genai.StreamingStatus.RUNNING


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("model_dir")
    parser.add_argument("prompt")
    args = parser.parse_args()

    device = "CPU"  # GPU can be used as well
    tokens_len = 10  # chunk size
    pipe = openvino_genai.LLMPipeline(args.model_dir, device)

    text_print_streamer = ChunkStreamer(pipe.get_tokenizer(), tokens_len)

    def token_printer():
        # Getting next elements from iterable will be blocked until a new token is available.
        for word in text_print_streamer:
            print(word, end="", flush=True)

    printer_thread = threading.Thread(target=token_printer, daemon=True)
    printer_thread.start()

    config = openvino_genai.GenerationConfig()
    config.max_new_tokens = 100
    config.do_sample = True
    config.top_p = 0.9
    config.top_k = 30

    # Since the streamer is set, the results will be printed
    # every time a new token is generated and put into the streamer queue.
    pipe.generate(args.prompt, config, text_print_streamer)
    printer_thread.join()


if "__main__" == __name__:
    main()


================================================================================

FILE: context\qwen3_model_context\README.md
----------------------------------------
# Qwen3 Model-Specific Context
# ============================

**Generated**: January 8, 2025  
**Version**: 1.0.0  
**Purpose**: Model-specific knowledge for Qwen3-8B INT4 with OpenVINO GenAI

## 🎯 Overview

This directory contains comprehensive Qwen3-8B model-specific context that bridges the gap between the general OpenVINO GenAI implementation and the specific requirements, optimizations, and characteristics of the Qwen3 architecture.

---

## 📁 Directory Structure

```
qwen3_model_context/
├── model_architecture.py ⭐⭐⭐⭐⭐     # Complete architecture specs & config
├── special_tokens.py ⭐⭐⭐⭐⭐        # Token handling & chat templates  
├── npu_optimization.py ⭐⭐⭐⭐⭐      # NPU deployment & optimization
└── README.md                         # This comprehensive guide
```

---

## 🥇 Critical Knowledge Areas

### 1. **Model Architecture Specifics** (`model_architecture.py`)

**🔍 Key Discoveries:**
- **8B parameters** with Qwen3ForCausalLM architecture
- **40,960 max position embeddings** (40K context length)
- **Grouped Query Attention (GQA)**: 32 attention heads, 8 key-value heads
- **151,936 vocabulary size** with extensive special token support
- **INT4 quantization** optimized for NPU inference

**💡 Critical for Implementation:**
```python
# Qwen3-specific architecture constraints
QWEN3_CONSTRAINTS = {
    "max_context": 40960,          # But NPU limited to ~2048 practically
    "vocab_size": 151936,          # Larger than typical 32K models
    "hidden_size": 4096,           # Standard for 8B models
    "num_layers": 36,              # Deep architecture
    "rope_theta": 1000000          # Large RoPE base for long context
}
```

### 2. **Special Token Ecosystem** (`special_tokens.py`)

**🔍 Key Discoveries:**
- **26+ special tokens** including vision, tools, and reasoning tokens
- **Complex chat template** supporting multiple modes (basic, tools, thinking)
- **Multi-modal capabilities** with vision and object reference tokens
- **Fill-in-Middle (FIM)** support for code completion
- **Tool calling** with structured JSON format

**💡 Critical Token IDs:**
```python
CRITICAL_TOKENS = {
    151643: "<|endoftext|>",     # BOS/PAD/EOS token
    151644: "<|im_start|>",      # Chat message start
    151645: "<|im_end|>",        # Chat message end
    151667: "<think>",           # Reasoning start
    151668: "</think>",          # Reasoning end
    151657: "<tool_call>",       # Tool execution start
    151658: "</tool_call>"       # Tool execution end
}
```

**⚠️ Filtering Requirements:**
Many tokens should NOT appear in user-visible output and must be filtered during streaming.

### 3. **NPU Optimization Requirements** (`npu_optimization.py`)

**🔍 Key Discoveries:**
- **NPUW configuration is MANDATORY** for NPU compilation success
- **Memory constraints** require conservative prompt lengths (<2048 tokens)
- **Three-tier deployment strategy** needed for maximum compatibility
- **Performance profiles** for different use cases (conservative/balanced/aggressive)

**💡 Essential NPUW Configuration:**
```python
MANDATORY_NPUW_CONFIG = {
    "NPU_USE_NPUW": "YES",              # Enable NPU Wrapper
    "NPUW_LLM": "YES",                  # Enable LLM optimizations
    "NPUW_LLM_BATCH_DIM": 0,            # Batch dimension
    "NPUW_LLM_SEQ_LEN_DIM": 1,          # Sequence dimension
    "NPUW_LLM_MAX_PROMPT_LEN": 2048,    # Conservative limit
    "NPUW_LLM_MIN_RESPONSE_LEN": 256    # Minimum response
}
```

---

## 🚨 Critical Issues Addressed

### 1. **NPU Compilation Failures**
**Problem**: "Failed to compile Model0_FCEW000__0" errors  
**Root Cause**: Missing or incorrect NPUW configuration  
**Solution**: Use complete NPUW config with proper dimension settings

### 2. **Token Filtering in Streaming**
**Problem**: Special tokens appearing in chat UI  
**Root Cause**: No filtering of Qwen3's 26+ special tokens  
**Solution**: Implement streaming filter that removes display-inappropriate tokens

### 3. **Memory Management on NPU**
**Problem**: Out of memory errors during generation  
**Root Cause**: NPU memory constraints with 8B model  
**Solution**: Conservative prompt lengths and memory optimization settings

### 4. **Chat Template Complexity**
**Problem**: Incorrect conversation formatting  
**Root Cause**: Qwen3's complex multi-mode chat template  
**Solution**: Use proper template formatting for different modes (basic/tools/thinking)

---

## 🎯 Integration with Your Current Application

### Current Application Analysis:
Your `gradio_qwen_debug.py` already implements several optimizations:

✅ **What's Working:**
- NPU device targeting
- Basic NPUW configuration
- Streaming implementation with GradioStreamer
- Token limit management
- Performance monitoring

⚠️ **What's Missing (Critical Gaps):**
1. **Complete NPUW configuration** - You have basic settings but missing critical LLM-specific parameters
2. **Special token filtering** - No filtering of Qwen3's extensive special tokens in streaming
3. **Proper chat template** - Basic prompt concatenation vs. proper Qwen3 chat format
4. **Session management** - Missing `start_chat()/finish_chat()` patterns

### Recommended Improvements:

#### 1. **Enhance NPU Configuration**
```python
# Replace your current NPU config with:
from qwen3_model_context.npu_optimization import Qwen3NPUConfigBuilder

builder = Qwen3NPUConfigBuilder("balanced")
npu_config = builder.build_complete_config()
pipe = ov_genai.LLMPipeline(model_path, "NPU", **npu_config)
```

#### 2. **Implement Proper Token Filtering**
```python
# Enhance your GradioStreamer:
from qwen3_model_context.special_tokens import Qwen3StreamingFilter

class EnhancedGradioStreamer(ov_genai.StreamerBase):
    def __init__(self, tokenizer):
        super().__init__()
        self.tokenizer = tokenizer
        self.filter = Qwen3StreamingFilter()
        
    def put(self, token_id):
        token_text = self.tokenizer.decode([token_id])
        display_text = self.filter.process_token(token_id, token_text)
        
        if display_text:
            # Only queue non-filtered tokens
            self.text_queue.put(display_text)
        
        return False
```

#### 3. **Add Proper Chat Templates**
```python
# Replace basic prompt concatenation:
from qwen3_model_context.special_tokens import Qwen3ChatTemplate

def format_conversation(history):
    if not history:
        return ""
    
    # Use proper Qwen3 chat template
    system_msg = "You are a helpful AI assistant."
    user_msg = history[-1]["content"]
    
    return Qwen3ChatTemplate.format_basic_chat(system_msg, user_msg)
```

#### 4. **Implement Session Management**
```python
# Add proper session management:
def generate_response(message):
    pipe.start_chat()
    try:
        response = pipe.generate(message, config, streamer)
        return response
    finally:
        pipe.finish_chat()
```

---

## 📊 Performance Expectations

### NPU Performance Targets:
- **Load Time**: 60-75 seconds (first run), 20-30 seconds (cached)
- **First Token Latency**: <2 seconds
- **Generation Speed**: 15-25 tokens/second
- **Max Concurrent Users**: 1 (NPU limitation)
- **Recommended Prompt Length**: <2048 tokens

### Memory Usage:
- **NPU Memory**: Optimized for INT4 quantization
- **System RAM**: ~4GB for model + overhead
- **Cache Size**: ~2GB for .ovcache directory

---

## 🔍 Debugging and Troubleshooting

### Common Issues:

1. **"Failed to compile Model0_FCEW000__0"**
   - ✅ Solution: Ensure complete NPUW configuration
   - 🔧 Check: `NPU_USE_NPUW=YES` and `NPUW_LLM=YES`

2. **"Out of memory" during generation**
   - ✅ Solution: Reduce `NPUW_LLM_MAX_PROMPT_LEN` to 1024
   - 🔧 Check: Enable `NPU_LOW_MEMORY_MODE=YES`

3. **Special tokens in chat output**
   - ✅ Solution: Implement Qwen3StreamingFilter
   - 🔧 Check: Filter tokens 151644, 151645, 151667, 151668

4. **Slow generation speed**
   - ✅ Solution: Verify NPU drivers and cache
   - 🔧 Check: Use greedy decoding (temperature=0) for speed

### Diagnostic Tools:
```python
# Use built-in validation:
from qwen3_model_context.npu_optimization import Qwen3NPUCompilationValidator

is_valid, issues = Qwen3NPUCompilationValidator.validate_config(your_config)
if not is_valid:
    print(f"Config issues: {issues}")
```

---

## 🚀 Next Steps for Integration

### Immediate Actions (High Priority):
1. **Update NPU configuration** with complete NPUW settings
2. **Implement special token filtering** in streaming
3. **Add proper chat template formatting**
4. **Implement session management** with start_chat/finish_chat

### Medium-term Improvements:
1. **Add performance monitoring** with Qwen3NPUPerformanceMonitor
2. **Implement tool calling support** using Qwen3's tool tokens
3. **Add thinking mode support** for reasoning visibility
4. **Create multi-modal interface** using vision tokens

### Long-term Enhancements:
1. **Implement conversation memory management** within token limits
2. **Add batch processing** for multiple users (when NPU supports it)
3. **Create model switching** between NPU/CPU based on load
4. **Add fine-tuning support** for domain-specific applications

---

## 💡 Key Insights for Development

1. **Qwen3 is NOT a standard LLM** - It has extensive multi-modal and tool capabilities
2. **NPU compilation is fragile** - Requires precise NPUW configuration
3. **Token management is complex** - 26+ special tokens need careful handling
4. **Performance is memory-bound** - Conservative limits prevent crashes
5. **Chat templates matter** - Proper formatting affects model behavior significantly

This context provides the missing model-specific knowledge layer to complement your existing OpenVINO GenAI implementation and official Gradio patterns! 🎯

================================================================================

FILE: context\qwen3_model_context\model_architecture.py
----------------------------------------
# Qwen3 Model Architecture & Configuration Guide
# =============================================
#
# PRIORITY: ⭐⭐⭐⭐⭐ (Critical for Qwen3 optimization)
#
# This file documents the Qwen3-8B model architecture, configuration patterns,
# and OpenVINO GenAI optimization strategies specific to this model.
#
# Based on analysis of: C:\OpenVinoModels\qwen3-8b-int4-cw-ov\config.json

# =======================================
# MODEL ARCHITECTURE SPECIFICATIONS
# =======================================

QWEN3_8B_ARCHITECTURE = {
    "model_type": "qwen3",
    "architecture_class": "Qwen3ForCausalLM",
    "parameters": "8B",
    
    # Core Architecture
    "hidden_size": 4096,
    "intermediate_size": 12288,  # Feed-forward network size
    "num_hidden_layers": 36,
    "num_attention_heads": 32,
    "num_key_value_heads": 8,    # GQA (Grouped Query Attention)
    "head_dim": 128,             # hidden_size / num_attention_heads
    
    # Position & Context
    "max_position_embeddings": 40960,  # 40K context length
    "max_window_layers": 36,
    "rope_theta": 1000000,
    "sliding_window": None,
    "use_sliding_window": False,
    
    # Vocabulary & Tokens
    "vocab_size": 151936,
    "bos_token_id": 151643,     # <|endoftext|>
    "eos_token_id": 151645,     # <|im_end|>
    "pad_token_id": None,       # Uses bos_token for padding
    
    # Technical Details
    "attention_dropout": 0.0,
    "hidden_act": "silu",       # Swish activation
    "initializer_range": 0.02,
    "rms_norm_eps": 1e-06,
    "torch_dtype": "bfloat16",
    "tie_word_embeddings": False,
    "use_cache": True
}

# =======================================
# QWEN3 OPTIMIZATION FOR OPENVINO GENAI
# =======================================

class Qwen3OpenVINOConfig:
    """Optimized OpenVINO configuration for Qwen3-8B models"""
    
    @staticmethod
    def get_npu_config():
        """NPU-specific configuration for Qwen3"""
        return {
            # Base OpenVINO properties (removed PERFORMANCE_HINT to avoid conflicts)
            "CACHE_DIR": ".ovcache_qwen3",
            "NUM_STREAMS": 1,
            "INFERENCE_NUM_THREADS": 1,
            
            # NPUW configuration (required for NPU compilation)
            "NPU_USE_NPUW": "YES",
            "NPUW_LLM": "YES",
            "NPUW_LLM_BATCH_DIM": 0,
            "NPUW_LLM_SEQ_LEN_DIM": 1,
            
            # Qwen3-specific NPUW settings
            "NPUW_LLM_MAX_PROMPT_LEN": 2048,    # Conservative for NPU
            "NPUW_LLM_MIN_RESPONSE_LEN": 256,
            "NPUW_LLM_PREFILL_HINT": "LATENCY",
            "NPUW_LLM_GENERATE_HINT": "LATENCY",
            
            # Memory optimization for 8B model
            "CACHE_MODE": "OPTIMIZE_SPEED"
        }
    
    @staticmethod
    def get_cpu_config():
        """CPU-optimized configuration for Qwen3"""
        return {
            "PERFORMANCE_HINT": "THROUGHPUT",
            "CACHE_DIR": ".ovcache_qwen3_cpu", 
            "NUM_STREAMS": 2,
            "INFERENCE_NUM_THREADS": 0,        # Auto-detect
            
            # CPU-specific optimizations for 8B model
            "ENABLE_CPU_PINNING": True,
            "CPU_BIND_THREAD": "NUMA",
            
            # Larger prompt support on CPU
            "MAX_PROMPT_LEN": 8192,            # Use more of 40K context
            "MIN_RESPONSE_LEN": 512
        }
    
    @staticmethod 
    def get_generation_config():
        """Default generation configuration for Qwen3"""
        return {
            # From generation_config.json analysis
            "do_sample": True,
            "temperature": 0.6,
            "top_k": 20,
            "top_p": 0.95,
            
            # Token IDs (critical for proper generation)
            "bos_token_id": 151643,  # <|endoftext|>
            "eos_token_id": [151645, 151643],  # <|im_end|>, <|endoftext|>
            "pad_token_id": 151643,  # Same as bos
            
            # Response controls
            "max_new_tokens": 2048,
            "repetition_penalty": 1.1,
            "no_repeat_ngram_size": 3,
            
            # Qwen3 specific
            "do_sample": True,       # Enable sampling for creative responses
            "early_stopping": True
        }

# =======================================
# QWEN3 PIPELINE INITIALIZATION PATTERNS
# =======================================

def initialize_qwen3_pipeline(model_path, device="NPU", **kwargs):
    """
    Robust Qwen3 pipeline initialization with proper error handling
    
    Args:
        model_path: Path to Qwen3 OpenVINO model
        device: Target device ("NPU", "CPU", "GPU")
        **kwargs: Additional configuration overrides
    
    Returns:
        Initialized LLMPipeline optimized for Qwen3
    """
    import openvino_genai as ov_genai
    
    # Try modern OpenVINO imports with fallback
    try:
        import openvino.properties as props
        import openvino.properties.hint as hints
        OPENVINO_PROPERTIES_AVAILABLE = True
    except ImportError:
        print("⚠️ OpenVINO properties could not be imported. Using fallback.")
        OPENVINO_PROPERTIES_AVAILABLE = False
        # Create mock objects for compatibility
        class MockProps:
            pass
        class MockHints:
            class PerformanceMode:
                LATENCY = "LATENCY"
                THROUGHPUT = "THROUGHPUT"
        props = MockProps()
        hints = MockHints()
    
    config_provider = Qwen3OpenVINOConfig()
    
    # Get device-specific configuration
    if device == "NPU":
        base_config = config_provider.get_npu_config()
    elif device == "CPU":
        base_config = config_provider.get_cpu_config()
    elif device == "GPU":
        base_config = {
            "PERFORMANCE_HINT": "LATENCY",
            "CACHE_DIR": ".ovcache_qwen3_gpu",
            "GPU_ENABLE_LOOP_UNROLLING": False  # Better for large models
        }
    else:
        raise ValueError(f"Unsupported device: {device}")
    
    # Merge with user overrides
    final_config = {**base_config, **kwargs}
    
    # Three-tier initialization strategy for maximum compatibility
    try:
        # Tier 1: Full configuration with all optimizations
        print(f"Initializing Qwen3 on {device} with full optimization...")
        pipe = ov_genai.LLMPipeline(model_path, device, **final_config)
        print(f"✅ Qwen3 initialization successful with full config")
        return pipe
        
    except Exception as e1:
        print(f"⚠️ Full config failed: {e1}")
        
        try:
            # Tier 2: Basic device properties only
            basic_config = {
                "PERFORMANCE_HINT": "LATENCY",
                "CACHE_DIR": f".ovcache_qwen3_{device.lower()}"
            }
            pipe = ov_genai.LLMPipeline(model_path, device, **basic_config)
            print(f"✅ Qwen3 initialization successful with basic config")
            return pipe
            
        except Exception as e2:
            print(f"⚠️ Basic config failed: {e2}")
            
            # Tier 3: Minimal configuration
            pipe = ov_genai.LLMPipeline(model_path, device)
            print(f"✅ Qwen3 initialization successful with minimal config")
            return pipe

# =======================================
# QWEN3 PERFORMANCE CHARACTERISTICS
# =======================================

QWEN3_PERFORMANCE_PROFILES = {
    "NPU": {
        "typical_load_time": "60-75 seconds",
        "tokens_per_second": "15-25 tokens/sec",
        "max_concurrent_users": 1,
        "recommended_prompt_length": "< 2048 tokens",
        "memory_usage": "Low (NPU optimized)",
        
        # NPU-specific considerations
        "compilation_time": "High (first run)",
        "cache_benefits": "Significant",
        "batch_size": 1,  # NPU works best with single requests
        
        "optimization_notes": [
            "NPUW configuration is mandatory for compilation",
            "Conservative prompt lengths prevent memory issues",
            "Cache warmup improves subsequent loads significantly",
            "Greedy decoding often faster than sampling on NPU"
        ]
    },
    
    "CPU": {
        "typical_load_time": "10-20 seconds", 
        "tokens_per_second": "8-15 tokens/sec",
        "max_concurrent_users": 2,
        "recommended_prompt_length": "< 8192 tokens",
        "memory_usage": "High (8B parameters in RAM)",
        
        # CPU-specific considerations
        "compilation_time": "Low",
        "cache_benefits": "Moderate",
        "batch_size": 1,  # Sequential processing
        
        "optimization_notes": [
            "Larger context windows possible vs NPU",
            "Thread count auto-detection usually optimal", 
            "Memory usage scales with context length",
            "Good fallback when NPU unavailable"
        ]
    },
    
    "GPU": {
        "typical_load_time": "15-30 seconds",
        "tokens_per_second": "20-40 tokens/sec", 
        "max_concurrent_users": 1,
        "recommended_prompt_length": "< 4096 tokens",
        "memory_usage": "Very High (GPU VRAM required)",
        
        "optimization_notes": [
            "Requires adequate GPU VRAM (>8GB recommended)",
            "Fastest generation once loaded",
            "Good for high-throughput scenarios"
        ]
    }
}

# =======================================
# QWEN3 TROUBLESHOOTING GUIDE  
# =======================================

QWEN3_COMMON_ISSUES = {
    "compilation_failures": {
        "symptoms": ["Failed to compile Model0_FCEW000__0", "NPU compilation error"],
        "solutions": [
            "Ensure NPU_USE_NPUW=YES and NPUW_LLM=YES are set",
            "Check NPUW_LLM_MAX_PROMPT_LEN matches your pipeline config",
            "Verify model is in correct INT4 format for NPU",
            "Try reducing MAX_PROMPT_LEN to 1024 for problematic models"
        ]
    },
    
    "memory_issues": {
        "symptoms": ["Out of memory", "Allocation failed", "NPU memory exceeded"],
        "solutions": [
            "Reduce MAX_PROMPT_LEN and MIN_RESPONSE_LEN",
            "Use smaller batch sizes (batch_size=1)",
            "Enable CACHE_MODE=OPTIMIZE_SPEED",
            "Consider CPU fallback for long conversations"
        ]
    },
    
    "slow_generation": {
        "symptoms": ["Very slow token generation", "High latency"],
        "solutions": [
            "Verify NPU drivers are properly installed",
            "Check if model cache exists (.ovcache directory)",
            "Use PERFORMANCE_HINT=LATENCY",
            "Consider greedy decoding (temperature=0) for speed"
        ]
    },
    
    "tokenization_errors": {
        "symptoms": ["Special token errors", "Chat template issues"],
        "solutions": [
            "Check special token handling in your tokenizer",
            "Verify chat template is being applied correctly", 
            "Filter special tokens before display (see special_tokens.py)",
            "Ensure proper eos_token_id configuration"
        ]
    }
}

# =======================================
# USAGE EXAMPLES
# =======================================

def qwen3_basic_usage():
    """Basic Qwen3 usage example"""
    
    # Initialize pipeline
    model_path = "C:\\OpenVinoModels\\qwen3-8b-int4-cw-ov"
    pipe = initialize_qwen3_pipeline(model_path, device="NPU")
    
    # Configure generation
    config = ov_genai.GenerationConfig()
    config.max_new_tokens = 512
    config.temperature = 0.7
    config.do_sample = True
    
    # Generate response  
    pipe.start_chat()
    response = pipe.generate("Explain quantum computing", config)
    pipe.finish_chat()
    
    return response

def qwen3_streaming_usage():
    """Qwen3 streaming usage example"""
    
    class Qwen3Streamer(ov_genai.StreamerBase):
        def __init__(self, tokenizer):
            super().__init__()
            self.tokenizer = tokenizer
            self.tokens = []
        
        def put(self, token_id):
            self.tokens.append(token_id)
            # Filter Qwen3 special tokens
            if token_id not in [151644, 151645, 151646]:  # <|im_start|>, <|im_end|>, etc.
                decoded = self.tokenizer.decode([token_id])
                print(decoded, end='', flush=True)
            return False
        
        def end(self):
            print()  # New line at end
    
    # Usage
    model_path = "C:\\OpenVinoModels\\qwen3-8b-int4-cw-ov"
    pipe = initialize_qwen3_pipeline(model_path, device="NPU")
    
    # Note: tokenizer initialization would be needed
    # streamer = Qwen3Streamer(tokenizer)
    # pipe.start_chat()
    # pipe.generate("Hello!", config, streamer)
    # pipe.finish_chat()

# =======================================
# INTEGRATION WITH GRADIO
# =======================================

def create_qwen3_gradio_interface():
    """Create optimized Gradio interface for Qwen3"""
    
    import gradio as gr
    
    # Initialize Qwen3 pipeline globally
    global qwen3_pipe
    model_path = "C:\\OpenVinoModels\\qwen3-8b-int4-cw-ov"
    qwen3_pipe = initialize_qwen3_pipeline(model_path, device="NPU")
    
    def qwen3_chat_response(message, history):
        """Chat response function optimized for Qwen3"""
        
        config = ov_genai.GenerationConfig()
        config.max_new_tokens = 512
        config.temperature = 0.6  # Qwen3 default
        config.top_p = 0.95       # Qwen3 default
        config.do_sample = True
        
        # Qwen3 session management
        qwen3_pipe.start_chat()
        try:
            response = qwen3_pipe.generate(message, config)
            return response
        finally:
            qwen3_pipe.finish_chat()
    
    # Create interface with Qwen3-optimized settings
    interface = gr.ChatInterface(
        qwen3_chat_response,
        title="🎯 Qwen3-8B Chat (OpenVINO NPU)",
        description="Optimized for Intel NPU with Qwen3-specific configurations",
        examples=[
            "Explain the benefits of Intel NPU acceleration",
            "What are the key features of the Qwen3 model?",
            "How does OpenVINO optimize transformer models?"
        ],
        theme=gr.themes.Soft(),
        css=".container { max-width: 900px; margin: auto; }"
    )
    
    return interface

if __name__ == "__main__":
    # Example usage
    print("Qwen3 Model Configuration Loaded")
    print(f"Architecture: {QWEN3_8B_ARCHITECTURE['architecture_class']}")
    print(f"Parameters: {QWEN3_8B_ARCHITECTURE['parameters']}")
    print(f"Context Length: {QWEN3_8B_ARCHITECTURE['max_position_embeddings']:,}")
    print(f"Vocab Size: {QWEN3_8B_ARCHITECTURE['vocab_size']:,}")

================================================================================

FILE: context\qwen3_model_context\npu_optimization.py
----------------------------------------
# Qwen3 NPU Optimization Guide
# =============================
#
# Copyright (c) 2025 sbran
# Licensed under the MIT License
# 
# Based on OpenVINO GenAI patterns and Intel NPU optimization techniques.
# Integrates knowledge from official OpenVINO documentation and samples.
#
# PRIORITY: ⭐⭐⭐⭐⭐ (Critical for NPU deployment)
#
# This file contains comprehensive NPU optimization strategies specifically
# for Qwen3-8B model deployment with OpenVINO GenAI on Intel NPU hardware.
#
# Key Focus Areas:
# - NPUW (NPU Wrapper) configuration for successful compilation
# - Memory management for 8B parameter models
# - Performance optimization techniques
# - Troubleshooting NPU-specific issues

import os
import time
from typing import Dict, Any, Optional, Tuple, List
from dataclasses import dataclass

# =======================================
# QWEN3 NPU CONFIGURATION PROFILES
# =======================================

@dataclass
class NPUPerformanceProfile:
    """NPU performance configuration profile"""
    max_prompt_len: int
    min_response_len: int
    cache_mode: str
    performance_hint: str
    compilation_strategy: str
    memory_optimization: str

# Pre-configured profiles for different use cases
QWEN3_NPU_PROFILES = {
    "conservative": NPUPerformanceProfile(
        max_prompt_len=1024,
        min_response_len=128,
        cache_mode="OPTIMIZE_SPEED",
        performance_hint="LATENCY",
        compilation_strategy="FAST",
        memory_optimization="HIGH"
    ),
    
    "balanced": NPUPerformanceProfile(
        max_prompt_len=2048,
        min_response_len=256,
        cache_mode="OPTIMIZE_SPEED", 
        performance_hint="LATENCY",
        compilation_strategy="BALANCED",
        memory_optimization="MEDIUM"
    ),
    
    "aggressive": NPUPerformanceProfile(
        max_prompt_len=4096,
        min_response_len=512,
        cache_mode="OPTIMIZE_SPEED",
        performance_hint="THROUGHPUT",
        compilation_strategy="OPTIMAL",
        memory_optimization="LOW"
    )
}

# =======================================
# QWEN3 NPUW CONFIGURATION BUILDER
# =======================================

class Qwen3NPUConfigBuilder:
    """Build optimized NPU configuration for Qwen3 models"""
    
    def __init__(self, profile: str = "balanced"):
        if profile not in QWEN3_NPU_PROFILES:
            raise ValueError(f"Unknown profile: {profile}. Available: {list(QWEN3_NPU_PROFILES.keys())}")
        
        self.profile = QWEN3_NPU_PROFILES[profile]
        self.config = {}
    
    def build_base_config(self) -> Dict[str, Any]:
        """Build base OpenVINO configuration"""
        # The generic 'PERFORMANCE_HINT' conflicts with the more specific NPUW hints.
        # By removing it, we allow the NPUW-specific hints (e.g., NPUW_LLM_GENERATE_HINT)
        # to be correctly applied by the OpenVINO plugin.
        return {
            # OpenVINO Core Properties
            "CACHE_DIR": ".ovcache_qwen3_npu",
            "NUM_STREAMS": 1,  # NPU works best with single stream
            "INFERENCE_NUM_THREADS": 1,
            
            # NPU-specific settings
            "DEVICE_PRIORITY": "NPU",
            "NPU_COMPILATION_MODE_PARAMS": self.profile.compilation_strategy,
        }
    
    def build_npuw_config(self) -> Dict[str, Any]:
        """Build NPUW (NPU Wrapper) configuration - REQUIRED for compilation"""
        return {
            # NPUW Core Settings (MANDATORY)
            "NPU_USE_NPUW": "YES",              # Enable NPU Wrapper
            "NPUW_LLM": "YES",                  # Enable LLM-specific optimizations
            "NPUW_LLM_BATCH_DIM": 0,            # Batch dimension index
            "NPUW_LLM_SEQ_LEN_DIM": 1,          # Sequence length dimension index
            
            # Qwen3-specific NPUW settings
            "NPUW_LLM_MAX_PROMPT_LEN": self.profile.max_prompt_len,
            "NPUW_LLM_MIN_RESPONSE_LEN": self.profile.min_response_len,
            
            # Performance hints for NPUW (using actually supported values)
            "NPUW_LLM_PREFILL_HINT": "BEST_PERF",
            "NPUW_LLM_GENERATE_HINT": "BEST_PERF",
            
            # Advanced NPUW settings (removed unsupported options)
            "NPUW_WEIGHTS_BANK": "YES",         # Memory optimization
            "NPUW_FOLD_ELTWISE_UP": "YES"       # Optimization
        }
    
    def build_memory_config(self) -> Dict[str, Any]:
        """Build memory optimization configuration"""
        base_memory = {
            "CACHE_MODE": self.profile.cache_mode,
            "NPU_MEMORY_POOL_SIZE": "AUTO",
            "NPUW_CACHE_WEIGHTS": "YES",
        }
        
        # Add memory optimization based on profile
        if self.profile.memory_optimization == "HIGH":
            base_memory.update({
                "NPUW_LLM_MIN_RESPONSE_LEN": min(128, self.profile.min_response_len),
                "NPUW_COMPRESS_WEIGHTS": "YES",
                "NPU_LOW_MEMORY_MODE": "YES"
            })
        elif self.profile.memory_optimization == "MEDIUM":
            base_memory.update({
                "NPUW_COMPRESS_WEIGHTS": "AUTO",
            })
        
        return base_memory
    
    def build_complete_config(self, **overrides) -> Dict[str, Any]:
        """Build complete NPU configuration with all optimizations"""
        config = {}
        
        # Merge all configuration sections
        config.update(self.build_base_config())
        config.update(self.build_npuw_config()) 
        config.update(self.build_memory_config())
        
        # Apply user overrides
        config.update(overrides)
        
        return config

# =======================================
# QWEN3 NPU COMPILATION VALIDATOR
# =======================================

class Qwen3NPUCompilationValidator:
    """Validate NPU compilation requirements for Qwen3"""
    
    REQUIRED_NPUW_SETTINGS = [
        "NPU_USE_NPUW",
        "NPUW_LLM", 
        "NPUW_LLM_BATCH_DIM",
        "NPUW_LLM_SEQ_LEN_DIM",
        "NPUW_LLM_MAX_PROMPT_LEN",
        "NPUW_LLM_MIN_RESPONSE_LEN"
    ]
    
    @classmethod
    def validate_config(cls, config: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """
        Validate NPU configuration for compilation success
        
        Returns:
            (is_valid, missing_settings)
        """
        missing = []
        
        for setting in cls.REQUIRED_NPUW_SETTINGS:
            if setting not in config:
                missing.append(setting)
            elif config[setting] in [None, "", "NO", "False"]:
                missing.append(f"{setting} (should not be empty/NO/False)")
        
        # Validate specific values
        if "NPU_USE_NPUW" in config and config["NPU_USE_NPUW"] != "YES":
            missing.append("NPU_USE_NPUW must be 'YES'")
        
        if "NPUW_LLM" in config and config["NPUW_LLM"] != "YES":
            missing.append("NPUW_LLM must be 'YES'")
        
        # Validate prompt length constraints
        max_prompt = config.get("NPUW_LLM_MAX_PROMPT_LEN", 0)
        if isinstance(max_prompt, (int, str)) and int(max_prompt) > 4096:
            missing.append(f"NPUW_LLM_MAX_PROMPT_LEN ({max_prompt}) exceeds NPU memory limit (4096)")
        
        return len(missing) == 0, missing
    
    @classmethod
    def diagnose_compilation_failure(cls, error_message: str) -> Dict[str, Any]:
        """
        Diagnose common NPU compilation failures
        
        Returns:
            Dictionary with diagnosis and suggested fixes
        """
        diagnosis = {
            "error_type": "unknown",
            "likely_cause": "unknown",
            "suggested_fixes": []
        }
        
        error_lower = error_message.lower()

        # Add check for configuration parsing errors (most specific first)
        if "unsupported" in error_lower and ("option" in error_lower or "parse" in error_lower):
            diagnosis.update({
                "error_type": "configuration_parsing_error",
                "likely_cause": "An invalid value was provided for an NPU configuration key (e.g., NPUW_LLM_GENERATE_HINT).",
                "suggested_fixes": [
                    "Verify all NPUW hint values are supported (e.g., 'BEST_PERF' or 'FAST_COMPILE')",
                    "Ensure the generic 'PERFORMANCE_HINT' is removed when specific 'NPUW_LLM_*_HINT' keys are used",
                    "Check for typos in configuration keys",
                    "Remove conflicting generic and specific hint settings"
                ]
            })
            return diagnosis
        
        # Common error patterns
        if "failed to compile model0_fcew000__0" in error_lower:
            diagnosis.update({
                "error_type": "npuw_compilation_failure",
                "likely_cause": "Missing or incorrect NPUW configuration",
                "suggested_fixes": [
                    "Ensure NPU_USE_NPUW=YES is set",
                    "Verify NPUW_LLM=YES is configured", 
                    "Check NPUW_LLM_MAX_PROMPT_LEN matches pipeline config",
                    "Try conservative profile with smaller prompt length"
                ]
            })
        
        elif "memory" in error_lower or "allocation" in error_lower:
            diagnosis.update({
                "error_type": "memory_error",
                "likely_cause": "NPU memory constraints exceeded",
                "suggested_fixes": [
                    "Reduce NPUW_LLM_MAX_PROMPT_LEN to 1024 or lower",
                    "Enable NPU_LOW_MEMORY_MODE=YES",
                    "Use conservative profile",
                    "Consider CPU fallback for large contexts"
                ]
            })
        
        elif "device" in error_lower or "npu" in error_lower:
            diagnosis.update({
                "error_type": "device_error", 
                "likely_cause": "NPU device or driver issue",
                "suggested_fixes": [
                    "Verify NPU drivers are installed and up-to-date",
                    "Check if NPU is accessible via OpenVINO device list",
                    "Try restarting the application",
                    "Consider CPU fallback"
                ]
            })
        
        return diagnosis

# =======================================
# QWEN3 NPU PERFORMANCE MONITOR
# =======================================

class Qwen3NPUPerformanceMonitor:
    """Monitor NPU performance metrics for Qwen3"""
    
    def __init__(self):
        self.metrics = {
            "load_time": 0,
            "first_token_latency": 0,
            "tokens_per_second": 0,
            "compilation_time": 0,
            "memory_usage": 0,
            "cache_hit_rate": 0
        }
        
        self.load_start_time = None
        self.generation_start_time = None
        self.token_count = 0
    
    def start_load_timing(self):
        """Start timing model load"""
        self.load_start_time = time.time()
    
    def end_load_timing(self):
        """End timing model load"""
        if self.load_start_time:
            self.metrics["load_time"] = time.time() - self.load_start_time
            self.load_start_time = None
    
    def start_generation_timing(self):
        """Start timing generation"""
        self.generation_start_time = time.time()
        self.token_count = 0
    
    def record_token(self):
        """Record a token generation"""
        if self.generation_start_time:
            self.token_count += 1
            
            # Calculate first token latency
            if self.token_count == 1:
                self.metrics["first_token_latency"] = time.time() - self.generation_start_time
    
    def end_generation_timing(self):
        """End timing generation"""
        if self.generation_start_time and self.token_count > 0:
            total_time = time.time() - self.generation_start_time
            self.metrics["tokens_per_second"] = self.token_count / total_time
            self.generation_start_time = None
    
    def get_performance_report(self) -> str:
        """Generate performance report"""
        report = f"""
Qwen3 NPU Performance Report
============================
Load Time: {self.metrics['load_time']:.2f}s
First Token Latency: {self.metrics['first_token_latency']:.3f}s  
Tokens/Second: {self.metrics['tokens_per_second']:.1f}
Token Count: {self.token_count}

Performance Analysis:
- Load time {'✅ Good' if self.metrics['load_time'] < 90 else '⚠️ Slow'} (target: <90s)
- First token {'✅ Good' if self.metrics['first_token_latency'] < 2.0 else '⚠️ Slow'} (target: <2s)  
- Generation rate {'✅ Good' if self.metrics['tokens_per_second'] >= 15 else '⚠️ Slow'} (target: ≥15 tok/s)
        """
        return report.strip()

# =======================================
# QWEN3 NPU DEPLOYMENT UTILITIES
# =======================================

class Qwen3NPUDeployment:
    """Complete deployment utilities for Qwen3 on NPU"""
    
    def __init__(self, model_path: str, profile: str = "balanced"):
        self.model_path = model_path
        self.profile = profile
        self.config_builder = Qwen3NPUConfigBuilder(profile)
        self.performance_monitor = Qwen3NPUPerformanceMonitor()
        self.pipeline = None
    
    def deploy(self, **config_overrides) -> Optional[Any]:
        """
        Deploy Qwen3 model on NPU with comprehensive error handling
        
        Returns:
            Initialized LLMPipeline or None if deployment failed
        """
        import openvino_genai as ov_genai
        
        # Build configuration
        config = self.config_builder.build_complete_config(**config_overrides)
        
        # Validate configuration
        is_valid, missing = Qwen3NPUCompilationValidator.validate_config(config)
        if not is_valid:
            print(f"❌ Invalid NPU configuration. Missing: {missing}")
            return None
        
        print(f"🚀 Deploying Qwen3 on NPU with {self.profile} profile...")
        print(f"📊 Max prompt length: {config['NPUW_LLM_MAX_PROMPT_LEN']}")
        print(f"📊 Min response length: {config['NPUW_LLM_MIN_RESPONSE_LEN']}")
        
        # Attempt deployment with performance monitoring
        self.performance_monitor.start_load_timing()
        
        try:
            self.pipeline = ov_genai.LLMPipeline(self.model_path, "NPU", **config)
            self.performance_monitor.end_load_timing()
            
            print(f"✅ Qwen3 deployed successfully on NPU")
            print(f"⏱️ Load time: {self.performance_monitor.metrics['load_time']:.1f}s")
            
            return self.pipeline
            
        except Exception as e:
            self.performance_monitor.end_load_timing()
            
            print(f"❌ NPU deployment failed: {str(e)}")
            
            # Diagnose the failure
            diagnosis = Qwen3NPUCompilationValidator.diagnose_compilation_failure(str(e))
            print(f"🔍 Diagnosis: {diagnosis['likely_cause']}")
            
            for i, fix in enumerate(diagnosis['suggested_fixes'], 1):
                print(f"   {i}. {fix}")
            
            return None
    
    def benchmark(self, test_prompts: List[str] = None) -> Dict[str, Any]:
        """Run performance benchmark on deployed model"""
        
        if not self.pipeline:
            return {"error": "No pipeline deployed"}
        
        if not test_prompts:
            test_prompts = [
                "Explain quantum computing in simple terms.",
                "Write a Python function to calculate fibonacci numbers.", 
                "What are the benefits of using Intel NPU for AI inference?"
            ]
        
        results = []
        
        print("🔬 Running Qwen3 NPU benchmark...")
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"Test {i}/{len(test_prompts)}: {prompt[:50]}...")
            
            # Configure generation
            config = ov_genai.GenerationConfig()
            config.max_new_tokens = 256
            config.do_sample = False  # Greedy for consistent timing
            
            # Run test with timing
            self.performance_monitor.start_generation_timing()
            
            try:
                self.pipeline.start_chat()
                response = self.pipeline.generate(prompt, config)
                self.pipeline.finish_chat()
                
                # Record results
                self.performance_monitor.end_generation_timing()
                
                results.append({
                    "prompt": prompt,
                    "response_length": len(response.split()),
                    "first_token_latency": self.performance_monitor.metrics["first_token_latency"],
                    "tokens_per_second": self.performance_monitor.metrics["tokens_per_second"]
                })
                
            except Exception as e:
                results.append({
                    "prompt": prompt,
                    "error": str(e)
                })
        
        # Calculate averages
        successful_tests = [r for r in results if "error" not in r]
        if successful_tests:
            avg_latency = sum(r["first_token_latency"] for r in successful_tests) / len(successful_tests)
            avg_throughput = sum(r["tokens_per_second"] for r in successful_tests) / len(successful_tests)
            
            benchmark_summary = {
                "profile": self.profile,
                "total_tests": len(test_prompts),
                "successful_tests": len(successful_tests),
                "average_first_token_latency": avg_latency,
                "average_tokens_per_second": avg_throughput,
                "load_time": self.performance_monitor.metrics["load_time"],
                "detailed_results": results
            }
        else:
            benchmark_summary = {
                "profile": self.profile, 
                "error": "All tests failed",
                "detailed_results": results
            }
        
        return benchmark_summary
    
    def get_optimization_recommendations(self) -> List[str]:
        """Get optimization recommendations based on performance"""
        
        recommendations = []
        metrics = self.performance_monitor.metrics
        
        # Load time recommendations
        if metrics["load_time"] > 120:
            recommendations.append("Consider using a smaller model or different quantization")
            recommendations.append("Ensure NPU drivers are optimized")
        elif metrics["load_time"] > 90:
            recommendations.append("Load time is acceptable but could be improved with driver updates")
        
        # Generation speed recommendations  
        if metrics["tokens_per_second"] < 10:
            recommendations.append("Very slow generation - check NPU utilization and memory")
            recommendations.append("Consider switching to conservative profile")
        elif metrics["tokens_per_second"] < 15:
            recommendations.append("Generation speed could be improved - try balanced profile")
        
        # First token latency
        if metrics["first_token_latency"] > 3.0:
            recommendations.append("High first token latency - reduce max prompt length")
            recommendations.append("Consider greedy decoding for faster response start")
        
        if not recommendations:
            recommendations.append("Performance looks good! Current configuration is well-optimized.")
        
        return recommendations

# =======================================
# USAGE EXAMPLES
# =======================================

def example_basic_npu_deployment():
    """Example of basic NPU deployment"""
    
    model_path = "C:\\OpenVinoModels\\qwen3-8b-int4-cw-ov"
    
    # Deploy with balanced profile
    deployment = Qwen3NPUDeployment(model_path, profile="balanced")
    pipeline = deployment.deploy()
    
    if pipeline:
        print("✅ Deployment successful!")
        
        # Run benchmark
        benchmark_results = deployment.benchmark()
        print("\n📊 Benchmark Results:")
        for key, value in benchmark_results.items():
            if key != "detailed_results":
                print(f"  {key}: {value}")
    
        # Get recommendations
        recommendations = deployment.get_optimization_recommendations()
        print("\n💡 Optimization Recommendations:")
        for rec in recommendations:
            print(f"  • {rec}")

def example_custom_npu_configuration():
    """Example of custom NPU configuration"""
    
    # Build custom config
    builder = Qwen3NPUConfigBuilder("aggressive")
    
    # Add custom overrides
    custom_config = builder.build_complete_config(
        NPUW_LLM_MAX_PROMPT_LEN=8192,  # Larger context
        NPUW_COMPRESS_WEIGHTS="YES",   # Extra compression
        NPU_LOW_MEMORY_MODE="YES"      # Memory optimization
    )
    
    print("Custom NPU Configuration:")
    for key, value in custom_config.items():
        if key.startswith(("NPU", "NPUW")):
            print(f"  {key}: {value}")

def example_configuration_validation():
    """Example of configuration validation"""
    
    # Test configuration
    test_config = {
        "NPU_USE_NPUW": "YES",
        "NPUW_LLM": "YES", 
        "NPUW_LLM_BATCH_DIM": 0,
        "NPUW_LLM_SEQ_LEN_DIM": 1,
        "NPUW_LLM_MAX_PROMPT_LEN": 2048,
        "NPUW_LLM_MIN_RESPONSE_LEN": 256
    }
    
    # Validate
    is_valid, missing = Qwen3NPUCompilationValidator.validate_config(test_config)
    
    if is_valid:
        print("✅ Configuration is valid for NPU compilation")
    else:
        print("❌ Configuration issues found:")
        for issue in missing:
            print(f"  • {issue}")

if __name__ == "__main__":
    print("Qwen3 NPU Optimization Guide Loaded")
    print("=" * 50)
    
    # Show available profiles
    print("Available NPU Profiles:")
    for name, profile in QWEN3_NPU_PROFILES.items():
        print(f"  {name}: max_prompt={profile.max_prompt_len}, min_response={profile.min_response_len}")
    
    # Run validation example
    print("\nRunning configuration validation example...")
    example_configuration_validation()

================================================================================

FILE: context\qwen3_model_context\special_tokens.py
----------------------------------------
# Qwen3 Special Tokens & Chat Template Guide
# ==========================================
#
# Copyright (c) 2025 sbran
# Licensed under the MIT License
#
# Based on Qwen3 model specifications and tokenizer analysis.
# Incorporates patterns from official Qwen documentation and examples.
#
# PRIORITY: ⭐⭐⭐⭐⭐ (Critical for proper text processing)
#
# This file documents all Qwen3 special tokens, chat template format,
# and proper handling patterns for OpenVINO GenAI applications.

import re
from typing import Dict, List, Optional, Union

# =======================================
# QWEN3 SPECIAL TOKENS MAPPING
# =======================================

QWEN3_SPECIAL_TOKENS = {
    # Core conversation tokens
    151643: "<|endoftext|>",      # BOS/PAD token
    151644: "<|im_start|>",       # Instant Message start
    151645: "<|im_end|>",         # Instant Message end (also EOS)
    
    # Vision and multimodal tokens
    151646: "<|object_ref_start|>",  # Object reference start
    151647: "<|object_ref_end|>",    # Object reference end
    151648: "<|box_start|>",         # Bounding box start
    151649: "<|box_end|>",           # Bounding box end
    151650: "<|quad_start|>",        # Quadrilateral start
    151651: "<|quad_end|>",          # Quadrilateral end
    151652: "<|vision_start|>",      # Vision content start
    151653: "<|vision_end|>",        # Vision content end
    151654: "<|vision_pad|>",        # Vision padding
    151655: "<|image_pad|>",         # Image padding
    151656: "<|video_pad|>",         # Video padding
    
    # Tool calling tokens
    151657: "<tool_call>",           # Tool call start
    151658: "</tool_call>",          # Tool call end
    151665: "<tool_response>",       # Tool response start
    151666: "</tool_response>",      # Tool response end
    
    # Code completion tokens (Fill-in-Middle)
    151659: "<|fim_prefix|>",        # FIM prefix
    151660: "<|fim_middle|>",        # FIM middle
    151661: "<|fim_suffix|>",        # FIM suffix
    151662: "<|fim_pad|>",           # FIM padding
    
    # Repository tokens
    151663: "<|repo_name|>",         # Repository name
    151664: "<|file_sep|>",          # File separator
    
    # Reasoning tokens
    151667: "<think>",               # Thinking start
    151668: "</think>",              # Thinking end
}

# Reverse mapping for token lookup
QWEN3_TOKEN_TO_ID = {token: token_id for token_id, token in QWEN3_SPECIAL_TOKENS.items()}

# Tokens that should be filtered from user-visible output
QWEN3_FILTER_TOKENS = {
    "<|im_start|>", "<|im_end|>", "<|endoftext|>",
    "<|vision_start|>", "<|vision_end|>", "<|vision_pad|>",
    "<|image_pad|>", "<|video_pad|>",
    "<tool_call>", "</tool_call>", "<tool_response>", "</tool_response>",
    "<think>", "</think>"
}

# =======================================
# QWEN3 CHAT TEMPLATE PATTERNS
# =======================================

class Qwen3ChatTemplate:
    """Qwen3 chat template handler with support for all modes"""
    
    # Basic chat template (no tools)
    BASIC_TEMPLATE = """<|im_start|>system
{system_message}<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
{assistant_response}<|im_end|>"""
    
    # Tool-enabled template
    TOOL_TEMPLATE = """<|im_start|>system
{system_message}

# Tools

You may call one or more functions to assist with the user query.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{tools}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{{"name": <function-name>, "arguments": <args-json-object>}}
</tool_call><|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
{assistant_response}<|im_end|>"""
    
    # Thinking template (with reasoning)
    THINKING_TEMPLATE = """<|im_start|>system
{system_message}<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
<think>
{thinking_content}
</think>

{assistant_response}<|im_end|>"""
    
    @classmethod
    def format_basic_chat(cls, system_message: str, user_message: str, assistant_response: str = "") -> str:
        """Format basic chat conversation"""
        return cls.BASIC_TEMPLATE.format(
            system_message=system_message,
            user_message=user_message,
            assistant_response=assistant_response
        )
    
    @classmethod
    def format_with_tools(cls, system_message: str, user_message: str, tools: List[dict], assistant_response: str = "") -> str:
        """Format chat with tool capabilities"""
        import json
        tools_json = json.dumps(tools, indent=2)
        
        return cls.TOOL_TEMPLATE.format(
            system_message=system_message,
            user_message=user_message,
            tools=tools_json,
            assistant_response=assistant_response
        )
    
    @classmethod
    def format_with_thinking(cls, system_message: str, user_message: str, thinking_content: str, assistant_response: str) -> str:
        """Format chat with reasoning/thinking"""
        return cls.THINKING_TEMPLATE.format(
            system_message=system_message,
            user_message=user_message,
            thinking_content=thinking_content,
            assistant_response=assistant_response
        )

# =======================================
# QWEN3 TOKEN FILTERING UTILITIES
# =======================================

class Qwen3TokenFilter:
    """Utilities for filtering and cleaning Qwen3 model output"""
    
    @staticmethod
    def clean_special_tokens(text: str) -> str:
        """Remove Qwen3 special tokens from text output"""
        cleaned = text
        
        # Remove all special tokens
        for token in QWEN3_FILTER_TOKENS:
            cleaned = cleaned.replace(token, "")
        
        # Clean up extra whitespace
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = cleaned.strip()
        
        return cleaned
    
    @staticmethod
    def extract_thinking_content(text: str) -> tuple[str, str]:
        """Extract thinking content from response"""
        thinking_pattern = r'<think>(.*?)</think>'
        thinking_match = re.search(thinking_pattern, text, re.DOTALL)
        
        if thinking_match:
            thinking_content = thinking_match.group(1).strip()
            response_without_thinking = re.sub(thinking_pattern, '', text, flags=re.DOTALL).strip()
            return thinking_content, response_without_thinking
        
        return "", text
    
    @staticmethod
    def extract_tool_calls(text: str) -> tuple[List[dict], str]:
        """Extract tool calls from response"""
        import json
        
        tool_call_pattern = r'<tool_call>(.*?)</tool_call>'
        tool_calls = []
        
        for match in re.finditer(tool_call_pattern, text, re.DOTALL):
            try:
                tool_call_json = match.group(1).strip()
                tool_call = json.loads(tool_call_json)
                tool_calls.append(tool_call)
            except json.JSONDecodeError:
                continue
        
        # Remove tool calls from main text
        text_without_tools = re.sub(tool_call_pattern, '', text, flags=re.DOTALL).strip()
        
        return tool_calls, text_without_tools
    
    @staticmethod
    def is_special_token_id(token_id: int) -> bool:
        """Check if token ID is a special token"""
        return token_id in QWEN3_SPECIAL_TOKENS
    
    @staticmethod
    def should_display_token(token_id: int) -> bool:
        """Check if token should be displayed to user"""
        if token_id not in QWEN3_SPECIAL_TOKENS:
            return True  # Regular content token
        
        token_text = QWEN3_SPECIAL_TOKENS[token_id]
        return token_text not in QWEN3_FILTER_TOKENS

# =======================================
# QWEN3 STREAMING TOKEN HANDLER
# =======================================

class Qwen3StreamingFilter:
    """Token-level filtering for streaming applications"""
    
    def __init__(self):
        self.accumulated_tokens = []
        self.current_text = ""
        self.in_thinking_mode = False
        self.thinking_buffer = ""
    
    def process_token(self, token_id: int, token_text: str) -> Optional[str]:
        """
        Process a single token and return text to display (if any)
        
        Args:
            token_id: Token ID from model
            token_text: Decoded token text
            
        Returns:
            Text to display to user, or None if token should be filtered
        """
        
        # Handle special tokens
        if token_id in QWEN3_SPECIAL_TOKENS:
            special_token = QWEN3_SPECIAL_TOKENS[token_id]
            
            # Handle thinking mode
            if special_token == "<think>":
                self.in_thinking_mode = True
                return None
            elif special_token == "</think>":
                self.in_thinking_mode = False
                self.thinking_buffer = ""  # Clear thinking buffer
                return None
            
            # Filter other special tokens
            if special_token in QWEN3_FILTER_TOKENS:
                return None
        
        # If in thinking mode, buffer but don't display
        if self.in_thinking_mode:
            self.thinking_buffer += token_text
            return None
        
        # Regular content token - display it
        self.current_text += token_text
        return token_text
    
    def get_thinking_content(self) -> str:
        """Get accumulated thinking content"""
        return self.thinking_buffer
    
    def reset(self):
        """Reset filter state for new conversation"""
        self.accumulated_tokens = []
        self.current_text = ""
        self.in_thinking_mode = False
        self.thinking_buffer = ""

# =======================================
# OPENVINO GENAI INTEGRATION PATTERNS
# =======================================

class Qwen3OpenVINOStreamer:
    """Custom streamer for Qwen3 with proper token filtering"""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.filter = Qwen3StreamingFilter()
        self.response_queue = []
    
    def put(self, token_id: int) -> bool:
        """Process token from OpenVINO GenAI pipeline"""
        
        # Decode token
        try:
            token_text = self.tokenizer.decode([token_id])
        except:
            token_text = f"[UNK_{token_id}]"
        
        # Filter and process
        display_text = self.filter.process_token(token_id, token_text)
        
        # Add to queue if should be displayed
        if display_text is not None:
            self.response_queue.append(display_text)
        
        # Continue generation (return False)
        return False
    
    def end(self):
        """Called when generation ends"""
        pass
    
    def get_response_text(self) -> str:
        """Get accumulated response text"""
        return "".join(self.response_queue)
    
    def get_thinking_content(self) -> str:
        """Get thinking content if any"""
        return self.filter.get_thinking_content()

# =======================================
# USAGE EXAMPLES
# =======================================

def example_basic_filtering():
    """Example of basic token filtering"""
    
    # Raw model output with special tokens
    raw_output = "<|im_start|>assistant\nHello! <think>\nThe user is greeting me.\n</think>\nHow can I help you today?<|im_end|>"
    
    # Clean for display
    filter = Qwen3TokenFilter()
    clean_output = filter.clean_special_tokens(raw_output)
    print(f"Cleaned output: {clean_output}")
    
    # Extract thinking content
    thinking, response = filter.extract_thinking_content(raw_output)
    print(f"Thinking: {thinking}")
    print(f"Response: {response}")

def example_chat_formatting():
    """Example of chat template formatting"""
    
    # Basic chat
    system_msg = "You are a helpful AI assistant."
    user_msg = "What is machine learning?"
    
    formatted = Qwen3ChatTemplate.format_basic_chat(system_msg, user_msg)
    print("Basic chat format:")
    print(formatted)
    
    # Chat with tools
    tools = [
        {
            "name": "search_web",
            "description": "Search the internet",
            "parameters": {
                "query": {"type": "string", "description": "Search query"}
            }
        }
    ]
    
    formatted_tools = Qwen3ChatTemplate.format_with_tools(system_msg, user_msg, tools)
    print("\nTools chat format:")
    print(formatted_tools)

def example_streaming_integration():
    """Example of streaming integration with OpenVINO GenAI"""
    
    import openvino_genai as ov_genai
    
    # Mock tokenizer for example
    class MockTokenizer:
        def decode(self, tokens):
            # Simplified for example
            return f"token_{tokens[0]}"
    
    # Create custom streamer
    tokenizer = MockTokenizer()
    streamer = Qwen3OpenVINOStreamer(tokenizer)
    
    # Simulate token processing
    test_tokens = [
        (151644, "<|im_start|>"),  # Should be filtered
        (12345, "Hello"),          # Should be displayed
        (67890, " world"),         # Should be displayed  
        (151645, "<|im_end|>")     # Should be filtered
    ]
    
    for token_id, expected_text in test_tokens:
        streamer.put(token_id)
    
    streamer.end()
    
    print(f"Final response: {streamer.get_response_text()}")

# =======================================
# INTEGRATION WITH GRADIO APPLICATIONS
# =======================================

def create_qwen3_gradio_streamer(tokenizer):
    """Create Gradio-compatible streamer for Qwen3"""
    
    import queue
    import threading
    
    class Qwen3GradioStreamer(Qwen3OpenVINOStreamer):
        def __init__(self, tokenizer):
            super().__init__(tokenizer)
            self.text_queue = queue.Queue()
            self.is_generating = True
        
        def put(self, token_id: int) -> bool:
            # Process token
            display_text = self.filter.process_token(token_id, self.tokenizer.decode([token_id]))
            
            if display_text:
                # Add to streaming queue
                self.text_queue.put(display_text)
            
            return False  # Continue generation
        
        def end(self):
            self.is_generating = False
            self.text_queue.put(None)  # End marker
        
        def stream_text(self):
            """Generator for Gradio streaming"""
            accumulated = ""
            while self.is_generating or not self.text_queue.empty():
                try:
                    chunk = self.text_queue.get(timeout=0.1)
                    if chunk is None:  # End marker
                        break
                    accumulated += chunk
                    yield accumulated
                except queue.Empty:
                    continue
    
    return Qwen3GradioStreamer(tokenizer)

# =======================================
# VALIDATION AND TESTING
# =======================================

def validate_qwen3_tokenization():
    """Validate Qwen3 special token handling"""
    
    print("Qwen3 Special Token Validation")
    print("=" * 40)
    
    # Test special token mapping
    for token_id, token_text in list(QWEN3_SPECIAL_TOKENS.items())[:5]:
        print(f"Token ID {token_id}: {token_text}")
        
        # Test filtering
        should_display = not (token_text in QWEN3_FILTER_TOKENS)
        print(f"  Display to user: {should_display}")
    
    print("\nChat Template Validation")
    print("=" * 40)
    
    # Test chat template
    system = "You are helpful."
    user = "Hello!"
    formatted = Qwen3ChatTemplate.format_basic_chat(system, user)
    
    print("Formatted chat:")
    print(formatted[:100] + "..." if len(formatted) > 100 else formatted)
    
    # Test filtering
    filter = Qwen3TokenFilter()
    cleaned = filter.clean_special_tokens(formatted)
    print(f"\nCleaned version: {cleaned}")

if __name__ == "__main__":
    # Run validation
    validate_qwen3_tokenization()
    
    print("\nSpecial tokens loaded:")
    print(f"Total special tokens: {len(QWEN3_SPECIAL_TOKENS)}")
    print(f"Tokens to filter: {len(QWEN3_FILTER_TOKENS)}")
    print(f"Core chat tokens: <|im_start|>, <|im_end|>, <|endoftext|>")

================================================================================

FILE: context\test_configs\generation_config.py
----------------------------------------
# Copyright (C) 2018-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# todo: CVS-162108: remove this file to habdle generation config directly in tests

from openvino_genai import GenerationConfig

def get_greedy() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.num_return_sequences = 1
    generation_config.max_new_tokens = 30
    return generation_config

def get_greedy_with_penalties() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.num_return_sequences = 1
    generation_config.presence_penalty = 2.0
    generation_config.frequency_penalty = 0.2
    generation_config.max_new_tokens = 30
    return generation_config

def get_beam_search() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.num_beam_groups = 3
    generation_config.num_beams = 6
    generation_config.diversity_penalty = 1
    generation_config.max_new_tokens = 30
    generation_config.num_return_sequences = 3
    generation_config.num_return_sequences = generation_config.num_beams
    return generation_config

def get_multinomial_temperature() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.8
    generation_config.num_return_sequences = 1
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_num_return_sequence() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.7
    generation_config.num_return_sequences = 3
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_top_p() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.num_return_sequences = 1
    generation_config.do_sample = True
    generation_config.temperature = 0.8
    generation_config.top_p = 0.9
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_top_k() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.num_return_sequences = 1
    generation_config.temperature = 0.8
    generation_config.top_k = 2
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_top_p_and_top_k() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.8
    generation_config.top_p = 0.9
    generation_config.num_return_sequences = 1
    generation_config.top_k = 2
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_repetition_penalty() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.num_return_sequences = 1
    generation_config.temperature = 0.8
    generation_config.repetition_penalty = 2.0
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_all_parameters() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.num_return_sequences = 4
    generation_config.temperature = 0.9
    generation_config.top_p = 0.8
    generation_config.top_k = 20
    generation_config.repetition_penalty = 2.0
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_frequence_penalty() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.8
    generation_config.frequency_penalty = 0.5
    generation_config.num_return_sequences = 1
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_presence_penalty() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.8
    generation_config.presence_penalty = 0.1
    generation_config.num_return_sequences = 1
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_max_and_min_token() -> GenerationConfig:
    multinomial = GenerationConfig()
    multinomial.do_sample = True
    multinomial.temperature = 0.9
    multinomial.top_p = 0.9
    multinomial.top_k = 20
    multinomial.num_return_sequences = 3
    multinomial.presence_penalty = 0.01
    multinomial.frequency_penalty = 0.1
    multinomial.min_new_tokens = 15
    multinomial.max_new_tokens = 30
    return multinomial


================================================================================

FILE: context\test_configs\hugging_face.py
----------------------------------------
# Copyright (C) 2018-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

from os.path import sep
from pathlib import Path
from typing import Type
from functools import lru_cache

from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import GenerationConfig as HFGenerationConfig

from optimum.intel import OVModelForCausalLM, OVModelForFeatureExtraction, OVModelForSequenceClassification
from optimum.intel.openvino.modeling import OVModel

from huggingface_hub import hf_hub_download

from openvino import save_model
from openvino_genai import GenerationResult, GenerationConfig, StopCriteria
from openvino_tokenizers import convert_tokenizer

from utils.constants import get_default_llm_properties, extra_generate_kwargs, get_ov_cache_models_dir
from utils.network import retry_request
import pytest

def generation_config_to_hf(
    default_generation_config : HFGenerationConfig,
    generation_config : GenerationConfig
) -> HFGenerationConfig:
    if generation_config is None:
        return

    kwargs = {}
    kwargs['return_dict_in_generate'] = True

    # generic parameters
    kwargs['max_length'] = generation_config.max_length
    # has higher priority than 'max_length'
    kwargs['max_new_tokens'] = generation_config.max_new_tokens
    kwargs['min_new_tokens'] = generation_config.min_new_tokens
    if generation_config.stop_strings:
        kwargs['stop_strings'] = generation_config.stop_strings

    # copy default parameters
    kwargs['bos_token_id'] = default_generation_config.bos_token_id
    kwargs['pad_token_id'] = default_generation_config.pad_token_id

    if (generation_config.ignore_eos):
        kwargs['eos_token_id'] = []
    else:
        if len(generation_config.stop_token_ids) > 0:
            kwargs['eos_token_id'] = list(generation_config.stop_token_ids)
        elif generation_config.eos_token_id != -1:
            kwargs['eos_token_id'] = generation_config.eos_token_id
        else:
            kwargs['eos_token_id'] = default_generation_config.eos_token_id

    # copy penalties
    kwargs['repetition_penalty'] = generation_config.repetition_penalty

    if generation_config.is_beam_search():
        # beam search case
        kwargs['num_beam_groups'] = generation_config.num_beam_groups
        kwargs['num_beams'] = generation_config.num_beams
        kwargs['length_penalty'] = generation_config.length_penalty
        kwargs['no_repeat_ngram_size'] = generation_config.no_repeat_ngram_size
        kwargs['num_return_sequences'] = generation_config.num_return_sequences
        kwargs['output_scores'] = True

        if generation_config.num_beam_groups > 1:
            kwargs['diversity_penalty'] = generation_config.diversity_penalty

        # in OpenVINO GenAI this parameter is called stop_criteria,
        # while in HF it's called early_stopping.
        # HF values True, False and "never" correspond to OV GenAI values "EARLY", "HEURISTIC" and "NEVER"
        STOP_CRITERIA_MAP = {
            StopCriteria.NEVER: "never",
            StopCriteria.EARLY: True,
            StopCriteria.HEURISTIC: False
        }

        kwargs['early_stopping'] = STOP_CRITERIA_MAP[generation_config.stop_criteria]
    elif generation_config.is_multinomial():
        # mulitinomial
        kwargs['temperature'] = generation_config.temperature
        kwargs['top_k'] = generation_config.top_k
        kwargs['top_p'] = generation_config.top_p
        kwargs['do_sample'] = generation_config.do_sample
    else:
        # greedy
        pass

    hf_generation_config = HFGenerationConfig(**kwargs)
    return hf_generation_config

def run_hugging_face(
    opt_model,
    hf_tokenizer,
    prompts: list[str],
    generation_configs: list[GenerationConfig] | GenerationConfig,
) -> list[GenerationResult]:
    generation_results = []

    if type(generation_configs) is list:
        # process prompt by promp as we have multiple generation configs
        for prompt, generation_config in zip(prompts, generation_configs):
            hf_generation_config = generation_config_to_hf(opt_model.generation_config, generation_config)
            inputs = {}
            if hf_tokenizer.chat_template and generation_config.apply_chat_template:
                prompt = hf_tokenizer.apply_chat_template([{'role': 'user', 'content': prompt}], tokenize=False, add_generation_prompt=True)
                inputs = hf_tokenizer(prompt, return_tensors="pt", add_special_tokens=False)
            else:
                inputs = hf_tokenizer(prompt, return_tensors="pt")
            input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']
            prompt_len = 0 if generation_config.echo else input_ids.numel()

            generate_outputs = opt_model.generate(input_ids=input_ids, attention_mask=attention_mask, generation_config=hf_generation_config, tokenizer=hf_tokenizer, **extra_generate_kwargs())
            all_text_batch = hf_tokenizer.batch_decode([generated_ids[prompt_len:] for generated_ids in generate_outputs.sequences], skip_special_tokens=True)

            generation_result = GenerationResult()
            generation_result.m_generation_ids = all_text_batch
            # sequences_scores are available only for beam search case
            if generation_config.is_beam_search():
                generation_result.m_scores = [score for score in generate_outputs.sequences_scores]
            generation_results.append(generation_result)
    else:
        inputs = {}
        if hf_tokenizer.chat_template and generation_configs.apply_chat_template:
            processed_prompts = []
            for prompt in prompts:
                processed_prompts.append(hf_tokenizer.apply_chat_template([{'role': 'user', 'content': prompt}], tokenize=False, add_generation_prompt=True))
            # process all prompts as a single batch as we have a single generation config for all prompts
            inputs = hf_tokenizer(processed_prompts, return_tensors='pt', padding=True, truncation=True, add_special_tokens=False, padding_side='left')
        else:
            inputs = hf_tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, padding_side='left')
        input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']
        hf_generation_config = generation_config_to_hf(opt_model.generation_config, generation_configs)
        hf_encoded_outputs = opt_model.generate(input_ids, attention_mask=attention_mask, generation_config=hf_generation_config, tokenizer=hf_tokenizer, **extra_generate_kwargs())

        generation_ids = []
        scores = []

        for idx, hf_encoded_out in enumerate(hf_encoded_outputs.sequences):
            prompt_idx = idx // hf_generation_config.num_return_sequences
            prompt_len = 0 if generation_configs.echo else input_ids[prompt_idx].numel()
            decoded_text = hf_tokenizer.decode(hf_encoded_out[prompt_len:], skip_special_tokens=True)
            generation_ids.append(decoded_text)
            if generation_configs.is_beam_search():
                scores.append(hf_encoded_outputs.sequences_scores[idx])

            # if we need to move to next generation result
            if (idx + 1) // hf_generation_config.num_return_sequences != prompt_idx:
                generation_result = GenerationResult()
                generation_result.m_generation_ids = generation_ids
                generation_result.m_scores = scores
                generation_results.append(generation_result)
                generation_ids = []
                scores = []

    del hf_tokenizer
    del opt_model

    return generation_results


# download HF model or read converted model
def get_huggingface_models(model_id: str | Path, model_class: Type[OVModel], local_files_only=False):
    hf_tokenizer = retry_request(lambda: AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, local_files_only=local_files_only))
    opt_model = retry_request(lambda: model_class.from_pretrained(model_id, export=isinstance(model_id, str), compile=False, load_in_8bit=False, trust_remote_code=isinstance(model_id, str), ov_config=get_default_llm_properties(), local_files_only=local_files_only))
    return opt_model, hf_tokenizer


def convert_and_save_tokenizer(hf_tokenizer : AutoTokenizer,
                               models_path: Path,
                               **tokenizer_kwargs):
    tokenizer, detokenizer = convert_tokenizer(hf_tokenizer, with_detokenizer=True, **tokenizer_kwargs)

    from utils.constants import OV_DETOKENIZER_FILENAME, OV_TOKENIZER_FILENAME
    save_model(tokenizer, models_path / OV_TOKENIZER_FILENAME)
    save_model(detokenizer, models_path / OV_DETOKENIZER_FILENAME)


def convert_models(opt_model : OVModelForCausalLM,
                   hf_tokenizer : AutoTokenizer,
                   models_path: Path,
                   **tokenizer_kwargs):
    opt_model.save_pretrained(models_path)
    # save generation config
    if opt_model.generation_config:
        opt_model.generation_config.save_pretrained(models_path)
    opt_model.config.save_pretrained(models_path)

    # to store tokenizer config jsons with special tokens
    hf_tokenizer.save_pretrained(models_path)
    # convert tokenizers as well
    convert_and_save_tokenizer(hf_tokenizer, models_path)


def download_and_convert_model(model_id: str, **tokenizer_kwargs):
    return _download_and_convert_model(model_id, OVModelForCausalLM, **tokenizer_kwargs)

@pytest.fixture()
def download_and_convert_embeddings_models(request):
    model_id = request.param
    return _download_and_convert_model(model_id, OVModelForFeatureExtraction)


@pytest.fixture()
def download_and_convert_rerank_model(request):
    model_id = request.param
    opt_model, hf_tokenizer, models_path = _download_and_convert_model(model_id, OVModelForSequenceClassification)
    ov_tokenizer = convert_tokenizer(hf_tokenizer, with_detokenizer=False, number_of_inputs=2)
    save_model(ov_tokenizer, models_path / "openvino_tokenizer.xml")
    return opt_model, hf_tokenizer, models_path


def _download_and_convert_model(model_id: str, model_class: Type[OVModel], **tokenizer_kwargs):
    dir_name = str(model_id).replace(sep, "_")
    ov_cache_models_dir = get_ov_cache_models_dir()
    models_path = ov_cache_models_dir / dir_name

    from utils.constants import OV_MODEL_FILENAME
    if (models_path / OV_MODEL_FILENAME).exists():
        opt_model, hf_tokenizer = get_huggingface_models(models_path, model_class, local_files_only=True)
    else:
        opt_model, hf_tokenizer = get_huggingface_models(model_id, model_class, local_files_only=False)
        convert_models(opt_model, hf_tokenizer, models_path)

    if "padding_side" in tokenizer_kwargs:
        hf_tokenizer.padding_side = tokenizer_kwargs.pop("padding_side")

    return opt_model, hf_tokenizer, models_path


def download_gguf_model(gguf_model_id: str,
                        gguf_filename: str):
    gguf_dir_name = str(gguf_model_id).replace(sep, "_")
    ov_cache_models_dir = get_ov_cache_models_dir()
    models_path_gguf = ov_cache_models_dir / gguf_dir_name

    gguf_path = hf_hub_download(
        repo_id=gguf_model_id,
        filename=gguf_filename,
        local_dir=models_path_gguf # Optional: Specify download directory
    )

    return gguf_path

@lru_cache(maxsize=None)
def load_hf_model_from_gguf(gguf_model_id, gguf_filename):
    return retry_request(lambda: AutoModelForCausalLM.from_pretrained(gguf_model_id, gguf_file=gguf_filename))

@lru_cache(maxsize=None)
def load_hf_tokenizer_from_gguf(gguf_model_id, gguf_filename):
    return retry_request(lambda: AutoTokenizer.from_pretrained(gguf_model_id, gguf_file=gguf_filename))


================================================================================

FILE: context\test_configs\ov_genai_pipelines.py
----------------------------------------
# Copyright (C) 2018-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

from enum import Enum
from pathlib import Path
from typing import Callable
from shutil import rmtree

from optimum.intel.openvino.utils import TemporaryDirectory
from openvino_genai import SchedulerConfig, draft_model, ContinuousBatchingPipeline, \
    LLMPipeline, GenerationConfig, GenerationResult, StreamerBase, DecodedResults

from utils.constants import get_default_llm_properties
from utils.comparation import compare_generation_results, compare_generation_results_vs_ref
from utils.hugging_face import download_and_convert_model, run_hugging_face

def dict_to_scheduler_config(scheduler_params: dict = None) -> SchedulerConfig:
    scheduler_config = SchedulerConfig()
    if scheduler_params is None:
        scheduler_config.dynamic_split_fuse = True
        # vLLM specific
        scheduler_config.max_num_batched_tokens = 256
        scheduler_config.max_num_seqs = 256

        # Expedited number of blocks = text_blocks_n * G * n_prompts, where
        # text_blocks_n - number of blocks required for storing prompt and generated text,
        # currently it is 1 block for prompt (31 token with block_size 32) + 1 block for generated text (max length of generated text - 30 tokens);
        # G - number of sequences in a sequence group, for beam search it is 2(group_size) * 3 (num_groups);
        # n_prompts - number of prompts.
        # For current parameters in tests expedited number of blocks is approximately 48.
        scheduler_config.num_kv_blocks = 60
    else:
        for param, value in scheduler_params.items():
            setattr(scheduler_config, param, value)

    return scheduler_config


class PipelineType(Enum):
    STATEFUL = 1
    PAGED_ATTENTION = 2
    CONTINUOUS_BATCHING = 3
    SPECULATIVE_DECODING = 4
    PROMPT_LOOKUP_DECODING = 5
    AUTO = 6


def get_all_pipeline_types():
    return [PipelineType.STATEFUL, PipelineType.PAGED_ATTENTION, PipelineType.CONTINUOUS_BATCHING, PipelineType.SPECULATIVE_DECODING, PipelineType.PROMPT_LOOKUP_DECODING, PipelineType.AUTO]

def get_main_pipeline_types():
    return [PipelineType.STATEFUL, PipelineType.PAGED_ATTENTION, PipelineType.SPECULATIVE_DECODING, PipelineType.PROMPT_LOOKUP_DECODING]

def get_gguf_pipeline_types():
    return [PipelineType.STATEFUL, PipelineType.PAGED_ATTENTION]

class StreamerWithResults:
    # Return a streamer which accumulates results in order to compare with results returned from generate.
    results: list[str] = []
    def __init__(self):
        self.results = []

    def accumulate(self, subword) -> bool:
        self.results.append(subword)
        return False
    
    def get_results(self) -> list[GenerationResult]:
        streaming_result = GenerationResult()
        streaming_result.m_generation_ids = [''.join(self.results)]
        return [streaming_result]
    
    def reset(self):
        self.results = []


def create_ov_pipeline(models_path: Path,
                       pipeline_type: PipelineType = PipelineType.AUTO,
                       device: str = "CPU",
                       ov_config: dict = get_default_llm_properties(),
                       scheduler_config: SchedulerConfig = SchedulerConfig(),
                       draft_model_path: Path = None,
                       enable_save_ov_model: bool = None,
                       dynamic_quantization_group_size: str = None):
    local_ov_config = ov_config.copy()
    if pipeline_type == PipelineType.AUTO:
        return LLMPipeline(models_path, device, ov_config)
    elif pipeline_type == PipelineType.STATEFUL:
        if enable_save_ov_model is not None: local_ov_config["enable_save_ov_model"] = enable_save_ov_model
        if dynamic_quantization_group_size is not None: local_ov_config["DYNAMIC_QUANTIZATION_GROUP_SIZE"] = dynamic_quantization_group_size
        return LLMPipeline(models_path, device, local_ov_config, ATTENTION_BACKEND="SDPA")
    elif pipeline_type == PipelineType.PAGED_ATTENTION:
        if enable_save_ov_model is not None: local_ov_config["enable_save_ov_model"] = enable_save_ov_model
        if dynamic_quantization_group_size is not None: local_ov_config["DYNAMIC_QUANTIZATION_GROUP_SIZE"] = dynamic_quantization_group_size
        return LLMPipeline(models_path, device, local_ov_config, scheduler_config=scheduler_config, ATTENTION_BACKEND="PA")
    elif pipeline_type == PipelineType.CONTINUOUS_BATCHING:
        return ContinuousBatchingPipeline(models_path, scheduler_config, device, ov_config)
    elif pipeline_type == PipelineType.SPECULATIVE_DECODING:
        ov_draft_model = draft_model(models_path) if draft_model_path is None else draft_model(draft_model_path)
        return LLMPipeline(models_path, device, ov_config, scheduler_config=scheduler_config, draft_model=ov_draft_model)
    elif pipeline_type == PipelineType.PROMPT_LOOKUP_DECODING:
        return LLMPipeline(models_path, device, ov_config, scheduler_config=scheduler_config, prompt_lookup=True)
    else:
        raise Exception(f"Unsupported pipeline type: {pipeline_type}")

def create_ov_cb_pipeline(models_path: Path,
                       pipeline_type: PipelineType = PipelineType.AUTO,
                       device: str = "CPU",
                       ov_config: dict = get_default_llm_properties(),
                       scheduler_config: SchedulerConfig = SchedulerConfig(),
                       draft_model_path: Path = None):
    local_ov_config = ov_config.copy()
    if pipeline_type == PipelineType.CONTINUOUS_BATCHING:
        return ContinuousBatchingPipeline(models_path, scheduler_config, device, local_ov_config)
    elif pipeline_type == PipelineType.SPECULATIVE_DECODING:
        ov_draft_model = draft_model(models_path) if draft_model_path is None else draft_model(draft_model_path)
        local_ov_config["draft_model"] = ov_draft_model
        return ContinuousBatchingPipeline(models_path, scheduler_config, device, local_ov_config)
    elif pipeline_type == PipelineType.PROMPT_LOOKUP_DECODING:
        local_ov_config["prompt_lookup"] = True
        return ContinuousBatchingPipeline(models_path, scheduler_config, device, local_ov_config)
    else:
        raise Exception(f"Unsupported pipeline type: {pipeline_type}")


def prepare_generation_config_by_pipe_type(generation_config : GenerationConfig,
                                           pipeline_type: PipelineType = PipelineType.AUTO):
    if pipeline_type == PipelineType.SPECULATIVE_DECODING:
        assert not generation_config.is_beam_search()
        generation_config.assistant_confidence_threshold = 0.9
    elif pipeline_type == PipelineType.PROMPT_LOOKUP_DECODING:
        assert not generation_config.is_beam_search()
        generation_config.num_assistant_tokens = 5
        generation_config.max_ngram_size = 3
    return generation_config


def prepare_generation_configs_by_pipe_type(generation_configs : list[GenerationConfig],
                                            pipeline_type: PipelineType = PipelineType.AUTO):
    return [ prepare_generation_config_by_pipe_type(generation_config, pipeline_type) for generation_config in generation_configs ]


def convert_decoded_results_to_generation_result(generate_outputs: DecodedResults,
                                                 num_prompts: int,
                                                 num_return_sequences: int,
                                                 is_beam_search: bool) -> list[GenerationResult]:
    index = 0
    generation_results = []

    for _ in range(num_prompts):
        generation_result = GenerationResult()

        generation_result.m_generation_ids = generate_outputs.texts[index : index + num_return_sequences]
        # sequences_scores are available only for beam search case
        if is_beam_search:
            generation_result.m_scores = generate_outputs.scores[index : index + num_return_sequences]
        generation_results.append(generation_result)

        index += num_return_sequences
    return generation_results


def run_ov_pipeline(models_path: Path,
                    prompt : str | list[str],
                    generation_config : GenerationConfig | list[GenerationConfig],
                    pipeline_type : PipelineType = PipelineType.AUTO,
                    streamer: StreamerWithResults | Callable | StreamerBase = None,
                    scheduler_config: SchedulerConfig = SchedulerConfig(),
                    draft_model_path: Path = None,
                    ov_config: dict = {},
                    device: str = "CPU"
    ) -> list[GenerationResult]:
    # update the generation config according pipeline_type
    updated_generation_config = None
    if isinstance(generation_config, list):
        if pipeline_type != PipelineType.CONTINUOUS_BATCHING:
            raise Exception(f"\'generation_config\' is \'list[GenerationConfig]\'. This type is supported only for \'PipelineType.CONTINIOUS_BATCHING\'! Please change pipeline_type or generation_config type!")
        assert isinstance(prompt, list)
        assert len(generation_config) == len(prompt)
        updated_generation_config = prepare_generation_configs_by_pipe_type(generation_config, pipeline_type)
    else:
        updated_generation_config = prepare_generation_config_by_pipe_type(generation_config, pipeline_type)

    # checking streamer
    if isinstance(prompt, str):
        if streamer is None and not (generation_config.is_beam_search() or generation_config.num_return_sequences > 1) and len(prompt) == 1:
            # We can use streamer only if we have a single prompt and not beam search.
            streamer = StreamerWithResults()
        if isinstance(streamer, StreamerWithResults):
            # Clear the accumulated strings to avoid side effects
            streamer.reset()

    # create pipeline and generate results
    ov_pipe = create_ov_pipeline(models_path=models_path,
                                 pipeline_type=pipeline_type,
                                 device=device,
                                 ov_config=ov_config,
                                 scheduler_config=scheduler_config,
                                 draft_model_path=draft_model_path)
    generation_results = ov_pipe.generate(prompt, updated_generation_config, streamer)

    # convert results to `list[GenerationResult]`
    if isinstance(generation_results, DecodedResults):
        assert isinstance(generation_config, GenerationConfig)
        num_prompts = 1 if isinstance(prompt, str) else len(prompt)
        generation_results = convert_decoded_results_to_generation_result(generation_results, num_prompts, generation_config.num_return_sequences, generation_config.is_beam_search())
    
    # cleanup test artifacts
    del ov_pipe

    # compare streaming results with generated results
    if isinstance(streamer, StreamerWithResults):
        prompts = [ prompt ] if isinstance(prompt, str) else prompt
        compare_generation_results(prompts, generation_results, streamer.get_results(), generation_config)

    return generation_results


def is_generation_available(generation_config: GenerationConfig | list[GenerationConfig],
                            pipeline_type: PipelineType):
    if type(generation_config) is GenerationConfig:
        if generation_config.is_beam_search():
            if pipeline_type == PipelineType.PROMPT_LOOKUP_DECODING or pipeline_type == PipelineType.SPECULATIVE_DECODING:
                return False
    else:
        for gen_config in generation_config:
            if gen_config.is_beam_search():
                if pipeline_type == PipelineType.PROMPT_LOOKUP_DECODING or pipeline_type == PipelineType.SPECULATIVE_DECODING:
                    return False
    return True


# TODO: remove `ref` after Generator property is supported by LLMPipeline / VLMPipeline
def generate_and_compare(model: str,
                         prompts : str | list[str],
                         generation_config: list[GenerationConfig] | GenerationConfig | dict,
                         pipeline_type: PipelineType = PipelineType.AUTO,
                         scheduler_config: SchedulerConfig | dict = SchedulerConfig(),
                         ref : list[list[str]] = None,
                         streamer: StreamerWithResults | Callable | StreamerBase = None):
    ov_prompts = prompts if type(prompts) is list else [prompts]

    ov_gen_config = GenerationConfig(**generation_config) if type(generation_config) is dict else generation_config
    hf_gen_config = ov_gen_config

    if not is_generation_available(ov_gen_config, pipeline_type):
        return

    if type(ov_gen_config) is list:
        assert len(ov_gen_config) == len(ov_prompts)
    elif pipeline_type == PipelineType.CONTINUOUS_BATCHING:
        ov_gen_config = [ov_gen_config] * len(ov_prompts)

    ov_scheduler_config = scheduler_config if isinstance(scheduler_config, SchedulerConfig) else dict_to_scheduler_config(scheduler_config)
    opt_model, hf_tokenizer, models_path = download_and_convert_model(model)

    # w/a to align different API between CB and LLM
    run_cnt = len(ov_gen_config) if pipeline_type != PipelineType.CONTINUOUS_BATCHING and type(ov_gen_config) is list else 1

    for i in range(run_cnt):
        current_it_prompts = [ov_prompts[i]] if run_cnt > 1 else ov_prompts
        current_it_gen_config = ov_gen_config[i] if run_cnt > 1 else ov_gen_config

        ov_results = run_ov_pipeline(models_path=models_path,
                                     prompt=current_it_prompts,
                                     generation_config=current_it_gen_config,
                                     pipeline_type=pipeline_type,
                                     streamer=streamer.accumulate if isinstance(streamer, StreamerWithResults) else streamer,
                                     scheduler_config=ov_scheduler_config,
                                     ov_config=get_default_llm_properties())

        if ref is None:
            current_it_hf_config = [hf_gen_config[i]] if run_cnt > 1 else hf_gen_config
            ref_results = run_hugging_face(opt_model, hf_tokenizer, current_it_prompts, current_it_hf_config)
            compare_generation_results(current_it_prompts, ref_results, ov_results, current_it_gen_config)
        else:
            compare_generation_results_vs_ref(ov_prompts[i], ref[i], ov_results)


================================================================================

FILE: create_context.py
----------------------------------------
#!/usr/bin/env python3
"""
LLM Project Context Generator
============================

Python script to create a consolidated context file for the enhanced Phi-3 chat system.
This replaces the batch file functionality with cross-platform Python.
"""

import os
import glob
from pathlib import Path
from datetime import datetime


def should_exclude_path(path_str):
    """Check if a path should be excluded from context generation"""
    exclude_patterns = [
        '__pycache__', '.pytest_cache', '.git', 'node_modules', '.venv', 
        'cache', '.cache', 'combined_project', 'combined_llm_context',
        '.vs', '.vscode', 'bin', 'obj'
    ]
    
    path_lower = path_str.lower()
    return any(pattern in path_lower for pattern in exclude_patterns)


def get_project_files(project_root):
    """Get all relevant project files"""
    include_extensions = {
        '.py', '.json', '.txt', '.md', '.js', '.css', '.html', 
        '.bat', '.sh', '.yml', '.yaml', '.gitignore', '.dockerfile'
    }
    
    project_files = []
    
    for root, dirs, files in os.walk(project_root):
        # Skip excluded directories
        dirs[:] = [d for d in dirs if not should_exclude_path(os.path.join(root, d))]
        
        for file in files:
            file_path = os.path.join(root, file)
            file_ext = os.path.splitext(file)[1].lower()
            
            if file_ext in include_extensions or file in ['.gitignore', 'Dockerfile']:
                if not should_exclude_path(file_path):
                    try:
                        # Check file size (skip very large files)
                        if os.path.getsize(file_path) < 1024 * 1024:  # 1MB limit
                            project_files.append(file_path)
                    except (OSError, IOError):
                        continue
    
    return sorted(project_files)


def create_context_file(project_root):
    """Create the combined context file"""
    output_file = os.path.join(project_root, 'combined_llm_context.txt')
    
    print(f"Creating LLM context file: {output_file}")
    print(f"Scanning project directory: {project_root}")
    
    project_files = get_project_files(project_root)
    
    with open(output_file, 'w', encoding='utf-8') as out_file:
        # Write header
        out_file.write("=" * 80 + "\n")
        out_file.write("ENHANCED PHI-3 CHAT SYSTEM - COMPLETE PROJECT CONTEXT\n")
        out_file.write("=" * 80 + "\n")
        out_file.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        out_file.write(f"Project Root: {project_root}\n")
        out_file.write(f"Total Files: {len(project_files)}\n")
        out_file.write("=" * 80 + "\n\n")
        
        # Write summary of strategic roadmap implementation
        out_file.write("STRATEGIC ROADMAP IMPLEMENTATION STATUS:\n")
        out_file.write("✅ Phase 1: Foundational Refactoring & Consolidation\n")
        out_file.write("✅ Phase 2: Productionization & Scalability\n") 
        out_file.write("✅ Phase 3: Advanced AI Capabilities\n")
        out_file.write("   ✅ Phase 3.1: Advanced Document Parsing (unstructured)\n")
        out_file.write("   ✅ Phase 3.2: Cross-Encoder Reranking (BAAI/bge-reranker-base)\n")
        out_file.write("   ✅ Phase 3.3: Agentic Architecture (ReAct + function-calling)\n")
        out_file.write("\nThe application has been successfully transformed from prototype to\n")
        out_file.write("production-ready AI system with state-of-the-art capabilities.\n\n")
        out_file.write("=" * 80 + "\n\n")
        
        # Process each file
        included_count = 0
        skipped_count = 0
        
        for file_path in project_files:
            try:
                # Get relative path for cleaner output
                rel_path = os.path.relpath(file_path, project_root)
                
                # Read file content
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    try:
                        with open(file_path, 'r', encoding='latin-1') as f:
                            content = f.read()
                    except UnicodeDecodeError:
                        print(f"Skipping binary file: {rel_path}")
                        skipped_count += 1
                        continue
                
                # Write file header
                out_file.write(f"FILE: {rel_path}\n")
                out_file.write("-" * 40 + "\n")
                out_file.write(content)
                out_file.write("\n\n" + "=" * 80 + "\n\n")
                
                included_count += 1
                if included_count % 10 == 0:
                    print(f"Processed {included_count} files...")
                    
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
                skipped_count += 1
                continue
    
    print(f"\n✅ Context file created successfully!")
    print(f"📁 Output: {output_file}")
    print(f"📊 Included {included_count} files")
    print(f"⚠️ Skipped {skipped_count} files")
    
    return output_file


if __name__ == "__main__":
    # Get current directory as project root
    project_root = os.getcwd()
    create_context_file(project_root)

================================================================================

FILE: docker-compose.yml
----------------------------------------
version: '3.8'

services:
  phi3-chat:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "7860:7860"
    volumes:
      # Mount model directory (adjust path as needed)
      - "${MODEL_PATH:-./models}:/app/models:ro"
      # Mount cache for persistence
      - "./cache:/app/cache"
      # Development: mount source code
      - "./app:/app/app:ro"
      - "./main.py:/app/main.py:ro"
    environment:
      - MODEL_PATH=${MODEL_PATH:-./models/phi3-128k-npu}
      - TARGET_DEVICE=${TARGET_DEVICE:-AUTO}
      - NPU_PROFILE=${NPU_PROFILE:-balanced}
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
    # Enable access to host devices (for NPU)
    privileged: true
    devices:
      - "/dev/accel:/dev/accel"  # NPU device access
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Optional: Development environment
  phi3-chat-dev:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "7861:7860"
    volumes:
      - "./:/app"  # Full source mount for development
    environment:
      - MODEL_PATH=${MODEL_PATH:-./models/phi3-128k-npu}
      - TARGET_DEVICE=CPU  # Safer for development
      - GRADIO_SERVER_NAME=0.0.0.0
    command: ["python3", "main.py", "--device", "CPU", "--debug"]
    profiles:
      - dev

================================================================================

FILE: export_model_for_npu.py
----------------------------------------
#!/usr/bin/env python3
"""
NPU-Compatible Model Export Script
==================================

This script exports language models (Phi-3, Qwen, etc.) with NPU-specific optimizations:
- Static shapes for KV-cache (required for NPU)
- Symmetric INT4 quantization
- Stateful KV-cache implementation
- Channel-wise quantization for >1B parameter models

Usage:
    python export_model_for_npu.py --model microsoft/Phi-3-mini-128k-instruct --output phi3-128k-npu
    python export_model_for_npu.py --model Qwen/Qwen2.5-7B-Instruct --output qwen2.5-7b-npu
"""

import argparse
import os
import sys
from pathlib import Path

def check_requirements():
    """Check if required packages are installed"""
    try:
        import optimum.intel
        import openvino
        import transformers
        print("✓ All required packages found")
    except ImportError as e:
        print(f"❌ Missing required package: {e}")
        print("Install with: pip install optimum-intel transformers")
        return False
    return True

def export_model_for_npu(model_name, output_dir, max_seq_len=2048, min_response_len=256):
    """
    Export language model optimized for NPU inference
    
    Args:
        model_name: Hugging Face model name (e.g., "microsoft/Phi-3-mini-128k-instruct") 
        output_dir: Directory to save the exported model
        max_seq_len: Maximum sequence length for static shapes
        min_response_len: Minimum response length for NPU optimization
    """
    
    print(f"🚀 Exporting {model_name} for NPU...")
    print(f"📁 Output directory: {output_dir}")
    print(f"⚙️  Max sequence length: {max_seq_len}")
    print(f"⚙️  Min response length: {min_response_len}")
    
    # Ensure output directory exists
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # NPU Export Command - Critical parameters for NPU compatibility
    cmd = [
        "optimum-cli", "export", "openvino",
        "--model", model_name,
        "--task", "text-generation-with-past",  # Enable KV-cache
        "--weight-format", "int4",              # NPU-optimized quantization
        "--sym",                                # Symmetric quantization (NPU preferred)
        "--group-size", "-1",                   # Channel-wise for >1B models
        "--ratio", "1.0",                       # Full model quantization
        "--trust-remote-code",                  # For models requiring custom code
        output_dir
    ]
    
    print("\n🔄 Running export command:")
    print(" ".join(cmd))
    print("\n" + "="*60)
    
    # Execute the export
    import subprocess
    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print("✓ Export completed successfully!")
        print(result.stdout)
        
        # Create NPU configuration file
        create_npu_config_file(output_dir, max_seq_len, min_response_len)
        
        print(f"\n🎉 NPU-compatible model exported to: {output_dir}")
        print(f"📋 Configuration saved to: {output_dir}/npu_config.py")
        
    except subprocess.CalledProcessError as e:
        print(f"❌ Export failed: {e}")
        print("STDOUT:", e.stdout)
        print("STDERR:", e.stderr)
        return False
    
    return True

def create_npu_config_file(output_dir, max_seq_len, min_response_len):
    """Create a configuration file with NPU-specific settings"""
    
    config_content = f'''# NPU Configuration for Exported Model
# Generated automatically - use with your NPU pipeline

import openvino_genai as ov_genai
import openvino.properties as props
import openvino.properties.hint as hints

# NPU Pipeline-specific configuration (critical for compilation)
pipeline_config = {{
    "MAX_PROMPT_LEN": {max_seq_len},        # Must match export static shape
    "MIN_RESPONSE_LEN": {min_response_len}   # Minimum response tokens
}}

# NPUW (NPU Wrapper) configuration for compilation success
npuw_config = {{
    "NPU_USE_NPUW": "YES",                    # Enable NPU Wrapper
    "NPUW_LLM": "YES",                        # Enable LLM-specific optimizations
    "NPUW_LLM_BATCH_DIM": 0,                  # Batch dimension index
    "NPUW_LLM_SEQ_LEN_DIM": 1,               # Sequence dimension index
    "NPUW_LLM_MAX_PROMPT_LEN": {max_seq_len}, # Must match MAX_PROMPT_LEN
    "NPUW_LLM_MIN_RESPONSE_LEN": {min_response_len}, # Must match MIN_RESPONSE_LEN
}}

# Device configuration for performance
device_config = {{
    hints.performance_mode: hints.PerformanceMode.LATENCY,
    props.cache_dir: ".npu_cache"
}}

def create_pipeline(model_path, device="NPU"):
    """Create optimized NPU pipeline with this exported model"""
    
    # Combine all configurations
    all_config = {{**device_config, **pipeline_config, **npuw_config}}
    
    try:
        print("Loading NPU-optimized model...")
        pipe = ov_genai.LLMPipeline(model_path, device, **all_config)
        print("✓ Successfully loaded model on NPU")
        return pipe
    except Exception as e:
        print(f"❌ Failed to load on NPU: {{e}}")
        print("Trying fallback configurations...")
        
        # Fallback: Try minimal NPUW config
        try:
            minimal_config = {{**device_config, **npuw_config}}
            pipe = ov_genai.LLMPipeline(model_path, device, **minimal_config)
            print("✓ Loaded with minimal NPUW configuration")
            return pipe
        except Exception as e2:
            print(f"❌ All NPU configurations failed: {{e2}}")
            return None

# Usage example:
# pipe = create_pipeline(r"{output_dir}")
# response = pipe.generate("Hello, how are you?")
'''
    
    config_path = Path(output_dir) / "npu_config.py"
    with open(config_path, 'w') as f:
        f.write(config_content)
    
    print(f"📄 Created NPU configuration: {config_path}")

def main():
    parser = argparse.ArgumentParser(description="Export language models for NPU inference")
    parser.add_argument("--model", "-m", 
                       default="microsoft/Phi-3-mini-128k-instruct",
                       help="Hugging Face model name")
    parser.add_argument("--output", "-o", 
                       default="phi3-npu-model",
                       help="Output directory")
    parser.add_argument("--max-seq-len", 
                       type=int, default=2048,
                       help="Maximum sequence length (static shape)")
    parser.add_argument("--min-response", 
                       type=int, default=256,
                       help="Minimum response length for NPU")
    
    args = parser.parse_args()
    
    print("🔧 NPU Model Export Tool")
    print("=" * 50)
    
    # Check requirements
    if not check_requirements():
        sys.exit(1)
    
    # Export the model
    success = export_model_for_npu(
        args.model, 
        args.output,
        args.max_seq_len,
        args.min_response
    )
    
    if success:
        print("\n✅ Export completed successfully!")
        print(f"🚀 Model ready for NPU inference: {args.output}")
        print("\n📖 Next steps:")
        print(f"1. Test with: python -c \"from {args.output}.npu_config import create_pipeline; create_pipeline(r'{args.output}')\"")
        print(f"2. Update your Gradio script to use: {args.output}")
    else:
        print("\n❌ Export failed. Check error messages above.")
        sys.exit(1)

if __name__ == "__main__":
    main()

================================================================================

FILE: main.py
----------------------------------------
#!/usr/bin/env python3
"""
Enhanced Phi-3 OpenVINO GenAI Chat Application
==============================================

Main entry point for the modular, production-ready implementation of Phi-3-mini-128k-instruct
chat interface using OpenVINO GenAI with Intel NPU optimization and RAG capabilities.

Copyright (c) 2025 sbran
Licensed under the MIT License - see LICENSE file for details

Usage:
    python main.py                          # Use defaults from config.json
    python main.py --device CPU             # Force CPU device
    python main.py --model-path /path/model # Use custom model path
    python main.py --help                   # Show all options

Features:
- Complete Phi-3 NPUW optimization with 128k context support
- Dynamic system prompt configuration
- RAG document processing
- Professional performance monitoring  
- Robust error handling and diagnostics
- Modular architecture for maintainability
"""

import argparse
import os
import sys
from typing import Optional

# Import application modules
from app.config import initialize_config
from app.model import initialize_system_with_validation
from app.chat import initialize_globals as init_chat_globals
from app.ui import create_enhanced_interface, initialize_ui_globals
from app.streamer import streaming_metrics


def create_argument_parser() -> argparse.ArgumentParser:
    """
    Create command-line argument parser with comprehensive options.
    
    Returns:
        Configured ArgumentParser instance
    """
    parser = argparse.ArgumentParser(
        description="Enhanced Phi-3 OpenVINO GenAI Chat Application",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py                              # Use config.json defaults
  python main.py --device CPU                 # Force CPU device  
  python main.py --npu-profile conservative  # Use conservative NPU settings
  python main.py --model-path ./models/qwen3 # Custom model location
  python main.py --share                     # Enable public sharing (use with caution)
  python main.py --port 8080                 # Use custom port

Configuration Priority:
  1. Command-line arguments (highest priority)
  2. Environment variables
  3. config.json file
  4. Built-in defaults (lowest priority)

Environment Variables:
  QWEN3_MODEL_PATH     - Model directory path
  TARGET_DEVICE        - Target device (NPU, CPU, GPU, AUTO)
  NPU_PROFILE          - NPU optimization profile
  CACHE_DIR            - Cache directory location
  MAX_MESSAGE_LENGTH   - Maximum message length
  GENERATION_TIMEOUT   - Generation timeout in seconds
  GRADIO_SHARE         - Enable public sharing (true/false)
        """
    )
    
    # Model configuration
    model_group = parser.add_argument_group("Model Configuration")
    model_group.add_argument(
        "--model-path",
        type=str,
        help="Path to the OpenVINO model directory (overrides config.json and env var)"
    )
    
    # Device configuration
    device_group = parser.add_argument_group("Device Configuration")
    device_group.add_argument(
        "--device",
        type=str,
        choices=["NPU", "CPU", "GPU", "AUTO"],
        help="Target device for inference (default: NPU)"
    )
    device_group.add_argument(
        "--npu-profile",
        type=str,
        choices=["conservative", "balanced", "aggressive"],
        help="NPU optimization profile (default: balanced)"
    )
    device_group.add_argument(
        "--cache-dir",
        type=str,
        help="OpenVINO cache directory path"
    )
    
    # Application configuration
    app_group = parser.add_argument_group("Application Configuration")
    app_group.add_argument(
        "--config",
        type=str,
        default="config.json",
        help="Path to configuration file (default: config.json)"
    )
    app_group.add_argument(
        "--max-message-length",
        type=int,
        help="Maximum message length in characters"
    )
    app_group.add_argument(
        "--generation-timeout",
        type=float,
        help="Generation timeout in seconds"
    )
    
    # Gradio interface configuration
    gradio_group = parser.add_argument_group("Interface Configuration")
    gradio_group.add_argument(
        "--host",
        type=str,
        default="127.0.0.1",
        help="Host to bind the interface to (default: 127.0.0.1)"
    )
    gradio_group.add_argument(
        "--port",
        type=int,
        default=7860,
        help="Port to bind the interface to (default: 7860)"
    )
    gradio_group.add_argument(
        "--share",
        action="store_true",
        help="Enable public sharing via Gradio (WARNING: Security risk)"
    )
    gradio_group.add_argument(
        "--auth",
        type=str,
        help="Basic authentication (format: username:password)"
    )
    gradio_group.add_argument(
        "--max-file-size",
        type=str,
        default="10mb",
        help="Maximum file upload size (default: 10mb)"
    )
    
    # Development options
    dev_group = parser.add_argument_group("Development Options")
    dev_group.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug mode with verbose logging"
    )
    dev_group.add_argument(
        "--validate-only",
        action="store_true",
        help="Only validate system requirements, don't start interface"
    )
    dev_group.add_argument(
        "--reset-metrics",
        action="store_true",
        help="Reset performance metrics on startup"
    )
    
    return parser


def validate_arguments(args: argparse.Namespace) -> None:
    """
    Validate command-line arguments.
    
    Args:
        args: Parsed command-line arguments
        
    Raises:
        SystemExit: If validation fails
    """
    # Validate model path if provided
    if args.model_path and not os.path.exists(args.model_path):
        print(f"❌ Error: Model path does not exist: {args.model_path}")
        sys.exit(1)
    
    # Validate cache directory if provided
    if args.cache_dir:
        cache_parent = os.path.dirname(args.cache_dir)
        if cache_parent and not os.path.exists(cache_parent):
            try:
                os.makedirs(cache_parent, exist_ok=True)
            except (PermissionError, OSError) as e:
                print(f"❌ Error: Cannot create cache directory parent: {cache_parent} ({e})")
                sys.exit(1)
    
    # Validate authentication format
    if args.auth and ':' not in args.auth:
        print("❌ Error: Authentication must be in format 'username:password'")
        sys.exit(1)
    
    # Validate port range
    if not (1024 <= args.port <= 65535):
        print(f"❌ Error: Port must be between 1024 and 65535, got {args.port}")
        sys.exit(1)
    
    # Warn about security risks
    if args.share:
        print("⚠️ WARNING: Public sharing enabled. Your application will be accessible from the internet.")
        print("   Ensure you trust all users who might access it.")
    
    if args.host != "127.0.0.1":
        print(f"⚠️ WARNING: Binding to {args.host}. Make sure your firewall is properly configured.")


def setup_launch_config(args: argparse.Namespace) -> dict:
    """
    Setup Gradio launch configuration from arguments.
    
    Args:
        args: Parsed command-line arguments
        
    Returns:
        Dictionary with launch configuration
    """
    launch_config = {
        "server_name": args.host,
        "server_port": args.port,
        "share": args.share or os.getenv("GRADIO_SHARE", "").lower() in ('true', '1', 'yes'),
        "show_error": True,
        "quiet": not args.debug,
        "max_file_size": args.max_file_size,
        "allowed_paths": []  # No file access by default for security
    }
    
    # Add authentication if provided
    if args.auth:
        username, password = args.auth.split(':', 1)
        launch_config["auth"] = (username, password)
        print(f"✅ Basic authentication enabled for user: {username}")
    
    return launch_config


def main():
    """Main application entry point"""
    
    # Parse command-line arguments
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # Validate arguments
    validate_arguments(args)
    
    # Setup debug logging if requested
    if args.debug:
        print("🔧 Debug mode enabled")
        import logging
        logging.basicConfig(level=logging.DEBUG)
    
    print("🚀 Starting Enhanced Phi-3 Chat Application")
    print("=" * 60)
    
    try:
        # Initialize configuration system
        print("🔧 Loading configuration...")
        config = initialize_config(args.config, args)
        
        if args.validate_only:
            print("🔍 Validation-only mode - checking system requirements...")
            # Import validation function
            from app.model import validate_system_requirements
            issues = validate_system_requirements()
            
            if issues:
                print("❌ Validation failed:")
                for i, issue in enumerate(issues, 1):
                    print(f"   {i}. {issue}")
                sys.exit(1)
            else:
                print("✅ All system requirements validated successfully")
                sys.exit(0)
        
        # Reset metrics if requested
        if args.reset_metrics:
            streaming_metrics.reset()
            print("📊 Performance metrics reset")
        
        # Initialize system with validation
        print("🚀 Initializing system...")
        pipeline, tokenizer, device_used, config_used, load_time = initialize_system_with_validation()
        
        # Initialize global instances
        init_chat_globals(pipeline, tokenizer)
        initialize_ui_globals(pipeline, device_used, config_used, load_time)
        
        # Create Gradio interface
        print("🌐 Creating Gradio interface...")
        demo = create_enhanced_interface()
        
        # Setup launch configuration
        launch_config = setup_launch_config(args)
        
        # Display startup information
        print("✨ Enhanced Phi-3 Chat System Ready!")
        print("=" * 60)
        print("Features Enabled:")
        print("   🎯 Dynamic System Prompts")
        print("   📚 RAG Document Processing") 
        print("   🔍 Advanced Token Filtering")
        print("   📊 Real-time Performance Monitoring")
        print("   🛡️ Security & Input Validation")
        print("   🏗️ Modular Architecture")
        print("=" * 60)
        print(f"🌐 Starting server on {args.host}:{args.port}")
        print(f"🎯 Device: {device_used} | Config: {config_used}")
        print(f"⏱️ Load Time: {load_time:.1f}s")
        
        if launch_config["share"]:
            print("🔗 Public sharing enabled - link will be displayed after startup")
        
        print("=" * 60)
        
        # Launch the application
        demo.queue(
            max_size=20,
            default_concurrency_limit=1  # NPU works best with single concurrent requests
        ).launch(**launch_config)
        
    except KeyboardInterrupt:
        print("\n🛑 Application stopped by user")
        sys.exit(0)
    
    except SystemExit:
        # Re-raise SystemExit to preserve exit codes
        raise
    
    except Exception as e:
        print(f"💥 Unexpected error: {e}")
        
        if args.debug:
            import traceback
            traceback.print_exc()
        else:
            print("   Use --debug flag for detailed error information")
        
        print("\n🆘 This may be a configuration or system issue")
        print("   Check your model path, device drivers, and dependencies")
        sys.exit(1)


if __name__ == "__main__":
    main()

================================================================================

FILE: requirements.txt
----------------------------------------
# Enhanced Phi-3-mini-128k-instruct OpenVINO GenAI Chat Application
# Requirements for public repository usage

# Core Dependencies
openvino-genai>=2024.4.0
gradio>=4.0.0
transformers>=4.30.0

# Essential Python Libraries
numpy>=1.21.0
typing-extensions>=4.0.0

# Optional Performance Libraries (recommended)
psutil>=5.8.0  # For system monitoring and resource tracking

# RAG Dependencies (for document processing)
langchain>=0.1.0
langchain-huggingface>=0.1.0  # New recommended package for HuggingFace integrations
faiss-cpu>=1.7.4
sentence-transformers>=2.2.0

# Advanced Document Parsing (Phase 3.1)
unstructured[local-inference]==0.15.7
langchain-unstructured==0.1.2

# Cross-Encoder Reranking (Phase 3.2)
sentence-transformers>=2.7.0
torch>=2.0.0

# Agentic Architecture (Phase 3.3)
langchain-core>=0.1.0
langchain-experimental>=0.3.0
requests>=2.25.0  # For web search tool
python-dateutil>=2.8.0  # For date/time tools

# Development Dependencies (optional)
# Uncomment for development/testing:
pytest>=6.0.0
pytest-cov>=4.0.0
pytest-mock>=3.10.0
flake8>=4.0.0
bandit>=1.7.0
safety>=2.3.0
mypy>=1.0.0

# Note: OpenVINO toolkit should be installed separately following official instructions:
# https://docs.openvino.ai/latest/openvino_docs_install_guides_overview.html
#
# For NPU support, ensure Intel NPU drivers are installed:
# https://www.intel.com/content/www/us/en/products/docs/processors/core/core-processors-with-intel-npu.html
#
# Model Requirements:
# - microsoft/Phi-3-mini-128k-instruct in OpenVINO format
# - Download from official sources (HuggingFace)
# - Ensure compliance with model licensing terms

================================================================================

FILE: tests\__init__.py
----------------------------------------
"""
Test Suite for Enhanced OpenVINO GenAI Chat Application
======================================================

Unit tests and integration tests for the modular chat application.
"""

================================================================================

FILE: tests\test_agent.py
----------------------------------------
"""
Unit tests for Agent system (Phase 3.3)
========================================

Tests the agentic architecture with function-calling capabilities.
"""

import pytest
from unittest.mock import Mock, patch
from app.agent import AgentTools, ReActAgent, ToolResult, should_use_agent


class TestAgentTools:
    """Test cases for individual agent tools"""
    
    def test_calculator_basic(self):
        """Test basic calculator functionality"""
        result = AgentTools.calculator("2 + 2")
        assert result.success is True
        assert result.result == "4"
    
    def test_calculator_advanced(self):
        """Test advanced math functions"""
        result = AgentTools.calculator("sqrt(16)")
        assert result.success is True
        assert result.result == "4.0"
    
    def test_calculator_security(self):
        """Test calculator security filtering"""
        result = AgentTools.calculator("import os")
        assert result.success is False
        assert "unsafe" in result.error
    
    def test_datetime_current(self):
        """Test current datetime functionality"""
        result = AgentTools.datetime_info("now")
        assert result.success is True
        assert "date and time" in result.result.lower()
    
    def test_datetime_tomorrow(self):
        """Test tomorrow date calculation"""
        result = AgentTools.datetime_info("tomorrow")
        assert result.success is True
        assert "Tomorrow's date" in result.result
    
    def test_web_search_mock(self):
        """Test mock web search functionality"""
        result = AgentTools.web_search_mock("test query")
        assert result.success is True
        assert "Search results for: 'test query'" in result.result
        assert "mock" in result.result.lower()
    
    def test_text_analysis_count(self):
        """Test text analysis word counting"""
        result = AgentTools.text_analysis("Hello world", "count")
        assert result.success is True
        assert "Word count: 2" in result.result
        assert "Character count:" in result.result
    
    def test_text_analysis_summary(self):
        """Test text analysis summary"""
        result = AgentTools.text_analysis("The quick brown fox jumps over the lazy dog.", "summary")
        assert result.success is True
        assert "Words: 9" in result.result
        assert "Sentences:" in result.result


class TestAgentDecisionLogic:
    """Test cases for agent decision logic"""
    
    def test_should_use_agent_calculation(self):
        """Test agent detection for calculation requests"""
        assert should_use_agent("What's 2 + 2?") is True
        assert should_use_agent("Calculate the square root of 16") is True
        assert should_use_agent("Help me solve this math problem") is True
    
    def test_should_use_agent_datetime(self):
        """Test agent detection for datetime requests"""
        assert should_use_agent("What time is it?") is True
        assert should_use_agent("What's today's date?") is True
        assert should_use_agent("What's tomorrow?") is True
    
    def test_should_use_agent_search(self):
        """Test agent detection for search requests"""
        assert should_use_agent("Search for information about AI") is True
        assert should_use_agent("Look up the weather") is True
        assert should_use_agent("Find information about quantum computing") is True
    
    def test_should_use_agent_text_analysis(self):
        """Test agent detection for text analysis"""
        assert should_use_agent("Analyze this text for word count") is True
        assert should_use_agent("How many characters are in this sentence?") is True
    
    def test_should_not_use_agent_regular_chat(self):
        """Test that regular chat doesn't trigger agent"""
        assert should_use_agent("Hello, how are you?") is False
        assert should_use_agent("Tell me about machine learning") is False
        assert should_use_agent("Write a poem about nature") is False


class TestReActAgent:
    """Test cases for ReAct agent system"""
    
    @patch('app.agent.AGENT_AVAILABLE', True)
    def test_agent_initialization(self):
        """Test agent initialization with mocked dependencies"""
        mock_llm = Mock()
        mock_tokenizer = Mock()
        
        agent = ReActAgent(mock_llm, mock_tokenizer)
        
        assert agent.llm == mock_llm
        assert agent.tokenizer == mock_tokenizer
        assert len(agent.tools) == 4  # calculator, datetime, web_search, text_analysis
        assert agent.available is True
    
    def test_parse_action_valid(self):
        """Test parsing valid action from LLM response"""
        mock_llm = Mock()
        mock_tokenizer = Mock()
        agent = ReActAgent(mock_llm, mock_tokenizer)
        
        response = """I need to calculate this.
        
        Thought: I should use the calculator tool.
        Action: calculator
        Action Input: 2 + 2
        """
        
        result = agent._parse_action(response)
        assert result is not None
        assert result[0] == "calculator"
        assert result[1] == "2 + 2"
    
    def test_parse_action_invalid(self):
        """Test parsing response without action"""
        mock_llm = Mock()
        mock_tokenizer = Mock()
        agent = ReActAgent(mock_llm, mock_tokenizer)
        
        response = "This is just a regular response without any action."
        
        result = agent._parse_action(response)
        assert result is None
    
    def test_execute_tool_calculator(self):
        """Test tool execution for calculator"""
        mock_llm = Mock()
        mock_tokenizer = Mock()
        agent = ReActAgent(mock_llm, mock_tokenizer)
        
        result = agent._execute_tool("calculator", "5 * 6")
        assert result.success is True
        assert result.result == "30"
    
    def test_execute_tool_invalid(self):
        """Test execution of invalid tool"""
        mock_llm = Mock()
        mock_tokenizer = Mock()
        agent = ReActAgent(mock_llm, mock_tokenizer)
        
        result = agent._execute_tool("nonexistent_tool", "input")
        assert result.success is False
        assert "not found" in result.error
    
    def test_get_available_tools(self):
        """Test getting available tools list"""
        mock_llm = Mock()
        mock_tokenizer = Mock()
        agent = ReActAgent(mock_llm, mock_tokenizer)
        
        tools = agent.get_available_tools()
        assert isinstance(tools, dict)
        assert "calculator" in tools
        assert "datetime" in tools
        assert "web_search" in tools
        assert "text_analysis" in tools


if __name__ == "__main__":
    pytest.main([__file__])

================================================================================

FILE: tests\test_config.py
----------------------------------------
"""
Unit tests for ConfigurationLoader class
========================================

Tests configuration loading and priority system.
"""

import pytest
import tempfile
import json
import os
from unittest.mock import patch, mock_open
from app.config import ConfigurationLoader


class TestConfigurationLoader:
    """Test cases for configuration management"""
    
    def test_default_config_loading(self):
        """Test loading of default configuration values"""
        with patch('os.path.exists', return_value=False):
            config = ConfigurationLoader()
            
            # Test default model configuration
            model_path = config.get("model", "path")
            assert "phi3-128k-npu" in model_path
            assert config.get("model", "type") == "phi3"
            
            # Test default deployment settings
            assert config.get("deployment", "target_device") == "NPU"
            assert config.get("deployment", "npu_profile") == "balanced"
    
    def test_json_config_loading(self):
        """Test loading configuration from JSON file"""
        test_config = {
            "model": {
                "path": "/test/model/path",
                "name": "Test-Model",
                "type": "test"
            },
            "deployment": {
                "target_device": "CPU",
                "npu_profile": "conservative"
            }
        }
        
        config_content = json.dumps(test_config)
        
        with patch('os.path.exists', return_value=True):
            with patch('builtins.open', mock_open(read_data=config_content)):
                config = ConfigurationLoader()
                
                assert config.get("model", "path") == "/test/model/path"
                assert config.get("model", "type") == "test"
                assert config.get("deployment", "target_device") == "CPU"
    
    @patch.dict(os.environ, {
        'MODEL_PATH': '/env/model/path',
        'TARGET_DEVICE': 'GPU',
        'NPU_PROFILE': 'aggressive'
    })
    def test_environment_override(self):
        """Test environment variable override"""
        with patch('os.path.exists', return_value=False):
            config = ConfigurationLoader()
            
            # Environment variables should override defaults
            assert config.get("model", "path") == "/env/model/path"
            assert config.get("deployment", "target_device") == "GPU"
            assert config.get("deployment", "npu_profile") == "aggressive"
    
    @patch.dict(os.environ, {
        'QWEN3_MODEL_PATH': '/legacy/model/path'
    })
    def test_legacy_env_variable_support(self):
        """Test backward compatibility with legacy environment variables"""
        with patch('os.path.exists', return_value=False):
            config = ConfigurationLoader()
            
            # Legacy QWEN3_MODEL_PATH should still work
            assert config.get("model", "path") == "/legacy/model/path"
    
    @patch.dict(os.environ, {
        'MODEL_PATH': '/new/model/path',
        'QWEN3_MODEL_PATH': '/legacy/model/path'
    })
    def test_env_variable_priority(self):
        """Test that new environment variables take precedence over legacy ones"""
        with patch('os.path.exists', return_value=False):
            config = ConfigurationLoader()
            
            # New MODEL_PATH should override legacy QWEN3_MODEL_PATH
            assert config.get("model", "path") == "/new/model/path"
    
    def test_nested_config_access(self):
        """Test nested configuration value access"""
        test_config = {
            "ui": {
                "performance": {
                    "timeout": 45.0
                }
            }
        }
        
        config_content = json.dumps(test_config)
        
        with patch('os.path.exists', return_value=True):
            with patch('builtins.open', mock_open(read_data=config_content)):
                config = ConfigurationLoader()
                
                # Test nested access
                section = config.get_section("ui")
                assert section["performance"]["timeout"] == 45.0
    
    def test_missing_config_fallback(self):
        """Test fallback when configuration keys are missing"""
        with patch('os.path.exists', return_value=False):
            config = ConfigurationLoader()
            
            # Should return default for missing keys
            missing_value = config.get("nonexistent", "key", "default_value")
            assert missing_value == "default_value"
    
    def test_invalid_json_handling(self):
        """Test handling of invalid JSON configuration"""
        invalid_json = "{ invalid json content"
        
        with patch('os.path.exists', return_value=True):
            with patch('builtins.open', mock_open(read_data=invalid_json)):
                # Should fall back to defaults without crashing
                config = ConfigurationLoader()
                
                # Should still have default values
                assert config.get("model", "type") == "phi3"
    
    def test_configuration_priority_order(self):
        """Test the 4-tier configuration priority system"""
        # Setup: JSON config with base values
        json_config = {
            "model": {"path": "/json/path"},
            "deployment": {"target_device": "CPU"}
        }
        
        config_content = json.dumps(json_config)
        
        with patch('os.path.exists', return_value=True):
            with patch('builtins.open', mock_open(read_data=config_content)):
                with patch.dict(os.environ, {'MODEL_PATH': '/env/path'}):
                    config = ConfigurationLoader()
                    
                    # ENV should override JSON
                    assert config.get("model", "path") == "/env/path"
                    # JSON should override defaults
                    assert config.get("deployment", "target_device") == "CPU"

================================================================================

FILE: tests\test_input_validator.py
----------------------------------------
"""
Unit tests for InputValidator class
===================================

Tests security validation and sanitization logic.
"""

import pytest
from app.chat import InputValidator


class TestInputValidator:
    """Test cases for InputValidator security features"""
    
    def test_validate_message_valid(self):
        """Test valid message validation"""
        message = "Hello, how are you?"
        is_valid, reason = InputValidator.validate_message(message)
        assert is_valid is True
        assert reason == ""
    
    def test_validate_message_empty(self):
        """Test empty message rejection"""
        message = ""
        is_valid, reason = InputValidator.validate_message(message)
        assert is_valid is False
        assert "Empty or invalid message" in reason
        
        message = None
        is_valid, reason = InputValidator.validate_message(message)
        assert is_valid is False
        assert "Empty or invalid message" in reason
    
    def test_validate_message_too_long(self):
        """Test overly long message rejection"""
        message = "x" * 10001  # Exceeds 10000 char limit
        is_valid, reason = InputValidator.validate_message(message)
        assert is_valid is False
        assert "exceeds maximum length" in reason
    
    def test_validate_message_script_injection(self):
        """Test script injection detection"""
        malicious_messages = [
            "<script>alert('xss')</script>",
            "<SCRIPT>malicious code</SCRIPT>",
            "javascript:void(0)",
            "data:text/html;base64,PHNjcmlwdD5hbGVydCgneHNzJyk8L3NjcmlwdD4=",
            "eval('malicious code')",
            "exec('malicious code')"
        ]
        
        for message in malicious_messages:
            is_valid, reason = InputValidator.validate_message(message)
            assert is_valid is False, f"Should reject: {message}"
            assert "potentially unsafe content" in reason
    
    def test_validate_message_excessive_special_chars(self):
        """Test excessive special character detection"""
        message = "!!!!!@@@@@#####$$$$$%%%%%"  # >50% special chars
        is_valid, reason = InputValidator.validate_message(message)
        assert is_valid is False
        assert "excessive special characters" in reason
    
    def test_sanitize_message_basic(self):
        """Test basic message sanitization"""
        message = "  Hello   world  "
        sanitized = InputValidator.sanitize_message(message)
        assert sanitized == "Hello world"
    
    def test_sanitize_message_control_chars(self):
        """Test control character removal"""
        message = "Hello\x00\x01world\x1f"
        sanitized = InputValidator.sanitize_message(message)
        assert sanitized == "Helloworld"
    
    def test_sanitize_message_preserve_valid_chars(self):
        """Test preservation of valid newlines and tabs"""
        message = "Hello\nworld\ttab"
        sanitized = InputValidator.sanitize_message(message)
        assert "Hello" in sanitized
        assert "world" in sanitized
        assert "tab" in sanitized
    
    def test_sanitize_message_repeated_chars(self):
        """Test repeated character limitation"""
        message = "Hellooooooooooooooo world"  # >10 repeated 'o's
        sanitized = InputValidator.sanitize_message(message)
        # Should limit to 3 consecutive characters
        assert "ooo" in sanitized
        assert "oooo" not in sanitized
    
    def test_security_bypass_attempts(self):
        """Test various security bypass attempts"""
        # These should definitely be blocked (obvious attacks)
        obvious_attacks = [
            "<script>alert(1)</script>",
            "javascript:alert(1)", 
            "eval(alert(1))",
            "exec(malicious_code)"
        ]
        
        # These are encoded/obfuscated - current implementation may not catch all
        # (which is acceptable for a basic validator)
        encoded_attempts = [
            "javascript&#58;alert(1)",
            "java\nscript:alert(1)", 
            "&lt;script&gt;alert(1)&lt;/script&gt;",
            "eval&#40;alert(1)&#41;",
            "&#x3C;script&#x3E;alert(1)&#x3C;/script&#x3E;"
        ]
        
        # Test obvious attacks - these MUST be blocked
        for attack in obvious_attacks:
            is_valid, _ = InputValidator.validate_message(attack)
            assert is_valid is False, f"Should block obvious attack: {attack}"
        
        # Test encoded attempts - document current behavior
        # (This test shows what the validator currently catches)
        for attempt in encoded_attempts:
            is_valid, _ = InputValidator.validate_message(attempt)
            # We document the current behavior but don't require blocking
            # This serves as documentation of current security coverage

================================================================================

FILE: tests\test_model.py
----------------------------------------
"""
Unit tests for model deployment and configuration
================================================

Tests core model deployment logic without requiring actual models.
"""

import pytest
from unittest.mock import patch, MagicMock
from app.model import LLMConfigurationManager, validate_system_requirements


class TestLLMConfigurationManager:
    """Test configuration management for LLM deployment"""
    
    def test_npu_config_generation(self):
        """Test NPU configuration generation"""
        manager = LLMConfigurationManager("balanced")
        config = manager.get_npu_config()
        
        # Should contain essential NPU settings
        assert config["NPU_USE_NPUW"] == "YES"
        assert config["NPUW_LLM"] == "YES"
        assert config["NPUW_LLM_MAX_PROMPT_LEN"] == 8192  # Phi-3 optimized
        assert config["NPUW_LLM_PREFILL_HINT"] == "LATENCY"
        assert config["NPUW_LLM_GENERATE_HINT"] == "LATENCY"
    
    def test_cpu_config_generation(self):
        """Test CPU configuration generation"""
        manager = LLMConfigurationManager("balanced")
        config = manager.get_cpu_config()
        
        # Should contain CPU-specific settings
        assert config["MAX_PROMPT_LEN"] == 16384  # Larger for CPU
        assert config["MIN_RESPONSE_LEN"] == 512
        
        # Should have performance settings
        assert "PERFORMANCE_HINT" in config or "performance_mode" in str(config)
    
    def test_profile_differences(self):
        """Test that different profiles generate different configurations"""
        conservative = LLMConfigurationManager("conservative")
        aggressive = LLMConfigurationManager("aggressive")
        
        # Both should work without errors
        conservative_config = conservative.get_npu_config()
        aggressive_config = aggressive.get_npu_config()
        
        # Both should have required NPUW settings
        for config in [conservative_config, aggressive_config]:
            assert config["NPU_USE_NPUW"] == "YES"
            assert config["NPUW_LLM"] == "YES"


class TestSystemValidation:
    """Test system requirements validation"""
    
    @patch('os.path.exists')
    @patch('os.path.isdir')
    def test_validate_missing_model_path(self, mock_isdir, mock_exists):
        """Test validation when model path doesn't exist"""
        mock_exists.return_value = False
        mock_isdir.return_value = False
        
        with patch('app.config.get_config') as mock_config:
            mock_config.return_value.get.side_effect = lambda section, key, default=None: {
                ("model", "path"): "/nonexistent/path"
            }.get((section, key), default)
            
            issues = validate_system_requirements()
            assert len(issues) > 0
            assert any("does not exist" in issue for issue in issues)
    
    @patch('os.path.exists')
    @patch('os.path.isdir')
    def test_validate_missing_model_files(self, mock_isdir, mock_exists):
        """Test validation when OpenVINO model files are missing"""
        def exists_side_effect(path):
            if "openvino_model.xml" in path or "openvino_model.bin" in path:
                return False
            return True
        
        mock_exists.side_effect = exists_side_effect
        mock_isdir.return_value = True
        
        with patch('app.config.get_config') as mock_config:
            mock_config.return_value.get.side_effect = lambda section, key, default=None: {
                ("model", "path"): "/test/model/path"
            }.get((section, key), default)
            
            issues = validate_system_requirements()
            assert len(issues) > 0
            assert any("Missing OpenVINO model file" in issue for issue in issues)
    
    @patch('os.path.exists')
    @patch('os.path.isdir')
    @patch('os.makedirs')
    def test_validate_cache_directory_creation(self, mock_makedirs, mock_isdir, mock_exists):
        """Test cache directory creation during validation"""
        def exists_side_effect(path):
            if "cache" in path and ".ovcache" not in path:
                return False  # Cache parent doesn't exist
            return True
        
        mock_exists.side_effect = exists_side_effect
        mock_isdir.return_value = True
        
        with patch('app.config.get_config') as mock_config:
            mock_config.return_value.get.side_effect = lambda section, key, default=None: {
                ("model", "path"): "/test/model/path",
                ("deployment", "cache_directory"): "./cache/.ovcache_phi3"
            }.get((section, key), default)
            
            issues = validate_system_requirements()
            
            # Should attempt to create cache directory
            mock_makedirs.assert_called()
    
    @patch('os.path.exists')
    @patch('os.path.isdir')
    def test_validate_openvino_import(self, mock_isdir, mock_exists):
        """Test OpenVINO import validation"""
        mock_exists.return_value = True
        mock_isdir.return_value = True
        
        with patch('app.config.get_config') as mock_config:
            mock_config.return_value.get.side_effect = lambda section, key, default=None: {
                ("model", "path"): "/test/model/path",
                ("deployment", "target_device"): "NPU",
                ("deployment", "cache_directory"): "./cache/.ovcache_phi3"
            }.get((section, key), default)
            
            with patch('openvino.Core') as mock_core:
                mock_core.return_value.available_devices = ["NPU", "CPU"]
                
                issues = validate_system_requirements()
                
                # Should have no issues if everything is properly mocked
                device_issues = [i for i in issues if "not available" in i]
                assert len(device_issues) == 0

================================================================================

