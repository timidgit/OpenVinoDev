================================================================================
OpenVINO GenAI Context Consolidation
Generated: 09/08/2025  9:08:15,28
Total Files: 16 curated essential files
================================================================================

This file contains all essential OpenVINO GenAI reference files needed
for building robust, performance-optimized Gradio chat applications.

Directory Structure:
  python_samples/     (4 files) - Python API examples
  test_configs/       (3 files) - Configuration patterns
  core_cpp/          (3 files) - C++ implementation details  
  documentation/     (2 files) - Architecture guides
  python_bindings/   (3 files) - C++ to Python bindings
  README.md          (1 file)  - Usage guide

================================================================================


################################################################################
# python_samples DIRECTORY
################################################################################


--- FILE: benchmark_genai.py ---
Path: python_samples/benchmark_genai.py

```python 
# Copyright (C) 2023-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

import sys
import argparse
import openvino_genai as ov_genai
from openvino import get_version

def main():
    parser = argparse.ArgumentParser(description="Help command")
    parser.add_argument("-m", "--model", type=str, required=True, help="Path to model and tokenizers base directory")
    parser.add_argument("-p", "--prompt", type=str, default=None, help="Prompt")
    parser.add_argument("-pf", "--prompt_file", type=str, help="Read prompt from file")
    parser.add_argument("-nw", "--num_warmup", type=int, default=1, help="Number of warmup iterations")
    parser.add_argument("-n", "--num_iter", type=int, default=2, help="Number of iterations")
    parser.add_argument("-mt", "--max_new_tokens", type=int, default=20, help="Maximal number of new tokens")
    parser.add_argument("-d", "--device", type=str, default="CPU", help="Device")
    
    args = parser.parse_args()

    if args.prompt is not None and args.prompt_file is not None:
        raise RuntimeError(f'Prompt and prompt file should not exist together!')
    else:
        if args.prompt_file is not None:
            with open(args.prompt_file, 'r', encoding='utf-8') as f:
                prompt = [f.read()]
        else:
            prompt = ['The Sky is blue because'] if args.prompt is None else [args.prompt]
    if len(prompt) == 0:
        raise RuntimeError(f'Prompt is empty!')

    print(f'openvino runtime version: {get_version()}')

    # Perf metrics is stored in DecodedResults. 
    # In order to get DecodedResults instead of a string input should be a list.
    models_path = args.model
    device = args.device
    num_warmup = args.num_warmup
    num_iter = args.num_iter
    
    config = ov_genai.GenerationConfig()
    config.max_new_tokens = args.max_new_tokens

    scheduler_config = ov_genai.SchedulerConfig()
    scheduler_config.enable_prefix_caching = False
    scheduler_config.max_num_batched_tokens = sys.maxsize

    pipe = ov_genai.LLMPipeline(models_path, device, scheduler_config=scheduler_config)
    
    input_data = pipe.get_tokenizer().encode(prompt)
    prompt_token_size = input_data.input_ids.get_shape()[1]
    print(f"Prompt token size: {prompt_token_size}")

    for _ in range(num_warmup):
        pipe.generate(prompt, config)
    
    res = pipe.generate(prompt, config)
    perf_metrics = res.perf_metrics
    for _ in range(num_iter - 1):
        res = pipe.generate(prompt, config)
        perf_metrics += res.perf_metrics
    
    print(f"Output token size: {res.perf_metrics.get_num_generated_tokens()}")
    print(f"Load time: {perf_metrics.get_load_time():.2f} ms")
    print(f"Generate time: {perf_metrics.get_generate_duration().mean:.2f} ± {perf_metrics.get_generate_duration().std:.2f} ms")
    print(f"Tokenization time: {perf_metrics.get_tokenization_duration().mean:.2f} ± {perf_metrics.get_tokenization_duration().std:.2f} ms")
    print(f"Detokenization time: {perf_metrics.get_detokenization_duration().mean:.2f} ± {perf_metrics.get_detokenization_duration().std:.2f} ms")
    print(f"TTFT: {perf_metrics.get_ttft().mean:.2f} ± {perf_metrics.get_ttft().std:.2f} ms")
    print(f"TPOT: {perf_metrics.get_tpot().mean:.2f} ± {perf_metrics.get_tpot().std:.2f} ms")
    print(f"Throughput : {perf_metrics.get_throughput().mean:.2f} ± {perf_metrics.get_throughput().std:.2f} tokens/s")

if __name__ == "__main__":
    main()

```

================================================================================


--- FILE: chat_sample.py ---
Path: python_samples/chat_sample.py

```python 
#!/usr/bin/env python3
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

import argparse
import openvino_genai


def streamer(subword):
    print(subword, end='', flush=True)
    # Return flag corresponds whether generation should be stopped.
    return openvino_genai.StreamingStatus.RUNNING

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('model_dir', help='Path to the model directory')
    parser.add_argument('device', nargs='?', default='CPU', help='Device to run the model on (default: CPU)')
    args = parser.parse_args()

    device = args.device
    pipe = openvino_genai.LLMPipeline(args.model_dir, device)

    config = openvino_genai.GenerationConfig()
    config.max_new_tokens = 100

    pipe.start_chat()
    while True:
        try:
            prompt = input('question:\n')
        except EOFError:
            break
        pipe.generate(prompt, config, streamer)
        print('\n----------')
    pipe.finish_chat()


if '__main__' == __name__:
    main()

```

================================================================================


--- FILE: greedy_causal_lm.py ---
Path: python_samples/greedy_causal_lm.py

```python 
#!/usr/bin/env python3
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

import argparse
import openvino_genai


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('model_dir')
    parser.add_argument('prompt')
    args = parser.parse_args()

    device = 'CPU'  # GPU can be used as well
    pipe = openvino_genai.LLMPipeline(args.model_dir, device)

    config = openvino_genai.GenerationConfig()
    config.max_new_tokens = 100

    print(pipe.generate(args.prompt, config))


if '__main__' == __name__:
    main()

```

================================================================================


--- FILE: multinomial_causal_lm.py ---
Path: python_samples/multinomial_causal_lm.py

```python 
#!/usr/bin/env python3
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

import argparse
import openvino_genai
import queue
import threading
from typing import Union


class IterableStreamer(openvino_genai.StreamerBase):
    """
    A custom streamer class for handling token streaming and detokenization with buffering.

    Attributes:
        tokenizer (Tokenizer): The tokenizer used for encoding and decoding tokens.
        tokens_cache (list): A buffer to accumulate tokens for detokenization.
        text_queue (Queue): A synchronized queue for storing decoded text chunks.
        print_len (int): The length of the printed text to manage incremental decoding.
    """

    def __init__(self, tokenizer):
        """
        Initializes the IterableStreamer with the given tokenizer.

        Args:
            tokenizer (Tokenizer): The tokenizer to use for encoding and decoding tokens.
        """
        super().__init__()
        self.tokenizer = tokenizer
        self.tokens_cache = []
        self.text_queue = queue.Queue()
        self.print_len = 0
        self.decoded_lengths = []

    def __iter__(self):
        """
        Returns the iterator object itself.
        """
        return self

    def __next__(self):
        """
        Returns the next value from the text queue.

        Returns:
            str: The next decoded text chunk.

        Raises:
            StopIteration: If there are no more elements in the queue.
        """
        # get() will be blocked until a token is available.
        value = self.text_queue.get()
        if value is None:
            raise StopIteration
        return value

    def get_stop_flag(self):
        """
        Checks whether the generation process should be stopped or cancelled.

        Returns:
            openvino_genai.StreamingStatus: Always returns RUNNING in this implementation.
        """
        return openvino_genai.StreamingStatus.RUNNING

    def write_word(self, word: str):
        """
        Puts a word into the text queue.

        Args:
            word (str): The word to put into the queue.
        """
        self.text_queue.put(word)

    def write(self, token: Union[int, list[int]]) -> openvino_genai.StreamingStatus:
        """
        Processes a token and manages the decoding buffer. Adds decoded text to the queue.

        Args:
            token (Union[int, list[int]]): The token(s) to process.

        Returns:
            bool: True if generation should be stopped, False otherwise.
        """
        if type(token) is list:
            self.tokens_cache += token
            self.decoded_lengths += [-2 for _ in range(len(token) - 1)]
        else:
            self.tokens_cache.append(token)

        text = self.tokenizer.decode(self.tokens_cache)
        self.decoded_lengths.append(len(text))

        word = ""
        delay_n_tokens = 3
        if len(text) > self.print_len and "\n" == text[-1]:
            # Flush the cache after the new line symbol.
            word = text[self.print_len :]
            self.tokens_cache = []
            self.decoded_lengths = []
            self.print_len = 0
        elif len(text) > 0 and text[-1] == chr(65533):
            # Don't print incomplete text.
            self.decoded_lengths[-1] = -1
        elif len(self.tokens_cache) >= delay_n_tokens:
            self.compute_decoded_length_for_position(
                len(self.decoded_lengths) - delay_n_tokens
            )
            print_until = self.decoded_lengths[-delay_n_tokens]
            if print_until != -1 and print_until > self.print_len:
                # It is possible to have a shorter text after adding new token.
                # Print to output only if text length is increased and text is complete (print_until != -1).
                word = text[self.print_len : print_until]
                self.print_len = print_until
        self.write_word(word)

        stop_flag = self.get_stop_flag()
        if stop_flag != openvino_genai.StreamingStatus.RUNNING:
            # When generation is stopped from streamer then end is not called, need to call it here manually.
            self.end()

        return stop_flag

    def compute_decoded_length_for_position(self, cache_position: int):
        # decode was performed for this position, skippping
        if self.decoded_lengths[cache_position] != -2:
            return

        cache_for_position = self.tokens_cache[: cache_position + 1]
        text_for_position = self.tokenizer.decode(cache_for_position)

        if len(text_for_position) > 0 and text_for_position[-1] == chr(65533):
            # Mark text as incomplete
            self.decoded_lengths[cache_position] = -1
        else:
            self.decoded_lengths[cache_position] = len(text_for_position)

    def end(self):
        """
        Flushes residual tokens from the buffer and puts a None value in the queue to signal the end.
        """
        text = self.tokenizer.decode(self.tokens_cache)
        if len(text) > self.print_len:
            word = text[self.print_len :]
            self.write_word(word)
            self.tokens_cache = []
            self.print_len = 0
        self.text_queue.put(None)


class ChunkStreamer(IterableStreamer):

    def __init__(self, tokenizer, tokens_len):
        super().__init__(tokenizer)
        self.tokens_len = tokens_len

    def write(self, token: Union[int, list[int]]) -> openvino_genai.StreamingStatus:
        if (len(self.tokens_cache) + 1) % self.tokens_len == 0:
            return super().write(token)

        if type(token) is list:
            self.tokens_cache += token
            # -2 means no decode was done for this token position
            self.decoded_lengths += [-2 for _ in range(len(token))]
        else:
            self.tokens_cache.append(token)
            self.decoded_lengths.append(-2)

        return openvino_genai.StreamingStatus.RUNNING


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("model_dir")
    parser.add_argument("prompt")
    args = parser.parse_args()

    device = "CPU"  # GPU can be used as well
    tokens_len = 10  # chunk size
    pipe = openvino_genai.LLMPipeline(args.model_dir, device)

    text_print_streamer = ChunkStreamer(pipe.get_tokenizer(), tokens_len)

    def token_printer():
        # Getting next elements from iterable will be blocked until a new token is available.
        for word in text_print_streamer:
            print(word, end="", flush=True)

    printer_thread = threading.Thread(target=token_printer, daemon=True)
    printer_thread.start()

    config = openvino_genai.GenerationConfig()
    config.max_new_tokens = 100
    config.do_sample = True
    config.top_p = 0.9
    config.top_k = 30

    # Since the streamer is set, the results will be printed
    # every time a new token is generated and put into the streamer queue.
    pipe.generate(args.prompt, config, text_print_streamer)
    printer_thread.join()


if "__main__" == __name__:
    main()

```

================================================================================


################################################################################
# test_configs DIRECTORY
################################################################################


--- FILE: generation_config.py ---
Path: test_configs/generation_config.py

```python 
# Copyright (C) 2018-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# todo: CVS-162108: remove this file to habdle generation config directly in tests

from openvino_genai import GenerationConfig

def get_greedy() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.num_return_sequences = 1
    generation_config.max_new_tokens = 30
    return generation_config

def get_greedy_with_penalties() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.num_return_sequences = 1
    generation_config.presence_penalty = 2.0
    generation_config.frequency_penalty = 0.2
    generation_config.max_new_tokens = 30
    return generation_config

def get_beam_search() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.num_beam_groups = 3
    generation_config.num_beams = 6
    generation_config.diversity_penalty = 1
    generation_config.max_new_tokens = 30
    generation_config.num_return_sequences = 3
    generation_config.num_return_sequences = generation_config.num_beams
    return generation_config

def get_multinomial_temperature() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.8
    generation_config.num_return_sequences = 1
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_num_return_sequence() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.7
    generation_config.num_return_sequences = 3
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_top_p() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.num_return_sequences = 1
    generation_config.do_sample = True
    generation_config.temperature = 0.8
    generation_config.top_p = 0.9
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_top_k() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.num_return_sequences = 1
    generation_config.temperature = 0.8
    generation_config.top_k = 2
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_top_p_and_top_k() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.8
    generation_config.top_p = 0.9
    generation_config.num_return_sequences = 1
    generation_config.top_k = 2
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_repetition_penalty() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.num_return_sequences = 1
    generation_config.temperature = 0.8
    generation_config.repetition_penalty = 2.0
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_all_parameters() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.num_return_sequences = 4
    generation_config.temperature = 0.9
    generation_config.top_p = 0.8
    generation_config.top_k = 20
    generation_config.repetition_penalty = 2.0
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_frequence_penalty() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.8
    generation_config.frequency_penalty = 0.5
    generation_config.num_return_sequences = 1
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_temperature_and_presence_penalty() -> GenerationConfig:
    generation_config = GenerationConfig()
    generation_config.do_sample = True
    generation_config.temperature = 0.8
    generation_config.presence_penalty = 0.1
    generation_config.num_return_sequences = 1
    generation_config.max_new_tokens = 30
    return generation_config

def get_multinomial_max_and_min_token() -> GenerationConfig:
    multinomial = GenerationConfig()
    multinomial.do_sample = True
    multinomial.temperature = 0.9
    multinomial.top_p = 0.9
    multinomial.top_k = 20
    multinomial.num_return_sequences = 3
    multinomial.presence_penalty = 0.01
    multinomial.frequency_penalty = 0.1
    multinomial.min_new_tokens = 15
    multinomial.max_new_tokens = 30
    return multinomial

```

================================================================================


--- FILE: hugging_face.py ---
Path: test_configs/hugging_face.py

```python 
# Copyright (C) 2018-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

from os.path import sep
from pathlib import Path
from typing import Type
from functools import lru_cache

from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import GenerationConfig as HFGenerationConfig

from optimum.intel import OVModelForCausalLM, OVModelForFeatureExtraction, OVModelForSequenceClassification
from optimum.intel.openvino.modeling import OVModel

from huggingface_hub import hf_hub_download

from openvino import save_model
from openvino_genai import GenerationResult, GenerationConfig, StopCriteria
from openvino_tokenizers import convert_tokenizer

from utils.constants import get_default_llm_properties, extra_generate_kwargs, get_ov_cache_models_dir
from utils.network import retry_request
import pytest

def generation_config_to_hf(
    default_generation_config : HFGenerationConfig,
    generation_config : GenerationConfig
) -> HFGenerationConfig:
    if generation_config is None:
        return

    kwargs = {}
    kwargs['return_dict_in_generate'] = True

    # generic parameters
    kwargs['max_length'] = generation_config.max_length
    # has higher priority than 'max_length'
    kwargs['max_new_tokens'] = generation_config.max_new_tokens
    kwargs['min_new_tokens'] = generation_config.min_new_tokens
    if generation_config.stop_strings:
        kwargs['stop_strings'] = generation_config.stop_strings

    # copy default parameters
    kwargs['bos_token_id'] = default_generation_config.bos_token_id
    kwargs['pad_token_id'] = default_generation_config.pad_token_id

    if (generation_config.ignore_eos):
        kwargs['eos_token_id'] = []
    else:
        if len(generation_config.stop_token_ids) > 0:
            kwargs['eos_token_id'] = list(generation_config.stop_token_ids)
        elif generation_config.eos_token_id != -1:
            kwargs['eos_token_id'] = generation_config.eos_token_id
        else:
            kwargs['eos_token_id'] = default_generation_config.eos_token_id

    # copy penalties
    kwargs['repetition_penalty'] = generation_config.repetition_penalty

    if generation_config.is_beam_search():
        # beam search case
        kwargs['num_beam_groups'] = generation_config.num_beam_groups
        kwargs['num_beams'] = generation_config.num_beams
        kwargs['length_penalty'] = generation_config.length_penalty
        kwargs['no_repeat_ngram_size'] = generation_config.no_repeat_ngram_size
        kwargs['num_return_sequences'] = generation_config.num_return_sequences
        kwargs['output_scores'] = True

        if generation_config.num_beam_groups > 1:
            kwargs['diversity_penalty'] = generation_config.diversity_penalty

        # in OpenVINO GenAI this parameter is called stop_criteria,
        # while in HF it's called early_stopping.
        # HF values True, False and "never" correspond to OV GenAI values "EARLY", "HEURISTIC" and "NEVER"
        STOP_CRITERIA_MAP = {
            StopCriteria.NEVER: "never",
            StopCriteria.EARLY: True,
            StopCriteria.HEURISTIC: False
        }

        kwargs['early_stopping'] = STOP_CRITERIA_MAP[generation_config.stop_criteria]
    elif generation_config.is_multinomial():
        # mulitinomial
        kwargs['temperature'] = generation_config.temperature
        kwargs['top_k'] = generation_config.top_k
        kwargs['top_p'] = generation_config.top_p
        kwargs['do_sample'] = generation_config.do_sample
    else:
        # greedy
        pass

    hf_generation_config = HFGenerationConfig(**kwargs)
    return hf_generation_config

def run_hugging_face(
    opt_model,
    hf_tokenizer,
    prompts: list[str],
    generation_configs: list[GenerationConfig] | GenerationConfig,
) -> list[GenerationResult]:
    generation_results = []

    if type(generation_configs) is list:
        # process prompt by promp as we have multiple generation configs
        for prompt, generation_config in zip(prompts, generation_configs):
            hf_generation_config = generation_config_to_hf(opt_model.generation_config, generation_config)
            inputs = {}
            if hf_tokenizer.chat_template and generation_config.apply_chat_template:
                prompt = hf_tokenizer.apply_chat_template([{'role': 'user', 'content': prompt}], tokenize=False, add_generation_prompt=True)
                inputs = hf_tokenizer(prompt, return_tensors="pt", add_special_tokens=False)
            else:
                inputs = hf_tokenizer(prompt, return_tensors="pt")
            input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']
            prompt_len = 0 if generation_config.echo else input_ids.numel()

            generate_outputs = opt_model.generate(input_ids=input_ids, attention_mask=attention_mask, generation_config=hf_generation_config, tokenizer=hf_tokenizer, **extra_generate_kwargs())
            all_text_batch = hf_tokenizer.batch_decode([generated_ids[prompt_len:] for generated_ids in generate_outputs.sequences], skip_special_tokens=True)

            generation_result = GenerationResult()
            generation_result.m_generation_ids = all_text_batch
            # sequences_scores are available only for beam search case
            if generation_config.is_beam_search():
                generation_result.m_scores = [score for score in generate_outputs.sequences_scores]
            generation_results.append(generation_result)
    else:
        inputs = {}
        if hf_tokenizer.chat_template and generation_configs.apply_chat_template:
            processed_prompts = []
            for prompt in prompts:
                processed_prompts.append(hf_tokenizer.apply_chat_template([{'role': 'user', 'content': prompt}], tokenize=False, add_generation_prompt=True))
            # process all prompts as a single batch as we have a single generation config for all prompts
            inputs = hf_tokenizer(processed_prompts, return_tensors='pt', padding=True, truncation=True, add_special_tokens=False, padding_side='left')
        else:
            inputs = hf_tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, padding_side='left')
        input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']
        hf_generation_config = generation_config_to_hf(opt_model.generation_config, generation_configs)
        hf_encoded_outputs = opt_model.generate(input_ids, attention_mask=attention_mask, generation_config=hf_generation_config, tokenizer=hf_tokenizer, **extra_generate_kwargs())

        generation_ids = []
        scores = []

        for idx, hf_encoded_out in enumerate(hf_encoded_outputs.sequences):
            prompt_idx = idx // hf_generation_config.num_return_sequences
            prompt_len = 0 if generation_configs.echo else input_ids[prompt_idx].numel()
            decoded_text = hf_tokenizer.decode(hf_encoded_out[prompt_len:], skip_special_tokens=True)
            generation_ids.append(decoded_text)
            if generation_configs.is_beam_search():
                scores.append(hf_encoded_outputs.sequences_scores[idx])

            # if we need to move to next generation result
            if (idx + 1) // hf_generation_config.num_return_sequences != prompt_idx:
                generation_result = GenerationResult()
                generation_result.m_generation_ids = generation_ids
                generation_result.m_scores = scores
                generation_results.append(generation_result)
                generation_ids = []
                scores = []

    del hf_tokenizer
    del opt_model

    return generation_results


# download HF model or read converted model
def get_huggingface_models(model_id: str | Path, model_class: Type[OVModel], local_files_only=False):
    hf_tokenizer = retry_request(lambda: AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, local_files_only=local_files_only))
    opt_model = retry_request(lambda: model_class.from_pretrained(model_id, export=isinstance(model_id, str), compile=False, load_in_8bit=False, trust_remote_code=isinstance(model_id, str), ov_config=get_default_llm_properties(), local_files_only=local_files_only))
    return opt_model, hf_tokenizer


def convert_and_save_tokenizer(hf_tokenizer : AutoTokenizer,
                               models_path: Path,
                               **tokenizer_kwargs):
    tokenizer, detokenizer = convert_tokenizer(hf_tokenizer, with_detokenizer=True, **tokenizer_kwargs)

    from utils.constants import OV_DETOKENIZER_FILENAME, OV_TOKENIZER_FILENAME
    save_model(tokenizer, models_path / OV_TOKENIZER_FILENAME)
    save_model(detokenizer, models_path / OV_DETOKENIZER_FILENAME)


def convert_models(opt_model : OVModelForCausalLM,
                   hf_tokenizer : AutoTokenizer,
                   models_path: Path,
                   **tokenizer_kwargs):
    opt_model.save_pretrained(models_path)
    # save generation config
    if opt_model.generation_config:
        opt_model.generation_config.save_pretrained(models_path)
    opt_model.config.save_pretrained(models_path)

    # to store tokenizer config jsons with special tokens
    hf_tokenizer.save_pretrained(models_path)
    # convert tokenizers as well
    convert_and_save_tokenizer(hf_tokenizer, models_path)


def download_and_convert_model(model_id: str, **tokenizer_kwargs):
    return _download_and_convert_model(model_id, OVModelForCausalLM, **tokenizer_kwargs)

@pytest.fixture()
def download_and_convert_embeddings_models(request):
    model_id = request.param
    return _download_and_convert_model(model_id, OVModelForFeatureExtraction)


@pytest.fixture()
def download_and_convert_rerank_model(request):
    model_id = request.param
    opt_model, hf_tokenizer, models_path = _download_and_convert_model(model_id, OVModelForSequenceClassification)
    ov_tokenizer = convert_tokenizer(hf_tokenizer, with_detokenizer=False, number_of_inputs=2)
    save_model(ov_tokenizer, models_path / "openvino_tokenizer.xml")
    return opt_model, hf_tokenizer, models_path


def _download_and_convert_model(model_id: str, model_class: Type[OVModel], **tokenizer_kwargs):
    dir_name = str(model_id).replace(sep, "_")
    ov_cache_models_dir = get_ov_cache_models_dir()
    models_path = ov_cache_models_dir / dir_name

    from utils.constants import OV_MODEL_FILENAME
    if (models_path / OV_MODEL_FILENAME).exists():
        opt_model, hf_tokenizer = get_huggingface_models(models_path, model_class, local_files_only=True)
    else:
        opt_model, hf_tokenizer = get_huggingface_models(model_id, model_class, local_files_only=False)
        convert_models(opt_model, hf_tokenizer, models_path)

    if "padding_side" in tokenizer_kwargs:
        hf_tokenizer.padding_side = tokenizer_kwargs.pop("padding_side")

    return opt_model, hf_tokenizer, models_path


def download_gguf_model(gguf_model_id: str,
                        gguf_filename: str):
    gguf_dir_name = str(gguf_model_id).replace(sep, "_")
    ov_cache_models_dir = get_ov_cache_models_dir()
    models_path_gguf = ov_cache_models_dir / gguf_dir_name

    gguf_path = hf_hub_download(
        repo_id=gguf_model_id,
        filename=gguf_filename,
        local_dir=models_path_gguf # Optional: Specify download directory
    )

    return gguf_path

@lru_cache(maxsize=None)
def load_hf_model_from_gguf(gguf_model_id, gguf_filename):
    return retry_request(lambda: AutoModelForCausalLM.from_pretrained(gguf_model_id, gguf_file=gguf_filename))

@lru_cache(maxsize=None)
def load_hf_tokenizer_from_gguf(gguf_model_id, gguf_filename):
    return retry_request(lambda: AutoTokenizer.from_pretrained(gguf_model_id, gguf_file=gguf_filename))

```

================================================================================


--- FILE: ov_genai_pipelines.py ---
Path: test_configs/ov_genai_pipelines.py

```python 
# Copyright (C) 2018-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

from enum import Enum
from pathlib import Path
from typing import Callable
from shutil import rmtree

from optimum.intel.openvino.utils import TemporaryDirectory
from openvino_genai import SchedulerConfig, draft_model, ContinuousBatchingPipeline, \
    LLMPipeline, GenerationConfig, GenerationResult, StreamerBase, DecodedResults

from utils.constants import get_default_llm_properties
from utils.comparation import compare_generation_results, compare_generation_results_vs_ref
from utils.hugging_face import download_and_convert_model, run_hugging_face

def dict_to_scheduler_config(scheduler_params: dict = None) -> SchedulerConfig:
    scheduler_config = SchedulerConfig()
    if scheduler_params is None:
        scheduler_config.dynamic_split_fuse = True
        # vLLM specific
        scheduler_config.max_num_batched_tokens = 256
        scheduler_config.max_num_seqs = 256

        # Expedited number of blocks = text_blocks_n * G * n_prompts, where
        # text_blocks_n - number of blocks required for storing prompt and generated text,
        # currently it is 1 block for prompt (31 token with block_size 32) + 1 block for generated text (max length of generated text - 30 tokens);
        # G - number of sequences in a sequence group, for beam search it is 2(group_size) * 3 (num_groups);
        # n_prompts - number of prompts.
        # For current parameters in tests expedited number of blocks is approximately 48.
        scheduler_config.num_kv_blocks = 60
    else:
        for param, value in scheduler_params.items():
            setattr(scheduler_config, param, value)

    return scheduler_config


class PipelineType(Enum):
    STATEFUL = 1
    PAGED_ATTENTION = 2
    CONTINUOUS_BATCHING = 3
    SPECULATIVE_DECODING = 4
    PROMPT_LOOKUP_DECODING = 5
    AUTO = 6


def get_all_pipeline_types():
    return [PipelineType.STATEFUL, PipelineType.PAGED_ATTENTION, PipelineType.CONTINUOUS_BATCHING, PipelineType.SPECULATIVE_DECODING, PipelineType.PROMPT_LOOKUP_DECODING, PipelineType.AUTO]

def get_main_pipeline_types():
    return [PipelineType.STATEFUL, PipelineType.PAGED_ATTENTION, PipelineType.SPECULATIVE_DECODING, PipelineType.PROMPT_LOOKUP_DECODING]

def get_gguf_pipeline_types():
    return [PipelineType.STATEFUL, PipelineType.PAGED_ATTENTION]

class StreamerWithResults:
    # Return a streamer which accumulates results in order to compare with results returned from generate.
    results: list[str] = []
    def __init__(self):
        self.results = []

    def accumulate(self, subword) -> bool:
        self.results.append(subword)
        return False
    
    def get_results(self) -> list[GenerationResult]:
        streaming_result = GenerationResult()
        streaming_result.m_generation_ids = [''.join(self.results)]
        return [streaming_result]
    
    def reset(self):
        self.results = []


def create_ov_pipeline(models_path: Path,
                       pipeline_type: PipelineType = PipelineType.AUTO,
                       device: str = "CPU",
                       ov_config: dict = get_default_llm_properties(),
                       scheduler_config: SchedulerConfig = SchedulerConfig(),
                       draft_model_path: Path = None,
                       enable_save_ov_model: bool = None,
                       dynamic_quantization_group_size: str = None):
    local_ov_config = ov_config.copy()
    if pipeline_type == PipelineType.AUTO:
        return LLMPipeline(models_path, device, ov_config)
    elif pipeline_type == PipelineType.STATEFUL:
        if enable_save_ov_model is not None: local_ov_config["enable_save_ov_model"] = enable_save_ov_model
        if dynamic_quantization_group_size is not None: local_ov_config["DYNAMIC_QUANTIZATION_GROUP_SIZE"] = dynamic_quantization_group_size
        return LLMPipeline(models_path, device, local_ov_config, ATTENTION_BACKEND="SDPA")
    elif pipeline_type == PipelineType.PAGED_ATTENTION:
        if enable_save_ov_model is not None: local_ov_config["enable_save_ov_model"] = enable_save_ov_model
        if dynamic_quantization_group_size is not None: local_ov_config["DYNAMIC_QUANTIZATION_GROUP_SIZE"] = dynamic_quantization_group_size
        return LLMPipeline(models_path, device, local_ov_config, scheduler_config=scheduler_config, ATTENTION_BACKEND="PA")
    elif pipeline_type == PipelineType.CONTINUOUS_BATCHING:
        return ContinuousBatchingPipeline(models_path, scheduler_config, device, ov_config)
    elif pipeline_type == PipelineType.SPECULATIVE_DECODING:
        ov_draft_model = draft_model(models_path) if draft_model_path is None else draft_model(draft_model_path)
        return LLMPipeline(models_path, device, ov_config, scheduler_config=scheduler_config, draft_model=ov_draft_model)
    elif pipeline_type == PipelineType.PROMPT_LOOKUP_DECODING:
        return LLMPipeline(models_path, device, ov_config, scheduler_config=scheduler_config, prompt_lookup=True)
    else:
        raise Exception(f"Unsupported pipeline type: {pipeline_type}")

def create_ov_cb_pipeline(models_path: Path,
                       pipeline_type: PipelineType = PipelineType.AUTO,
                       device: str = "CPU",
                       ov_config: dict = get_default_llm_properties(),
                       scheduler_config: SchedulerConfig = SchedulerConfig(),
                       draft_model_path: Path = None):
    local_ov_config = ov_config.copy()
    if pipeline_type == PipelineType.CONTINUOUS_BATCHING:
        return ContinuousBatchingPipeline(models_path, scheduler_config, device, local_ov_config)
    elif pipeline_type == PipelineType.SPECULATIVE_DECODING:
        ov_draft_model = draft_model(models_path) if draft_model_path is None else draft_model(draft_model_path)
        local_ov_config["draft_model"] = ov_draft_model
        return ContinuousBatchingPipeline(models_path, scheduler_config, device, local_ov_config)
    elif pipeline_type == PipelineType.PROMPT_LOOKUP_DECODING:
        local_ov_config["prompt_lookup"] = True
        return ContinuousBatchingPipeline(models_path, scheduler_config, device, local_ov_config)
    else:
        raise Exception(f"Unsupported pipeline type: {pipeline_type}")


def prepare_generation_config_by_pipe_type(generation_config : GenerationConfig,
                                           pipeline_type: PipelineType = PipelineType.AUTO):
    if pipeline_type == PipelineType.SPECULATIVE_DECODING:
        assert not generation_config.is_beam_search()
        generation_config.assistant_confidence_threshold = 0.9
    elif pipeline_type == PipelineType.PROMPT_LOOKUP_DECODING:
        assert not generation_config.is_beam_search()
        generation_config.num_assistant_tokens = 5
        generation_config.max_ngram_size = 3
    return generation_config


def prepare_generation_configs_by_pipe_type(generation_configs : list[GenerationConfig],
                                            pipeline_type: PipelineType = PipelineType.AUTO):
    return [ prepare_generation_config_by_pipe_type(generation_config, pipeline_type) for generation_config in generation_configs ]


def convert_decoded_results_to_generation_result(generate_outputs: DecodedResults,
                                                 num_prompts: int,
                                                 num_return_sequences: int,
                                                 is_beam_search: bool) -> list[GenerationResult]:
    index = 0
    generation_results = []

    for _ in range(num_prompts):
        generation_result = GenerationResult()

        generation_result.m_generation_ids = generate_outputs.texts[index : index + num_return_sequences]
        # sequences_scores are available only for beam search case
        if is_beam_search:
            generation_result.m_scores = generate_outputs.scores[index : index + num_return_sequences]
        generation_results.append(generation_result)

        index += num_return_sequences
    return generation_results


def run_ov_pipeline(models_path: Path,
                    prompt : str | list[str],
                    generation_config : GenerationConfig | list[GenerationConfig],
                    pipeline_type : PipelineType = PipelineType.AUTO,
                    streamer: StreamerWithResults | Callable | StreamerBase = None,
                    scheduler_config: SchedulerConfig = SchedulerConfig(),
                    draft_model_path: Path = None,
                    ov_config: dict = {},
                    device: str = "CPU"
    ) -> list[GenerationResult]:
    # update the generation config according pipeline_type
    updated_generation_config = None
    if isinstance(generation_config, list):
        if pipeline_type != PipelineType.CONTINUOUS_BATCHING:
            raise Exception(f"\'generation_config\' is \'list[GenerationConfig]\'. This type is supported only for \'PipelineType.CONTINIOUS_BATCHING\'! Please change pipeline_type or generation_config type!")
        assert isinstance(prompt, list)
        assert len(generation_config) == len(prompt)
        updated_generation_config = prepare_generation_configs_by_pipe_type(generation_config, pipeline_type)
    else:
        updated_generation_config = prepare_generation_config_by_pipe_type(generation_config, pipeline_type)

    # checking streamer
    if isinstance(prompt, str):
        if streamer is None and not (generation_config.is_beam_search() or generation_config.num_return_sequences > 1) and len(prompt) == 1:
            # We can use streamer only if we have a single prompt and not beam search.
            streamer = StreamerWithResults()
        if isinstance(streamer, StreamerWithResults):
            # Clear the accumulated strings to avoid side effects
            streamer.reset()

    # create pipeline and generate results
    ov_pipe = create_ov_pipeline(models_path=models_path,
                                 pipeline_type=pipeline_type,
                                 device=device,
                                 ov_config=ov_config,
                                 scheduler_config=scheduler_config,
                                 draft_model_path=draft_model_path)
    generation_results = ov_pipe.generate(prompt, updated_generation_config, streamer)

    # convert results to `list[GenerationResult]`
    if isinstance(generation_results, DecodedResults):
        assert isinstance(generation_config, GenerationConfig)
        num_prompts = 1 if isinstance(prompt, str) else len(prompt)
        generation_results = convert_decoded_results_to_generation_result(generation_results, num_prompts, generation_config.num_return_sequences, generation_config.is_beam_search())
    
    # cleanup test artifacts
    del ov_pipe

    # compare streaming results with generated results
    if isinstance(streamer, StreamerWithResults):
        prompts = [ prompt ] if isinstance(prompt, str) else prompt
        compare_generation_results(prompts, generation_results, streamer.get_results(), generation_config)

    return generation_results


def is_generation_available(generation_config: GenerationConfig | list[GenerationConfig],
                            pipeline_type: PipelineType):
    if type(generation_config) is GenerationConfig:
        if generation_config.is_beam_search():
            if pipeline_type == PipelineType.PROMPT_LOOKUP_DECODING or pipeline_type == PipelineType.SPECULATIVE_DECODING:
                return False
    else:
        for gen_config in generation_config:
            if gen_config.is_beam_search():
                if pipeline_type == PipelineType.PROMPT_LOOKUP_DECODING or pipeline_type == PipelineType.SPECULATIVE_DECODING:
                    return False
    return True


# TODO: remove `ref` after Generator property is supported by LLMPipeline / VLMPipeline
def generate_and_compare(model: str,
                         prompts : str | list[str],
                         generation_config: list[GenerationConfig] | GenerationConfig | dict,
                         pipeline_type: PipelineType = PipelineType.AUTO,
                         scheduler_config: SchedulerConfig | dict = SchedulerConfig(),
                         ref : list[list[str]] = None,
                         streamer: StreamerWithResults | Callable | StreamerBase = None):
    ov_prompts = prompts if type(prompts) is list else [prompts]

    ov_gen_config = GenerationConfig(**generation_config) if type(generation_config) is dict else generation_config
    hf_gen_config = ov_gen_config

    if not is_generation_available(ov_gen_config, pipeline_type):
        return

    if type(ov_gen_config) is list:
        assert len(ov_gen_config) == len(ov_prompts)
    elif pipeline_type == PipelineType.CONTINUOUS_BATCHING:
        ov_gen_config = [ov_gen_config] * len(ov_prompts)

    ov_scheduler_config = scheduler_config if isinstance(scheduler_config, SchedulerConfig) else dict_to_scheduler_config(scheduler_config)
    opt_model, hf_tokenizer, models_path = download_and_convert_model(model)

    # w/a to align different API between CB and LLM
    run_cnt = len(ov_gen_config) if pipeline_type != PipelineType.CONTINUOUS_BATCHING and type(ov_gen_config) is list else 1

    for i in range(run_cnt):
        current_it_prompts = [ov_prompts[i]] if run_cnt > 1 else ov_prompts
        current_it_gen_config = ov_gen_config[i] if run_cnt > 1 else ov_gen_config

        ov_results = run_ov_pipeline(models_path=models_path,
                                     prompt=current_it_prompts,
                                     generation_config=current_it_gen_config,
                                     pipeline_type=pipeline_type,
                                     streamer=streamer.accumulate if isinstance(streamer, StreamerWithResults) else streamer,
                                     scheduler_config=ov_scheduler_config,
                                     ov_config=get_default_llm_properties())

        if ref is None:
            current_it_hf_config = [hf_gen_config[i]] if run_cnt > 1 else hf_gen_config
            ref_results = run_hugging_face(opt_model, hf_tokenizer, current_it_prompts, current_it_hf_config)
            compare_generation_results(current_it_prompts, ref_results, ov_results, current_it_gen_config)
        else:
            compare_generation_results_vs_ref(ov_prompts[i], ref[i], ov_results)

```

================================================================================


################################################################################
# core_cpp DIRECTORY
################################################################################


--- FILE: generation_config.cpp ---
Path: core_cpp/generation_config.cpp

```cpp 
// Copyright (C) 2023-2025 Intel Corporation
// SPDX-License-Identifier: Apache-2.0

#include <fstream>
#include <limits>

#include <nlohmann/json.hpp>
#include <openvino/runtime/core.hpp>
#include "openvino/genai/generation_config.hpp"
#include "sampling/structured_output/structured_output_controller.hpp"
#include "json_utils.hpp"
#include "utils.hpp"


namespace ov {
namespace genai {

ov::Property<size_t> rng_seed{"rng_seed"};

GenerationConfig::GenerationConfig(const std::filesystem::path& json_path) {
    using utils::read_json_param;

    std::ifstream f(json_path);
    OPENVINO_ASSERT(f.is_open(), "Failed to open '", json_path, "' with generation config");

    nlohmann::json data = nlohmann::json::parse(f);

    read_json_param(data, "eos_token_id", eos_token_id);
    read_json_param(data, "max_new_tokens", max_new_tokens);
    read_json_param(data, "max_length", max_length);
    // note that ignore_eos is not present in HF GenerationConfig
    read_json_param(data, "ignore_eos", ignore_eos);
    read_json_param(data, "min_new_tokens", min_new_tokens);
    read_json_param(data, "stop_strings", stop_strings);
    // note that include_stop_str_in_output is not present in HF GenerationConfig
    read_json_param(data, "include_stop_str_in_output", include_stop_str_in_output);
    // note that stop_token_ids is not present in HF GenerationConfig, but some generation_config.json define
    // multiple eos_token_id (e.g. https://huggingface.co/OpenGVLab/InternVL2-4B/blob/main/generation_config.json)
    // so, we need to read them as 'stop_token_ids'
    std::vector<int64_t> ordered_stop_token_ids;
    read_json_param(data, "eos_token_id", ordered_stop_token_ids);

    if (!ordered_stop_token_ids.empty()) {
        for (int64_t stop_token_id : ordered_stop_token_ids)
            stop_token_ids.insert(stop_token_id);

        if (eos_token_id == -1) {
            eos_token_id = ordered_stop_token_ids[0];
        }
    }

    // note that echo is not present in HF GenerationConfig
    read_json_param(data, "echo", echo);
    // note that logprobs is not present in HF GenerationConfig
    read_json_param(data, "logprobs", logprobs);

    // penalties
    read_json_param(data, "repetition_penalty", repetition_penalty);
    // note that frequency_penalty is not present in HF GenerationConfig
    read_json_param(data, "frequency_penalty", frequency_penalty);
    // note that presence_penalty is not present in HF GenerationConfig
    read_json_param(data, "presence_penalty", presence_penalty);

    // beam search
    read_json_param(data, "num_beam_groups", num_beam_groups);
    read_json_param(data, "num_beams", num_beams);
    read_json_param(data, "diversity_penalty", diversity_penalty);
    read_json_param(data, "length_penalty", length_penalty);
    read_json_param(data, "num_return_sequences", num_return_sequences);
    read_json_param(data, "no_repeat_ngram_size", no_repeat_ngram_size);

    if (data.contains("early_stopping")) {
        auto field_type = data["early_stopping"].type();
        if (field_type == nlohmann::json::value_t::string && data["early_stopping"] == "never") {
            stop_criteria = StopCriteria::NEVER;
        } else if (field_type == nlohmann::json::value_t::boolean && data["early_stopping"] == true) {
            stop_criteria = StopCriteria::EARLY;
        } else if (field_type == nlohmann::json::value_t::boolean && data["early_stopping"] == false) {
            stop_criteria = StopCriteria::HEURISTIC;
        }
    }

    // multinomial
    read_json_param(data, "do_sample", do_sample);
    read_json_param(data, "temperature", temperature);
    read_json_param(data, "top_p", top_p);
    read_json_param(data, "top_k", top_k);

    // assistant generation
    read_json_param(data, "assistant_confidence_threshold", assistant_confidence_threshold);
    read_json_param(data, "num_assistant_tokens", num_assistant_tokens);
    read_json_param(data, "max_ngram_size", max_ngram_size);

    // append EOS to stop_token_ids
    if (eos_token_id != -1)
        set_eos_token_id(eos_token_id);
}

void GenerationConfig::set_eos_token_id(size_t tokenizer_eos_token_id) {
    eos_token_id = tokenizer_eos_token_id;
    stop_token_ids.insert(eos_token_id);
}

void GenerationConfig::update_generation_config(const ov::AnyMap& properties) {
    using utils::read_anymap_param;

    // stop conditions
    read_anymap_param(properties, "eos_token_id", eos_token_id);
    read_anymap_param(properties, "max_new_tokens", max_new_tokens);
    read_anymap_param(properties, "max_length", max_length);
    read_anymap_param(properties, "ignore_eos", ignore_eos);
    read_anymap_param(properties, "min_new_tokens", min_new_tokens);
    read_anymap_param(properties, "stop_strings", stop_strings);
    read_anymap_param(properties, "include_stop_str_in_output", include_stop_str_in_output);
    read_anymap_param(properties, "stop_token_ids", stop_token_ids);
    if (eos_token_id > 0) {
        set_eos_token_id(eos_token_id);
    }

    // generic
    read_anymap_param(properties, "echo", echo);
    read_anymap_param(properties, "logprobs", logprobs);
    read_anymap_param(properties, "num_return_sequences", num_return_sequences);
    read_anymap_param(properties, "adapters", adapters);
    read_anymap_param(properties, "apply_chat_template", apply_chat_template);

    // penalties
    read_anymap_param(properties, "frequency_penalty", frequency_penalty);
    read_anymap_param(properties, "presence_penalty", presence_penalty);
    read_anymap_param(properties, "repetition_penalty", repetition_penalty);

    // beam search
    read_anymap_param(properties, "num_beam_groups", num_beam_groups);
    read_anymap_param(properties, "num_beams", num_beams);
    read_anymap_param(properties, "diversity_penalty", diversity_penalty);
    read_anymap_param(properties, "length_penalty", length_penalty);
    read_anymap_param(properties, "stop_criteria", stop_criteria);
    read_anymap_param(properties, "no_repeat_ngram_size", no_repeat_ngram_size);

    // multinomial
    read_anymap_param(properties, "do_sample", do_sample);
    read_anymap_param(properties, "temperature", temperature);
    read_anymap_param(properties, "top_p", top_p);
    read_anymap_param(properties, "top_k", top_k);
    // TODO: add support of 'generator' property similar to Image generation
    read_anymap_param(properties, "rng_seed", rng_seed);

    // assistant generation
    read_anymap_param(properties, "assistant_confidence_threshold", assistant_confidence_threshold);
    read_anymap_param(properties, "num_assistant_tokens", num_assistant_tokens);
    read_anymap_param(properties, "max_ngram_size", max_ngram_size);

    // Structured output
    read_anymap_param(properties, "structured_output_config", structured_output_config);
}


StructuralTagItem::StructuralTagItem(const ov::AnyMap& properties) {
    update_config(properties);
}

void StructuralTagItem::update_config(const ov::AnyMap& properties) {
    using utils::read_anymap_param;

    read_anymap_param(properties, "begin", begin);
    read_anymap_param(properties, "schema", schema);
    read_anymap_param(properties, "end", end);
}


std::string StructuralTagItem::to_string() const {
    return "StructuralTagItem(begin=" + begin +
           ", schema=" + schema +
           ", end=" + end + ")";
}


StructuralTagsConfig::StructuralTagsConfig(const ov::AnyMap& properties) {
    update_config(properties);
}


void StructuralTagsConfig::update_config(const ov::AnyMap& properties) {
    using utils::read_anymap_param;

    read_anymap_param(properties, "structural_tags", structural_tags);
    read_anymap_param(properties, "triggers", triggers);
}


std::string StructuralTagsConfig::to_string() const {
    std::ostringstream tags_repr;
    tags_repr << "[";
    for (auto it = structural_tags.begin(); it != structural_tags.end(); ++it) {
        if (it != structural_tags.begin()) tags_repr << ", ";
        tags_repr << it->to_string();
    }
    tags_repr << "]";

    std::ostringstream triggers_repr;
    triggers_repr << "[";
    for (auto it = triggers.begin(); it != triggers.end(); ++it) {
        if (it != triggers.begin()) triggers_repr << ", ";
        triggers_repr << *it;
    }
    triggers_repr << "]";

    return "StructuralTagsConfig(structural_tags=" + tags_repr.str() +
           ", triggers=" + triggers_repr.str() + ")";
}

StructuredOutputConfig::StructuredOutputConfig(const ov::AnyMap& properties) {
    update_config(properties);
    validate();
}

void StructuredOutputConfig::update_config(const ov::AnyMap& properties) {
    using utils::read_anymap_param;

    read_anymap_param(properties, "json_schema", json_schema);
    read_anymap_param(properties, "regex", regex);
    read_anymap_param(properties, "grammar", grammar);
    read_anymap_param(properties, "structural_tags_config", structural_tags_config);
    read_anymap_param(properties, "backend", backend);
}

size_t GenerationConfig::get_max_new_tokens(size_t prompt_length) const {
    // max_new_tokens has priority over max_length, only if max_new_tokens was not specified use max_length
    if (max_new_tokens != SIZE_MAX) {
        return max_new_tokens;
    } else {
        return max_length - prompt_length;
    }
}

bool GenerationConfig::is_greedy_decoding() const {
    return !do_sample && !is_beam_search();
}

bool GenerationConfig::is_beam_search() const {
    return num_beams > 1;
}

bool GenerationConfig::is_multinomial() const {
    return do_sample;
}

bool GenerationConfig::is_speculative_decoding() const {
    return is_assisting_generation();
}

bool GenerationConfig::is_assisting_generation() const {
    return assistant_confidence_threshold > 0 || num_assistant_tokens > 0;
}

bool GenerationConfig::is_structured_output_generation() const {
    return structured_output_config.has_value();
}

bool GenerationConfig::is_prompt_lookup() const {
    return max_ngram_size > 0 && num_assistant_tokens > 0;
}

void GenerationConfig::validate() const {
    OPENVINO_ASSERT(num_return_sequences > 0, "num_return_sequences must be greater than 0");

    // Stop conditions

    OPENVINO_ASSERT(eos_token_id == -1 || stop_token_ids.find(eos_token_id) != stop_token_ids.end(),
        "'stop_token_ids' must contain 'eos_token_id'. Please, call 'set_eos_token_id' with 'eos_token_id' value");

    auto stop_token_ids_it = std::find_if(stop_token_ids.begin(), stop_token_ids.end(), [] (int64_t stop_token_id) -> bool {
        return stop_token_id < 0;
    });
    OPENVINO_ASSERT(stop_token_ids_it == stop_token_ids.end(), "'stop_token_ids' must be non-negative, but it contains a value ", *stop_token_ids_it);

    OPENVINO_ASSERT(!ignore_eos || max_new_tokens != SIZE_MAX || max_length != SIZE_MAX,
                    "ignore_eos is true, in this case either 'max_new_tokens', or 'max_length' should be defined.");

    OPENVINO_ASSERT(eos_token_id != -1 || !stop_token_ids.empty() || !stop_strings.empty() || max_new_tokens != SIZE_MAX || max_length != SIZE_MAX,
                    "Either 'eos_token_id', or 'stop_token_ids', or 'stop_strings', or 'max_new_tokens', or 'max_length' should be defined.");

    OPENVINO_ASSERT(max_new_tokens > 0 || (max_new_tokens == 0 && echo), "'max_new_tokens' must be greater than 0, if `echo` is set, 0 is also accepted");
    OPENVINO_ASSERT(min_new_tokens <= max_new_tokens, "min_new_tokens must be less or equal max_new_tokens");

    // Sampling strategies

    OPENVINO_ASSERT(num_return_sequences == 1 || (is_multinomial() || is_beam_search()), 
        "'num_return_sequences' can be more than 1 only in case of beam search or multinomial sampling, but got ", num_return_sequences);

    // generic penalties, but not supported by beam search currently
    if (!is_beam_search()) {
        OPENVINO_ASSERT(frequency_penalty >= -2.0f && frequency_penalty <= 2.0f, "'frequence_penalty' penalty must be within [-2.0; 2.0], but got ", frequency_penalty);
        OPENVINO_ASSERT(presence_penalty >= -2.0f && presence_penalty <= 2.0f, "'presence_penalty' penalty must be within [-2.0; 2.0], but got ", presence_penalty);
        OPENVINO_ASSERT(repetition_penalty > 0.0f, "'repetition_penalty' must be a strictly positive float, but got ", repetition_penalty);
    } else {
        OPENVINO_ASSERT(frequency_penalty == 0.0f, "'frequency_penalty' is not currently supported by beam search and should be 0.0f, but got ", frequency_penalty);
        OPENVINO_ASSERT(presence_penalty == 0.0f, "'presence_penalty' is not currently supported by beam search and should be 0.0f, but got ", presence_penalty);
        OPENVINO_ASSERT(repetition_penalty == 1.0f, "'repetition_penalty' is not currently supported by beam search and should be 1.0f, but got ", repetition_penalty);
    }

    if (is_multinomial()) {
        OPENVINO_ASSERT(top_p > 0 && top_p <= 1.0f, "When 'do_sample' is true, top_p must be a positive float > 0.0 and <= 1.0, but got ", top_p);
        OPENVINO_ASSERT(temperature > 0, "When 'do_sample' is true, temperature must be a strictly positive float, but got ", temperature);
    } else {
        // parameters requiring multinomial
        // OPENVINO_ASSERT(top_k == std::numeric_limits<size_t>::max(), "When 'do_sample' is false, top_k must be max of size_t, but got ", top_k);
        // OPENVINO_ASSERT(top_p == 1.0f, "When 'do_sample' is false, top_p must be 1.0f, but got ", top_p);
        // OPENVINO_ASSERT(temperature == 1.0f, "When 'do_sample' is false, temperature must be a 1.0f, but got ", temperature);
    }

    if (is_beam_search()) {
        OPENVINO_ASSERT(num_beams % num_beam_groups == 0, "'num_beams' (", num_beams, ") should be divisible by 'num_beam_groups' (", num_beam_groups, ")");
        OPENVINO_ASSERT(num_beams >= num_return_sequences, "'num_beams' (", num_beams, ") must be greater equal than 'num_return_sequences' (", num_return_sequences, ")");

        OPENVINO_ASSERT(!do_sample,
                        "Beam search with sampling is not supported yet. "
                        "Please either set do_sample=false to use beam search "
                        "or set num_beams=1 if you with to use multinomial sampling.");

        OPENVINO_ASSERT(no_repeat_ngram_size > 0, "'no_repeat_ngram_size' must be positive");
        if (num_beam_groups > 1) {
            OPENVINO_ASSERT(diversity_penalty != 0.0f, "For grouped beam search 'diversity_penalty' should not be zero, otherwise it fallbacks to non-grouped beam search");
        } else {
            OPENVINO_ASSERT(diversity_penalty == 0.0f, "For beam search 'diversity_penalty' is applicable only when grouped beam search is used, but got 'num_beam_groups' == 1");
        }
    } else {
        // parameters requiring beam search
        // OPENVINO_ASSERT(num_beam_groups == 1, "'num_beam_groups' is supported by beam search only and should be 1 otherwise, but got ", num_beam_groups);
        // OPENVINO_ASSERT(no_repeat_ngram_size == std::numeric_limits<size_t>::max(), "'no_repeat_ngram_size' is supported only by beam search, otherwise should be set to max of size_t, but got ", no_repeat_ngram_size);
        // OPENVINO_ASSERT(diversity_penalty == 0.0f, "'diversity_penalty' is set to ", diversity_penalty, " (default is 0.0f), which is supported only by beam search sampling");
        // OPENVINO_ASSERT(length_penalty == 1.0f, "'length_penalty' is set to ", length_penalty, " (default is 1.0f), which is supported only by beam search sampling");
    }

    // assistant generation

    if (is_assisting_generation()) {
        OPENVINO_ASSERT(!is_beam_search() && num_return_sequences == 1, "Beam search and parallel sampling are not compatible with assistant generation");
        OPENVINO_ASSERT(assistant_confidence_threshold == 0.0f || num_assistant_tokens == 0, "Parameters `assistant_confidence_threshold` and `num_assistant_tokens` are mutually exclusive in `GenerationConfig`");
    }

    if (num_assistant_tokens == 0) {
        OPENVINO_ASSERT(max_ngram_size == 0, "'max_ngram_size' should be set to default value 0 when prompt lookup is disabled");
    }

    if(is_structured_output_generation()) {
        (*structured_output_config).validate();
    }
}

void StructuredOutputConfig::validate() const {
    auto& registry = StructuredOutputController::get_backend_registry();
    std::string backend_name = backend.has_value() ? *backend : StructuredOutputController::get_default_backend_name();
    std::string upper_name = backend_name;
    std::transform(upper_name.begin(), upper_name.end(), upper_name.begin(), [](unsigned char c){ return std::toupper(c); });

    OPENVINO_ASSERT(registry.find(backend_name) != registry.end(),
                    "Structured output backend '", backend_name, "' is not registered. "
                    "Please recompile with -DENABLE_" + upper_name + "=ON option to enable it.");

    OPENVINO_ASSERT(
        (json_schema.has_value() + regex.has_value() + grammar.has_value() + structural_tags_config.has_value()) == 1,
        "Only one of json, regex, grammar or structural_tags_config should be set in StructuredOutputConfig, but got: ",
        (json_schema.has_value() ? "json=" + *json_schema +", " : ""),
        (regex.has_value() ? "regex=" + *regex + ", " : ""),
        (grammar.has_value() ? "grammar=" + *grammar : ""),
        (structural_tags_config.has_value() ? "structural_tags_config=" + structural_tags_config->to_string() : "")
    );
}

GenerationConfig beam_search() {
    GenerationConfig beam_search_config;
    beam_search_config.num_beams = 4;
    beam_search_config.num_return_sequences = 3;
    beam_search_config.num_beam_groups = 2;
    beam_search_config.max_new_tokens = 100;
    beam_search_config.diversity_penalty = 2.0f;
    return beam_search_config;
}

GenerationConfig greedy() {
    GenerationConfig greedy_config;
    greedy_config.max_new_tokens = 30;
    return greedy_config;
}

GenerationConfig multinomial() {
    GenerationConfig multinomial_config;
    multinomial_config.do_sample = true;
    multinomial_config.temperature = 0.9f;
    multinomial_config.top_p = 0.9f;
    multinomial_config.top_k = 20;
    multinomial_config.num_return_sequences = 3;
    multinomial_config.presence_penalty = 0.01f;
    multinomial_config.frequency_penalty = 0.1f;
    multinomial_config.min_new_tokens = 15;
    multinomial_config.max_new_tokens = 30;
    return multinomial_config;
}

}  // namespace genai
}  // namespace ov

```

================================================================================


--- FILE: pipeline_stateful.cpp ---
Path: core_cpp/pipeline_stateful.cpp

```cpp 

// Copyright (C) 2023-2025 Intel Corporation
// SPDX-License-Identifier: Apache-2.0

#include "llm/pipeline_stateful.hpp"

#include "lora/helper.hpp"
#include "lm_encoding.hpp"
#include "openvino/genai/text_streamer.hpp"

#include "utils.hpp"

namespace ov::genai {

StatefulLLMPipeline::StatefulLLMPipeline(
    const ov::InferRequest& request,
    const ov::genai::Tokenizer& tokenizer,
    OptionalGenerationConfig generation_config)
    : LLMPipelineImplBase(tokenizer, generation_config.has_value() ? *generation_config : GenerationConfig()),
    m_model_runner(request) {
    auto compiled_model = m_model_runner.get_compiled_model();
    auto execution_devices = compiled_model.get_property(ov::execution_devices);
    if (execution_devices[0].find("NPU") != std::string::npos) {
        OPENVINO_ASSERT(execution_devices.size() == 1u);
        m_is_npu = true;
        m_max_prompt_len = compiled_model.get_property("NPUW_LLM_MAX_PROMPT_LEN").as<uint32_t>();
        const auto min_response_len = compiled_model.get_property("NPUW_LLM_MIN_RESPONSE_LEN").as<uint32_t>();
        m_max_kv_cache_size = m_max_prompt_len + min_response_len;
    }
}

StatefulLLMPipeline::StatefulLLMPipeline(
    const std::filesystem::path& models_path,
    const ov::genai::Tokenizer& tokenizer,
    const std::string& device,
    const ov::AnyMap& properties)
    : StatefulLLMPipeline{
        utils::read_model(models_path, properties),
        tokenizer,
        device,
        properties,
        utils::from_config_json_if_exists(models_path)
    } {}

StatefulLLMPipeline::StatefulLLMPipeline(
    const std::shared_ptr<ov::Model>& model,
    const ov::genai::Tokenizer& tokenizer,
    const std::string& device,
    const ov::AnyMap& properties,
    const ov::genai::GenerationConfig& generation_config)
    : LLMPipelineImplBase(tokenizer, generation_config), m_sampler(m_tokenizer) {
    if (device.find("NPU") != std::string::npos) {
        m_is_npu = true;
        m_use_full_chat_history = true;
    }

    // FIXME: slicing produces incorrect results for some models on NPU.
    // On NPU, applying slice the safe way is done by the underlying plugin
    if (!m_is_npu) {
        utils::apply_slice_before_matmul_transformation(model);
    }

    auto kv_pos = ov::genai::utils::get_kv_axes_pos(model);

    if (!m_use_full_chat_history)
        m_kv_cache_state.seq_length_axis = kv_pos.seq_len;

    auto [filtered_properties_without_gguf, enable_save_ov_model] = utils::extract_gguf_properties(properties);
    auto filtered_properties = extract_adapters_from_properties(filtered_properties_without_gguf, &m_generation_config.adapters);
    if (m_generation_config.adapters) {
        m_generation_config.adapters->set_tensor_name_prefix("base_model.model.");
        m_adapter_controller = AdapterController(model, *m_generation_config.adapters, device);   // TODO: Make the prefix name configurable
    }
    ov::CompiledModel compiled_model;
    if (m_is_npu) {
        utils::KVDesc kv_desc;
        std::tie(compiled_model, kv_desc) = utils::compile_decoder_for_npu(model, *filtered_properties, kv_pos);
        m_max_prompt_len = kv_desc.max_prompt_len;
        m_max_kv_cache_size = kv_desc.max_prompt_len + kv_desc.min_response_len;
    } else {
       compiled_model = utils::singleton_core().compile_model(model, device, *filtered_properties);
    }
    m_model_runner = compiled_model.create_infer_request();
    ov::genai::utils::print_compiled_model_properties(compiled_model, "Stateful LLM model");

    // If eos_token_id was not provided, take value
    if (m_generation_config.eos_token_id == -1)
        m_generation_config.set_eos_token_id(m_tokenizer.get_eos_token_id());

    m_sampler.set_seed(m_generation_config.rng_seed);
}

StatefulLLMPipeline::StatefulLLMPipeline(
    const std::filesystem::path& models_path,
    const std::string& device,
    const ov::AnyMap& plugin_config)
    : StatefulLLMPipeline{models_path, Tokenizer(models_path, plugin_config), device, plugin_config} {}

DecodedResults StatefulLLMPipeline::generate(
    StringInputs inputs,
    OptionalGenerationConfig generation_config,
    StreamerVariant streamer) {
    if (is_chat_conversation && m_chat_input_type == ov::genai::utils::GenerationChatInputsType::UNDEF)
        m_chat_input_type = ov::genai::utils::GenerationChatInputsType::STRING;

    if (is_chat_conversation)
        OPENVINO_ASSERT(m_chat_input_type != ov::genai::utils::GenerationChatInputsType::ENCODED_INPUTS,
                        "Chat doesn't support switching between input types. Please, continue using EncodedInputs or restart the chat.");

    auto start_time = std::chrono::steady_clock::now();
    GenerationConfig config = (generation_config.has_value()) ? *generation_config : m_generation_config;
    // If stop_token_ids were not provided, take value from default m_generation_config
    if (config.stop_token_ids.empty())
        config.stop_token_ids = m_generation_config.stop_token_ids;
    // If eos_token_id was not provided, take value from default m_generation_config
    if (config.eos_token_id == -1)
        config.set_eos_token_id(m_generation_config.eos_token_id);
    config.validate();

    TokenizedInputs encoded_input;

    if (auto input_vector = std::get_if<std::vector<std::string>>(&inputs)) {
        OPENVINO_ASSERT(!is_chat_conversation, "Can't chat with multiple prompts");
        if (config.apply_chat_template && !m_tokenizer.get_chat_template().empty()) {
            std::vector<std::string> templated_input_vector;
            for (auto& input : *input_vector) {
                ChatHistory history({{{"role", "user"}, {"content", input}}});
                constexpr bool add_generation_prompt = true;
                auto templated_prompt = m_tokenizer.apply_chat_template(history, add_generation_prompt);
                templated_input_vector.push_back(templated_prompt);
            }
            encoded_input = m_tokenizer.encode(templated_input_vector, ov::genai::add_special_tokens(false));
        } else {
            encoded_input = m_tokenizer.encode(*input_vector, ov::genai::add_special_tokens(true));
        }
    } else if (auto input_prompt = std::get_if<std::string>(&inputs)) {
        std::string& prompt = *input_prompt;

        if (is_chat_conversation) {
            m_history.push_back({{"role", "user"}, {"content", prompt}});
            constexpr bool add_generation_prompt = true;
            auto new_templated_chat_history = m_tokenizer.apply_chat_template(m_history, add_generation_prompt);
            // Do not add special tokens in chat scenario to be aligned with HF.
            auto new_chat_tokens = m_tokenizer.encode(new_templated_chat_history, ov::genai::add_special_tokens(false));

            if (m_use_full_chat_history) {
                encoded_input = new_chat_tokens;
            } else {
                ov::genai::align_kv_cache_and_history(new_chat_tokens.input_ids, m_kv_cache_state);
                encoded_input = get_chat_encoded_input(new_chat_tokens.input_ids, m_kv_cache_state);
            }
            // TODO: Forbid LoRA config change if we are in the chat mode, because it requires regenerating the history with LoRA applied
        } else {
            std::string& prompt = *input_prompt;
            if (config.apply_chat_template && !m_tokenizer.get_chat_template().empty()) {
                ChatHistory history({{{"role", "user"}, {"content", prompt}}});
                constexpr bool add_generation_prompt = true;
                auto templated_prompt = m_tokenizer.apply_chat_template(history, add_generation_prompt);
                encoded_input = m_tokenizer.encode(templated_prompt, ov::genai::add_special_tokens(false));
            } else {
                // in case when chat_template was not found in tokenizer_config.json or set
                encoded_input = m_tokenizer.encode(prompt, ov::genai::add_special_tokens(true));
            }
        }
    }

    auto encode_stop_time =  std::chrono::steady_clock::now();
    auto encoded_results = generate(encoded_input, config, streamer);

    auto decode_start_time =  std::chrono::steady_clock::now();
    DecodedResults decoded_results = {m_tokenizer.decode(encoded_results.tokens), encoded_results.scores};
    auto decode_stop_time =  std::chrono::steady_clock::now();

    if (is_chat_conversation) {
        if (m_chat_generation_finish_status == ov::genai::GenerationStatus::CANCEL) {
            // If chat generation process was cancelled by user, let's rollback to previous state of history
            m_history.pop_back();
        } else {
            // Tail of chat template is missing in KV cache.
            // Find the tail to concatenate it with the next input prompt.
            auto answer = decoded_results.texts[0];
            m_history.push_back({{"role", "assistant"}, {"content", std::move(answer)}});
        }
    }

    // generate_durations
    decoded_results.perf_metrics = encoded_results.perf_metrics;

    auto& raw_counters = decoded_results.perf_metrics.raw_metrics;
    auto stop_time = std::chrono::steady_clock::now();
    raw_counters.generate_durations.clear();
    raw_counters.generate_durations.emplace_back(PerfMetrics::get_microsec(stop_time - start_time));
    raw_counters.tokenization_durations.emplace_back(PerfMetrics::get_microsec(encode_stop_time - start_time));
    raw_counters.detokenization_durations.emplace_back(PerfMetrics::get_microsec(decode_stop_time - decode_start_time));

    // Added tokenization/detokenization times, and updated generate duration, need to reevaluate statistics.
    decoded_results.perf_metrics.m_evaluated = false;
    decoded_results.perf_metrics.evaluate_statistics(start_time);
    return decoded_results;
}

EncodedResults StatefulLLMPipeline::generate(
    const EncodedInputs& inputs,
    OptionalGenerationConfig generation_config,
    StreamerVariant streamer) {
    if (is_chat_conversation && m_chat_input_type == ov::genai::utils::GenerationChatInputsType::UNDEF)
        m_chat_input_type = ov::genai::utils::GenerationChatInputsType::ENCODED_INPUTS;

    if (is_chat_conversation)
        // if chat was run in StringInputs mode, but it was called EncodedInputs generate, last m_history entry will be with assistant role
        OPENVINO_ASSERT(m_chat_input_type == ov::genai::utils::GenerationChatInputsType::ENCODED_INPUTS || m_history.back()["role"] == "user",
                        "Chat doesn't support switching between input types. Please, continue using StringInputs or restart the chat.");

    if (!is_chat_conversation) {
        reset_kv_state();
        m_model_runner.get_tensor("attention_mask").set_shape({1, 0});
        m_kv_cache_state.reset_state();
    }

    auto start_time = std::chrono::steady_clock::now();
    ov::Tensor input_ids;
    ov::Tensor attention_mask;
    if (auto data = std::get_if<ov::Tensor>(&inputs)) {
        if (m_is_npu) {
            // Prefill model in NPU is reshaped to NPUW_LLM_MAX_PROMPT_LEN x NPUW_LLM_MAX_PROMPT_LEN
            OPENVINO_ASSERT(data->get_size() <= m_max_prompt_len,
                "Stateful LLM pipeline on NPU may only process prompts or hold chat history up to ",
                m_max_prompt_len,
                " tokens. ",
                data->get_size(),
                " is passed.\n Set the \"MAX_PROMPT_LEN\" config option to increase the limit.");
        }
        input_ids = ov::Tensor(data->get_element_type(), data->get_shape());
        data->copy_to(input_ids);
        attention_mask = ov::genai::utils::init_attention_mask(input_ids);
    } else if (auto data = std::get_if<TokenizedInputs>(&inputs)) {
        if (m_is_npu) {
            // Prefill model in NPU is reshaped to NPUW_LLM_MAX_PROMPT_LEN x NPUW_LLM_MAX_PROMPT_LEN
            OPENVINO_ASSERT(data->input_ids.get_size() <= m_max_prompt_len,
                "Stateful LLM pipeline on NPU may only process prompts or hold chat history up to ",
                m_max_prompt_len,
                " tokens. ",
                data->input_ids.get_size(),
                " is passed.\n Set the \"MAX_PROMPT_LEN\" config option to increase the limit.");
        }
        input_ids = ov::Tensor(data->input_ids.get_element_type(), data->input_ids.get_shape());
        data->input_ids.copy_to(input_ids);

        attention_mask = ov::Tensor{data->attention_mask.get_element_type(), data->attention_mask.get_shape()};
        data->attention_mask.copy_to(attention_mask);
    }

    if (is_chat_conversation && m_chat_input_type == ov::genai::utils::GenerationChatInputsType::ENCODED_INPUTS) 
        std::copy(input_ids.data<int64_t>(), input_ids.data<int64_t>() + input_ids.get_size(), std::back_inserter(m_tokenized_chat_history));

    size_t real_input_ids_size = input_ids.get_shape().at(1);

    // Tail of previous output in chat mode is missing in KV cache.
    if (is_chat_conversation && m_chat_input_type == ov::genai::utils::GenerationChatInputsType::ENCODED_INPUTS) {
        ov::Tensor new_chat_tokens = ov::Tensor{ov::element::i64, {1, m_tokenized_chat_history.size()}, m_tokenized_chat_history.data()};
        ov::genai::align_kv_cache_and_history(new_chat_tokens, m_kv_cache_state);

        auto encoded_input = get_chat_encoded_input(new_chat_tokens, m_kv_cache_state);
        input_ids = encoded_input.input_ids;
        attention_mask = encoded_input.attention_mask;
    }

    GenerationConfig config = (generation_config.has_value()) ? *generation_config : m_generation_config;

    // If stop_token_ids were not provided, take value from default m_generation_config
    if (config.stop_token_ids.empty())
        config.stop_token_ids = m_generation_config.stop_token_ids;
    // If eos_token_id was not provided, take value from default m_generation_config
    if (config.eos_token_id == -1)
        config.set_eos_token_id(m_generation_config.eos_token_id);
    config.validate();

    auto batch_size = input_ids.get_shape().at(0);

    if (m_is_npu) {
        OPENVINO_ASSERT(batch_size == 1u, "Currently only batch size equal to 1 is supported for NPU device!");
        OPENVINO_ASSERT(config.is_greedy_decoding() || config.is_multinomial(),
            "Currently only greedy and multinomial decoding are supported for NPU device!");
        OPENVINO_ASSERT(config.num_return_sequences == 1u,
            "Currently only \"num_return_sequences\" equal to 1 is supported for NPU device!");
    }

    // Stateful pipeline does not provide logprobs for prompt tokens
    OPENVINO_ASSERT(config.echo == false, "Echo is not supported in the stateful pipeline");

    std::shared_ptr<StreamerBase> streamer_ptr = ov::genai::utils::create_streamer(streamer, m_tokenizer);

    OPENVINO_ASSERT(streamer_ptr == nullptr || batch_size == 1 && config.num_return_sequences == 1 &&
        (config.is_greedy_decoding() || config.is_multinomial()),
        "Currently streaming is possible only with batch size=1 and only for greedy or multinomial decoding");

    auto num_inputs = m_model_runner.get_compiled_model().inputs().size();
    OPENVINO_ASSERT(num_inputs == 4 || num_inputs == 3, "Model should have 3 or 4 inputs: "
                    "either (input_ids, attention_mask, beam_idx) or "
                    "(input_ids, attention_mask, position_ids, beam_idx) "
                    "but you have '" + std::to_string(num_inputs) + "' inputs");

    if (is_chat_conversation) {
        if (m_use_full_chat_history)
            reset_kv_state();
        else
            ov::genai::utils::trim_kv_cache(m_model_runner, m_kv_cache_state, m_adapter_controller);
    }

    size_t kv_cache_len = 0;
    ov::Tensor concatenated_attention_mask;
    if (is_chat_conversation && !m_kv_cache_state.get_state().empty() && !m_use_full_chat_history) {
        OPENVINO_ASSERT(batch_size == 1, "continuation of generation is possible only for batch 1");
        // If history is saved in KV cache, concatenate new attention_mask with the already existing.
        // Between subsequent runs attention_mask should not be modified.
        auto atten_mask_history = m_model_runner.get_tensor("attention_mask");
        auto prompt_len = attention_mask.get_shape()[1];

        kv_cache_len = m_kv_cache_state.get_state().size();

        ov::Tensor new_atten_mask = ov::Tensor{ov::element::i64, {batch_size, kv_cache_len + prompt_len}};
        auto start_atten_hst = atten_mask_history.data<int64_t>();

        std::copy(start_atten_hst, start_atten_hst + kv_cache_len,
                new_atten_mask.data<int64_t>());
        std::copy(attention_mask.data<int64_t>(), attention_mask.data<int64_t>() + prompt_len,
                new_atten_mask.data<int64_t>() + kv_cache_len);
        concatenated_attention_mask = new_atten_mask;
    } else {
        concatenated_attention_mask = attention_mask;
    }

    size_t prev_attn_mask_size = concatenated_attention_mask.get_shape()[1];

    bool position_ids_available = (num_inputs == 4);
    std::optional<ov::Tensor> position_ids = std::nullopt;
    if (position_ids_available) {
        position_ids = ov::Tensor{ov::element::i64, input_ids.get_shape()};
        utils::initialize_position_ids(*position_ids, attention_mask, kv_cache_len);
    }

    if(m_adapter_controller) {
        m_adapter_controller->apply(m_model_runner, config.adapters);
    }

    std::vector<SequenceGroup::Ptr> requests;
    size_t block_size = 1;

    for (size_t request_id = 0; request_id < batch_size; request_id++) {
        SequenceGroup::Ptr sequence_group;
        if (is_chat_conversation) {
            std::vector<int64_t>& state = m_kv_cache_state.get_state();
            std::vector<int64_t> tokenized_chat_hist;
            tokenized_chat_hist.reserve(state.size() + input_ids.get_size());
            std::copy(state.begin(), state.end(), std::back_inserter(tokenized_chat_hist));
            std::copy(input_ids.data<int64_t>(), input_ids.data<int64_t>() + input_ids.get_size(), std::back_inserter(tokenized_chat_hist));
            sequence_group = std::make_shared<SequenceGroup>(request_id,  ov::Tensor(ov::element::i64, {1, tokenized_chat_hist.size()}, tokenized_chat_hist.data()), config, block_size);
        } else {
            size_t seq_len = input_ids.get_shape().at(1);
            size_t batch_offset = request_id * seq_len;
            const int64_t* prompt_start = input_ids.data<const int64_t>() + batch_offset;
            std::vector<int64_t> tokenized_prompt(prompt_start, prompt_start + seq_len);

            sequence_group = std::make_shared<SequenceGroup>(request_id, tokenized_prompt, config, block_size);
        }

        requests.push_back(sequence_group);
    }

    if (m_sampler.get_seed() != config.rng_seed) {
        m_sampler.set_seed(config.rng_seed);
    }

    ov::genai::utils::GenerationFinishInfo finish_info = get_lm_encoded_results(m_model_runner, input_ids, concatenated_attention_mask, streamer_ptr, m_sampler,
                                                                                requests, position_ids, m_kv_cache_state, nullptr, std::nullopt, m_max_kv_cache_size);
    ov::genai::EncodedResults& result = finish_info.results;
    m_chat_generation_finish_status = finish_info.streaming_finish_status;

    if (is_chat_conversation) {
        m_kv_cache_state.num_tokens_to_trim = 0;

        if (m_chat_input_type == ov::genai::utils::GenerationChatInputsType::ENCODED_INPUTS) {
            if (m_chat_generation_finish_status == ov::genai::GenerationStatus::CANCEL) {
                m_tokenized_chat_history.resize(m_tokenized_chat_history.size() - real_input_ids_size);
            } else {
                std::copy(result.tokens[0].begin(), result.tokens[0].end(), std::back_inserter(m_tokenized_chat_history));
            }
        }
        if (config.is_beam_search()) {
            m_kv_cache_state.num_tokens_to_trim = m_model_runner.get_tensor("attention_mask").get_shape()[1] - prev_attn_mask_size;
        }
    }

    auto stop_time = std::chrono::steady_clock::now();

    // If is called without tokenization then that stat will not be reported.
    auto& metrics = result.perf_metrics;
    metrics.num_input_tokens = batch_size * input_ids.get_shape().at(1);
    metrics.load_time = m_load_time_ms;
    metrics.raw_metrics.generate_durations.emplace_back(PerfMetrics::get_microsec(stop_time - start_time));
    metrics.evaluate_statistics(start_time);
    return result;
}

void StatefulLLMPipeline::start_chat(const std::string& system_message) {
    finish_chat();
    is_chat_conversation = true;

    if (system_message.empty())
        return;

    m_history.push_back({{"role", "system"}, {"content", system_message}});
}

void StatefulLLMPipeline::reset_kv_state() {
    if(m_adapter_controller) {
        for(auto& state: m_model_runner.query_state()) {
            if(!m_adapter_controller->has_state_name(state.get_name())) {
                state.reset();
            }
        }
    } else {
        m_model_runner.reset_state();
    }
}

void StatefulLLMPipeline::finish_chat() {
    is_chat_conversation = false;
    m_chat_input_type = ov::genai::utils::GenerationChatInputsType::UNDEF;
    bool have_state = 0 != m_model_runner.get_tensor("attention_mask").get_size();
    if (!m_kv_cache_state.get_state().empty() || have_state) {
        reset_kv_state();
        m_model_runner.get_tensor("attention_mask").set_shape({1, 0});
        m_history.clear();
        m_tokenized_chat_history.clear();
        m_kv_cache_state.reset_state();
    }
}

StatefulLLMPipeline::~StatefulLLMPipeline() {
    m_model_runner.get_compiled_model().release_memory();
}

} // namespace ov::genai

```

================================================================================


--- FILE: utils.cpp ---
Path: core_cpp/utils.cpp

```cpp 
// Copyright (C) 2023-2025 Intel Corporation
// SPDX-License-Identifier: Apache-2.0

#include "utils.hpp"

#include <variant>
#include <fstream>
#include <memory>

#include "openvino/op/add.hpp"
#include "openvino/op/divide.hpp"
#include "openvino/op/gather.hpp"
#include "openvino/op/multiply.hpp"
#include "openvino/op/matmul.hpp"
#include "openvino/op/slice.hpp"
#include "openvino/op/tanh.hpp"
#include "openvino/op/transpose.hpp"
#include "openvino/genai/text_streamer.hpp"
#include "gguf_utils/gguf_modeling.hpp"


#include "sampling/sampler.hpp"

namespace ov {

namespace genai {
const std::string PA_BACKEND = "PA";
const std::string SDPA_BACKEND = "SDPA";
}
}

namespace {

void update_config(ov::AnyMap& config, const std::pair<std::string, ov::Any>& pair) {
    if (config.count(pair.first) == 0) {
        config.insert(pair);
    }
}

void rename_key(ov::AnyMap& config, const std::string& old_key, const std::string& new_key) {
    if (config.count(old_key) != 0) {
        auto opt_value = ov::genai::utils::pop_option(config, old_key);
        config[new_key] = opt_value.value();
    }
}

template <typename T>
std::optional<T> get_option(const ov::AnyMap& config, const std::string& option_name) {
    if (auto it = config.find(option_name); it != config.end()) {
        return std::make_optional(it->second.as<T>());
    }
    return std::nullopt;
}

std::optional<uint32_t> pop_int_and_cast(ov::AnyMap& config, const std::string& key) {
    auto anyopt = ov::genai::utils::pop_option(config, key);
    if (anyopt.has_value()) {
        const auto any = anyopt.value();
        int64_t value;
        // NB: Integer value coming from python has int64_t datatype
        if (any.is<int64_t>()) {
            value = any.as<int64_t>();
        } else if (any.is<int>()) {
            value = any.as<int>();
        } else {
            OPENVINO_THROW("Failed to extract " + key + ". Type mismatch: expected types: int or int64_t");
        }
        if (value < 0) {
            OPENVINO_THROW(key + " cannot be negative!");
        }
        return std::make_optional(static_cast<uint32_t>(value));
    }
    return std::nullopt;
}

void update_npu_config(ov::AnyMap& config,
                       const std::shared_ptr<ov::Model>& model,
                       const ov::genai::utils::KVAxesPosition& kv_pos,
                       const ov::genai::utils::KVDesc& kv_desc) {
    update_config(config, {"NPU_USE_NPUW", "YES"});
    update_config(config, {"NPUW_LLM", "YES"});

    update_config(config, {"NPUW_LLM_BATCH_DIM", kv_pos.batch});
    update_config(config, {"NPUW_LLM_SEQ_LEN_DIM", kv_pos.seq_len});

    update_config(config, {"NPUW_LLM_MAX_PROMPT_LEN", kv_desc.max_prompt_len});
    update_config(config, {"NPUW_LLM_MIN_RESPONSE_LEN", kv_desc.min_response_len});

    rename_key(config, "++PREFILL_CONFIG", "++NPUW_LLM_PREFILL_CONFIG");
    rename_key(config, "++GENERATE_CONFIG", "++NPUW_LLM_GENERATE_CONFIG");
    rename_key(config, "PREFILL_CONFIG", "NPUW_LLM_PREFILL_CONFIG");
    rename_key(config, "PREFILL_HINT", "NPUW_LLM_PREFILL_HINT");
    rename_key(config, "GENERATE_CONFIG", "NPUW_LLM_GENERATE_CONFIG");
    rename_key(config, "GENERATE_HINT", "NPUW_LLM_GENERATE_HINT");
}

inline bool is_paged_attention_available() {
#if defined(OPENVINO_ARCH_X86_64) || defined(OPENVINO_ARCH_ARM64)
    return true;
#else
    return false;
#endif
}

} // anonymous

namespace ov {
namespace genai {
namespace utils {

Tensor init_attention_mask(const Tensor& input_ids) {
    auto shape = input_ids.get_shape();
    auto attention_mask = ov::Tensor{input_ids.get_element_type(), shape};
    std::fill_n(attention_mask.data<int64_t>(), shape[0] * shape[1], 1);
    return attention_mask;
}

/**
 * Initializes position ids based on attention mask and starting position
 */
void initialize_position_ids(ov::Tensor& position_ids, const ov::Tensor& attention_mask, int64_t start_pos) {
    OPENVINO_ASSERT(position_ids.get_element_type() == ov::element::i64,
                    "position_ids tensor element type should be an i64");
    OPENVINO_ASSERT(position_ids.get_shape().size() == 2,
                    "position_ids tensor should of rank 2 with shape [batch_size, seq_len]");
    OPENVINO_ASSERT(attention_mask.get_element_type() == ov::element::i64,
                    "attention_mask tensor element type should be an i64");
    OPENVINO_ASSERT(attention_mask.get_shape().size() == 2,
                    "attention_mask tensor should of rank 2 with shape [batch_size, seq_len]");

    const size_t batch_size = attention_mask.get_shape()[0];
    const size_t seq_length = attention_mask.get_shape()[1];

    const int64_t* attention_mask_data = attention_mask.data<int64_t>();
    int64_t* position_ids_data = position_ids.data<int64_t>();

    for (size_t batch = 0; batch < batch_size; batch++) {
        size_t sum = start_pos;
        for (size_t i = 0; i < seq_length; i++) {
            const size_t element_offset = batch * seq_length + i;
            position_ids_data[element_offset] = sum;
            if (attention_mask_data[element_offset] == 1) {
                sum += 1;
            }
        }
    }
}

ov::genai::StreamerVariant get_streamer_from_map(const ov::AnyMap& config_map) {
    ov::genai::StreamerVariant streamer = std::monostate();

    if (config_map.count(STREAMER_ARG_NAME)) {
        auto any_val = config_map.at(STREAMER_ARG_NAME);
        if (any_val.is<std::shared_ptr<ov::genai::StreamerBase>>()) {
            streamer = any_val.as<std::shared_ptr<ov::genai::StreamerBase>>();
        } else if (any_val.is<std::function<bool(std::string)>>()) {
            streamer = any_val.as<std::function<bool(std::string)>>();
        } else if (any_val.is<std::function<StreamingStatus(std::string)>>()) {
            streamer = any_val.as<std::function<StreamingStatus(std::string)>>();
        }
    }
    return streamer;
}

std::shared_ptr<StreamerBase> create_streamer(StreamerVariant streamer, Tokenizer tokenizer) {
    return std::visit(overloaded{
        [](std::monostate) -> std::shared_ptr<StreamerBase> {
            return nullptr;
        },
        [](const std::shared_ptr<StreamerBase>& streamer) {
            return streamer;
        },
        [&tokenizer = tokenizer](const std::function<bool(std::string)>& streamer) -> std::shared_ptr<StreamerBase> {
            return std::make_unique<TextStreamer>(tokenizer, streamer);
        },
        [&tokenizer = tokenizer](const std::function<ov::genai::StreamingStatus(std::string)>& streamer) -> std::shared_ptr<StreamerBase> {
            return std::make_unique<TextStreamer>(tokenizer, streamer);
        }
    }, streamer);
}

ov::genai::OptionalGenerationConfig get_config_from_map(const ov::AnyMap& config_map) {
    if (config_map.count(CONFIG_ARG_NAME))
        return config_map.at(CONFIG_ARG_NAME).as<ov::genai::GenerationConfig>();
    else
        return std::nullopt;
}

ProcessorConfig from_any_map(
    const ov::AnyMap& config_map,
    const ProcessorConfig& initial
) {
    auto iter = config_map.find("processor_config");
    ProcessorConfig extracted_config = config_map.end() != iter ?
        iter->second.as<ProcessorConfig>() : initial;
    using utils::read_anymap_param;
    read_anymap_param(config_map, "patch_size", extracted_config.patch_size);
    read_anymap_param(config_map, "scale_resolution", extracted_config.scale_resolution);
    read_anymap_param(config_map, "max_slice_nums", extracted_config.max_slice_nums);
    read_anymap_param(config_map, "norm_mean", extracted_config.norm_mean);
    read_anymap_param(config_map, "norm_std", extracted_config.norm_std);
    return extracted_config;
}

ov::genai::TokenizedInputs subtract_chat_tokenized_inputs(const ov::genai::TokenizedInputs& minuend, const ov::genai::TokenizedInputs& subtrahend) {
    auto minuend_size = minuend.input_ids.get_size();
    auto subtrahend_size = subtrahend.input_ids.get_size();
    ov::Shape new_shape{1, minuend_size - subtrahend_size};

    ov::Tensor new_input_ids(ov::element::i64, new_shape);
    auto data_ptr = minuend.input_ids.data<int64_t>();
    std::copy(data_ptr + subtrahend_size, data_ptr + minuend_size, new_input_ids.data<int64_t>());

    ov::Tensor new_attention_mask(ov::element::i64, new_shape);
    std::fill_n(new_attention_mask.data<int64_t>(), new_shape[1], 1);

    return {new_input_ids, new_attention_mask};
}

namespace {

bool has_op_with_type(const std::shared_ptr<const ov::Model>& function, const std::string& type_name) {
    for (const auto& op : function->get_ops()) {
        if (op->get_type_name() == type_name) {
            return true;
        }
    }
    return false;
}

std::tuple<std::shared_ptr<ov::Node>, int64_t> find_llm_matmul(const std::shared_ptr<ov::Model>& model) {
    auto last_node = model->output(0).get_node()->input_value(0).get_node_shared_ptr();
    std::shared_ptr<ov::Node> matmul = ov::as_type_ptr<ov::op::v0::MatMul>(last_node);

    // in case of PA all tokens are moved to batch dimension and we have to slice / gather accordingly
    const bool pa_based_model = has_op_with_type(model, "PagedAttentionExtension");
    int64_t slice_gather_dim = pa_based_model ? 0 : 1;

    // There are several patterns for matmul we are looking for:
    // Matmul -> Result
    // Matmul -> Add -> Result
    // Matmul -> Transpose -> Result
    // MatMul -> Divide -> Tanh -> Multiply -> Result
    if (!matmul) {
        if (auto add = ov::as_type_ptr<ov::op::v1::Add>(last_node)) {
            matmul = ov::as_type_ptr<ov::op::v0::MatMul>(add->input_value(0).get_node_shared_ptr());
        } else if (auto transpose = ov::as_type_ptr<ov::op::v1::Transpose>(last_node)) {
            matmul = ov::as_type_ptr<ov::op::v0::MatMul>(transpose->input_value(0).get_node_shared_ptr());
            auto order = ov::as_type_ptr<ov::op::v0::Constant>(transpose->input_value(1).get_node_shared_ptr())->get_axis_vector_val();
            slice_gather_dim = order[slice_gather_dim];
        } else if (auto multiply = ov::as_type_ptr<ov::op::v1::Multiply>(last_node)) {
            if (auto tanh = ov::as_type_ptr<ov::op::v0::Tanh>(multiply->input_value(0).get_node_shared_ptr())) {
                if (auto divide = ov::as_type_ptr<ov::op::v1::Divide>(tanh->input_value(0).get_node_shared_ptr())) {
                    matmul = as_type_ptr<ov::op::v0::MatMul>(divide->input_value(0).get_node_shared_ptr());
                }
            }
        }
    }
    return std::make_tuple(matmul, slice_gather_dim);
}

} // namespace

void apply_slice_before_matmul_transformation(std::shared_ptr<ov::Model> model) {
    std::shared_ptr<ov::Node> matmul = nullptr;
    int64_t slice_gather_dim = -1;
    std::tie(matmul, slice_gather_dim) = find_llm_matmul(model);

    if (matmul && matmul->input(0).get_partial_shape().rank().get_length() == 3) {
        auto start = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{1}, std::vector<int64_t>{-1});
        auto stop = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{1}, std::vector<int64_t>{-2});
        auto step = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{1}, std::vector<int64_t>{-1});
        auto axis = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{1}, std::vector<int64_t>{slice_gather_dim});
        auto slice = std::make_shared<ov::op::v8::Slice>(matmul->input_value(0), start, stop, step, axis);
        matmul->input(0).replace_source_output(slice);
    }
}

void apply_gather_before_matmul_transformation(std::shared_ptr<ov::Model> model) {
    std::shared_ptr<ov::Node> matmul = nullptr;
    int64_t slice_gather_dim = -1;
    std::tie(matmul, slice_gather_dim) = find_llm_matmul(model);

    if (matmul && matmul->input(0).get_partial_shape().rank().get_length() == 3) {
        auto indices = std::make_shared<ov::op::v0::Parameter>(ov::element::i64, ov::PartialShape{-1});
        indices->set_friendly_name("sampled_tokens_indices");
        indices->output(0).get_tensor().set_names({"sampled_tokens_indices"});
        auto axis = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{1}, std::vector<int64_t>{slice_gather_dim});
        auto gather = std::make_shared<ov::op::v8::Gather>(matmul->input_value(0), indices, axis);
        matmul->input(0).replace_source_output(gather);
        model->add_parameters({indices});
    }
}

ov::Core singleton_core() {
    static ov::Core core;
    return core;
}


namespace {
bool is_gguf_model(const std::filesystem::path& file_path) {
    return file_path.extension() == ".gguf";
}

} // namespace

std::pair<ov::AnyMap, bool> extract_gguf_properties(const ov::AnyMap& external_properties) {
    bool enable_save_ov_model = false;
    ov::AnyMap properties = external_properties;

    auto it = properties.find(ov::genai::enable_save_ov_model.name());
    if (it != properties.end()) {
        enable_save_ov_model = it->second.as<bool>();
        properties.erase(it);
    }

    return {properties, enable_save_ov_model};
}

void save_openvino_model(const std::shared_ptr<ov::Model>& model, const std::string& save_path, bool compress_to_fp16) {
    try {
        auto serialize_start_time = std::chrono::high_resolution_clock::now();
        ov::save_model(model, save_path, compress_to_fp16);
        auto serialize_finish_time = std::chrono::high_resolution_clock::now();
        auto serialize_duration = std::chrono::duration_cast<std::chrono::milliseconds>(serialize_finish_time - serialize_start_time).count();
        std::stringstream ss;
        ss << "Save generated OpenVINO model to: " << save_path << " done. Time: " << serialize_duration << " ms";
        ov::genai::utils::print_gguf_debug_info(ss.str());
    }
    catch (const ov::Exception& e) {
        OPENVINO_THROW("Exception during model serialization ", e.what(), ", user can disable it by setting 'ov::genai::enable_save_ov_model' property to false");
    }
}

std::shared_ptr<ov::Model> read_model(const std::filesystem::path& model_dir,  const ov::AnyMap& properties) {
    auto [filtered_properties, enable_save_ov_model] = extract_gguf_properties(properties);
    if (is_gguf_model(model_dir)) {
#ifdef ENABLE_GGUF
        return create_from_gguf(model_dir.string(), enable_save_ov_model);
#else
        OPENVINO_ASSERT("GGUF support is switched off. Please, recompile with 'cmake -DENABLE_GGUF=ON'");
#endif
    } else {
        std::filesystem::path model_path = model_dir;

        if (std::filesystem::exists(model_dir / "openvino_model.xml")) {
            model_path = model_dir / "openvino_model.xml";
        } else if (std::filesystem::exists(model_dir / "openvino_language_model.xml")) {
            model_path = model_path / "openvino_language_model.xml";
        } else {
            OPENVINO_THROW("Could not find a model in the directory '", model_dir, "'");
        }

        return singleton_core().read_model(model_path, {}, filtered_properties);
    }
}

size_t get_first_history_difference(const ov::Tensor& encoded_history, const std::vector<int64_t> tokenized_history) {
    size_t idx = 0;
    auto encoded_history_data = encoded_history.data<int64_t>();
    while(idx < encoded_history.get_size() && idx < tokenized_history.size()) {
        if (encoded_history_data[idx] != tokenized_history[idx])
            break;
        idx++;
    }

    return idx;
}

KVAxesPosition get_kv_axes_pos(std::shared_ptr<const ov::Model> model) {
    // sequence length axis in key/values tensors, for most cases [BATCH_SIZE, num_kv_heads, seq_len, head_size],
    // therefore usually seq_length_axis = 2 and batch = 0
    KVAxesPosition kv_pos { 0u, 2u };

    // "ReadValue" node is KV cache representation in stateful model
    std::string kv_node_type_name = std::string(ov::op::v6::ReadValue::get_type_info_static().name);

    for (const auto op : model->get_ops()) {
        // check input size, as in LoRA adapters case it could be 0
        if (op->get_type_name() != kv_node_type_name || op->get_input_size() < 1) {
            continue;
        }

        // Shape example: [-1,4,0,64]
        auto shape = op->get_input_partial_shape(0);

        for (size_t i = 0; i < shape.rank().get_length(); i++) {
            // Find axis = 0. This would be sequence length axis.
            if (shape[i] == 0) {
                kv_pos.seq_len = i;
            } else if (shape[i].is_dynamic()) {
                // Dynamic axis is a batch
                kv_pos.batch = i;
            }
        }
        break;
    }

    return kv_pos;
}

void trim_kv_cache(ov::InferRequest request, KVCacheState& kv_cache_state, std::optional<AdapterController> adapter_controller) {
    if (kv_cache_state.reset_mem_state) {
        if (adapter_controller) {
            for(auto& state: request.query_state()) {
                if(!adapter_controller->has_state_name(state.get_name())) {
                    state.reset();
                }
            }
        } else {
            request.reset_state();
        }

        return;
    }

    // nothing to trim in this case
    if (kv_cache_state.num_tokens_to_trim == 0)
        return;

    auto states = request.query_state();

    OPENVINO_ASSERT(states.size() > 0, "Request contains no states.");

    for (auto& state : states) {
        if(adapter_controller && adapter_controller->has_state_name(state.get_name()))
            continue;

        ov::Tensor old_tensor = state.get_state();
        // [BATCH_SIZE, num_kv_heads, seq_len, head_size]
        auto shape = old_tensor.get_shape();
        shape[kv_cache_state.seq_length_axis] -= kv_cache_state.num_tokens_to_trim;

        ov::Coordinate new_shape_begin{0, 0, 0, 0};
        ov::Coordinate new_shape_end{shape};

        auto trimmed_tensor = ov::Tensor(old_tensor, new_shape_begin, new_shape_end);

        ov::Tensor new_tensor(old_tensor.get_element_type(), shape);
        trimmed_tensor.copy_to(new_tensor);

        state.set_state(new_tensor);
    }
}

ov::Tensor push_front_inputs(const ov::Tensor& base_tensor, int64_t add_to_front) {
    ov::Tensor new_tensor = ov::Tensor{ov::element::i64, {base_tensor.get_shape().at(0), base_tensor.get_shape().at(1) + 1}};
    auto new_tensor_data = new_tensor.data<int64_t>();
    new_tensor_data[0] = add_to_front;
    std::copy_n(base_tensor.data<int64_t>(), base_tensor.get_size(), new_tensor_data + 1);
    return new_tensor;
}

bool env_setup_for_print_debug_info() {
    // Specify the name of the environment variable
    const char* env_var_name = "OPENVINO_LOG_LEVEL";
    const char* env_var_value = std::getenv(env_var_name);
    // Check if the environment variable was found
    return (env_var_value != nullptr && atoi(env_var_value) > static_cast<int>(ov::log::Level::WARNING));
}

void print_compiled_model_properties(ov::CompiledModel& compiled_Model, const char* model_title) {
    if (!env_setup_for_print_debug_info()) {
        return;
    }
    // output of the actual settings that the device selected
    auto supported_properties = compiled_Model.get_property(ov::supported_properties);
    std::cout << "Model: " << model_title << std::endl;
    for (const auto& cfg : supported_properties) {
        if (cfg == ov::supported_properties)
            continue;
        auto prop = compiled_Model.get_property(cfg);
        if (cfg == ov::device::properties) {
            auto devices_properties = prop.as<ov::AnyMap>();
            for (auto& item : devices_properties) {
                std::cout << "  " << item.first << ": " << std::endl;
                for (auto& item2 : item.second.as<ov::AnyMap>()) {
                    std::cout << "    " << item2.first << ": " << item2.second.as<std::string>() << std::endl;
                }
            }
        } else {
            std::cout << "  " << cfg << ": " << prop.as<std::string>() << std::endl;
        }
    }

    ov::Core core;
    std::vector<std::string> exeTargets;
    exeTargets = compiled_Model.get_property(ov::execution_devices);
    std::cout << "EXECUTION_DEVICES:" << std::endl;
    for (const auto& device : exeTargets) {
        std::cout << " " << device << ": " << core.get_property(device, ov::device::full_name) << std::endl;
    }
}

void print_gguf_debug_info(const std::string &debug_info) {
    if (!env_setup_for_print_debug_info()) {
        return;
    }
    std::cout << "[GGUF Reader]: " << debug_info << std::endl;
}

std::pair<ov::CompiledModel, KVDesc>
compile_decoder_for_npu(const std::shared_ptr<ov::Model>& model,
                        const ov::AnyMap& config,
                        const KVAxesPosition& kv_pos) {
    ov::CompiledModel compiled;
    ov::AnyMap properties = config;
    KVDesc kv_desc;

    auto blob_path = pop_or_default(properties, "BLOB_PATH", std::string{});
    const auto export_blob = pop_or_default(properties, "EXPORT_BLOB", false);
    const bool do_import = (!blob_path.empty() && !export_blob);

    if (do_import) {
        if (!std::filesystem::exists(blob_path)) {
            OPENVINO_THROW("Blob file is not found at: " + blob_path);
        }
        std::ifstream fin(blob_path, std::ios::in | std::ios::binary);
        if (!fin.is_open()) {
            OPENVINO_THROW("Blob file can't be opened: " + blob_path);
        }
        compiled = ov::genai::utils::singleton_core().import_model(fin, "NPU", config);
        kv_desc.max_prompt_len = compiled.get_property("NPUW_LLM_MAX_PROMPT_LEN").as<uint32_t>();
        kv_desc.min_response_len = compiled.get_property("NPUW_LLM_MIN_RESPONSE_LEN").as<uint32_t>();
    } else {
        kv_desc.max_prompt_len = pop_int_and_cast(properties, "MAX_PROMPT_LEN").value_or(1024u);
        kv_desc.min_response_len = pop_int_and_cast(properties, "MIN_RESPONSE_LEN").value_or(128u);
        update_npu_config(properties, model, kv_pos, kv_desc);
        compiled = ov::genai::utils::singleton_core().compile_model(model, "NPU", properties);
        // Also export compiled model if required
        if (export_blob) {
            if (blob_path.empty()) {
                blob_path = "openvino_model.blob";
            }
            // Check the path is full
            const int EXT_SIZE = 5; // ".blob"
            if (blob_path.size() < EXT_SIZE) {
                OPENVINO_THROW("Please provide a full path to blob file in BLOB_PATH: " + blob_path);
            }
            if (strncmp(".blob", &blob_path[blob_path.size() - EXT_SIZE], EXT_SIZE) != 0) {
                OPENVINO_THROW("Please provide a full path to blob file in BLOB_PATH: " + blob_path);
            }
            std::ofstream fout(blob_path, std::ios::out | std::ios::binary);
            if (!fout.is_open()) {
                OPENVINO_THROW("Blob file can't be exported to: " + blob_path);
            }
            compiled.export_model(fout);
        }
    }
    return { compiled, kv_desc };
}

std::optional<ov::Any> pop_option(ov::AnyMap& config, const std::string& option_name) {
    if (auto it = config.find(option_name); it != config.end()) {
        std::optional<ov::Any> found = std::make_optional(it->second);
        config.erase(it);
        return found;
    }
    return std::nullopt;
}

const ModelsMap::mapped_type& get_model_weights_pair(const ModelsMap& models_map, const std::string& key) {
    auto it = models_map.find(key);
    if (it != models_map.end()) {
        return it->second;
    }
    OPENVINO_THROW("Model with key '", key, "' not found in models map.");
}

std::pair<ov::AnyMap, SchedulerConfig> extract_scheduler_config(const ov::AnyMap& properties, std::optional<SchedulerConfig> default_config) {
    ov::AnyMap plugin_config = properties;
    auto it = plugin_config.find(ov::genai::scheduler_config.name());
    SchedulerConfig scheduler_config;
    if (it != plugin_config.end()) {
        scheduler_config = it->second.as<SchedulerConfig>();
        plugin_config.erase(it);
    } else if (default_config.has_value()) {
        scheduler_config = *default_config;
    }
    return {plugin_config, scheduler_config};
};

SchedulerConfig get_latency_oriented_scheduler_config() {
    SchedulerConfig default_config;
    default_config.max_num_batched_tokens = std::numeric_limits<size_t>::max(); // don't limit total batch size
    default_config.enable_prefix_caching = true; // for better TTFT in chat scenarios
    return default_config;
}

bool explicitly_requires_paged_attention(const ov::AnyMap& properties) {
    auto attention_backend_it = properties.find("ATTENTION_BACKEND");

    if (properties.find(ov::genai::scheduler_config.name()) != properties.end() ||
        (attention_backend_it != properties.end() && attention_backend_it->second.as<std::string>() == PA_BACKEND)) {
        if (is_paged_attention_available()) {
            return true;
        } else {
            OPENVINO_THROW("Continuous batching backend requires PagedAttention operation support, which is available on x86_64 or ARM64 platforms only");
        }
    }
    if (properties.find(utils::DRAFT_MODEL_ARG_NAME) != properties.end()) {
        if (is_paged_attention_available()) {
            return true;
        } else {
            OPENVINO_THROW("Speculative decoding requires PagedAttention operation support, which is available on x86_64 or ARM64 platforms only");
        }
    }

    auto prompt_lookup_prop = properties.find("prompt_lookup");
    if (prompt_lookup_prop != properties.end() && prompt_lookup_prop->second.as<bool>() == true) {
        if (is_paged_attention_available()) {
            return true;
        } else {
            OPENVINO_THROW("Prompt lookup decoding requires PagedAttention operation support, which is available on x86_64 or ARM64 platforms only");
        }
    }
    return false;
}

std::pair<ov::AnyMap, std::string> extract_attention_backend(const ov::AnyMap& external_properties) {
    std::string attention_backend = PA_BACKEND;
    ov::AnyMap properties = external_properties;

    auto it = properties.find("ATTENTION_BACKEND");
    if (it != properties.end()) {
        attention_backend = it->second.as<std::string>();
        OPENVINO_ASSERT(attention_backend == PA_BACKEND || attention_backend == SDPA_BACKEND,
            "Attention backend must be either '", PA_BACKEND, "' or '", SDPA_BACKEND, "', got '", attention_backend, "'");
        properties.erase(it);
    }

    if (explicitly_requires_paged_attention(external_properties)) {
        OPENVINO_ASSERT(attention_backend == PA_BACKEND,
            "User properties are conflicting: some of them requires PagedAttention backend, while 'ATTENTION_BACKEND' is set to 'SDPA'");
    }

    return {properties, attention_backend};
};

void release_core_plugin(const std::string& device) {
    try {
        singleton_core().unload_plugin(device);
    } catch (const ov::Exception&) {
        // Note: in a theory it can throw an exception when 2 different pipelines are created from
        // different threads and then both of them unload plugin for 'device' from ov::Core
    }
}

}  // namespace utils
}  // namespace genai
}  // namespace ov

```

================================================================================


################################################################################
# documentation DIRECTORY
################################################################################


--- FILE: DEBUG_LOG.md ---
Path: documentation/DEBUG_LOG.md

```markdown 
## 1. Using Debug Log

There are six levels of logs, which can be called explicitly or set via the ``OPENVINO_LOG_LEVEL`` environment variable:

0 - ``ov::log::Level::NO``
1 - ``ov::log::Level::ERR``
2 - ``ov::log::Level::WARNING``
3 - ``ov::log::Level::INFO``
4 - ``ov::log::Level::DEBUG``
5 - ``ov::log::Level::TRACE``

When setting the environment variable OPENVINO_LOG_LEVEL > ov::log::Level::WARNING, the properties of the compiled model can be printed.

For example:

Linux - export OPENVINO_LOG_LEVEL=3
Windows - set OPENVINO_LOG_LEVEL=3

the properties of the compiled model are printed as follows:
```sh
    NETWORK_NAME: Model0
    OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1
    NUM_STREAMS: 1
    INFERENCE_NUM_THREADS: 48
    PERF_COUNT: NO
    INFERENCE_PRECISION_HINT: bf16
    PERFORMANCE_HINT: LATENCY
    EXECUTION_MODE_HINT: PERFORMANCE
    PERFORMANCE_HINT_NUM_REQUESTS: 0
    ENABLE_CPU_PINNING: YES
    SCHEDULING_CORE_TYPE: ANY_CORE
    MODEL_DISTRIBUTION_POLICY:
    ENABLE_HYPER_THREADING: NO
    EXECUTION_DEVICES: CPU
    CPU_DENORMALS_OPTIMIZATION: NO
    LOG_LEVEL: LOG_NONE
    CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1
    DYNAMIC_QUANTIZATION_GROUP_SIZE: 32
    KV_CACHE_PRECISION: f16
    AFFINITY: CORE
    EXECUTION_DEVICES:
    CPU: Intel(R) Xeon(R) Platinum 8468
```

When Speculative Decoding or Prompt Lookup pipeline is executed, performance metrics will be also printed.

For example:

```
===============================
Total duration, sec: 26.6217
Draft model duration, sec: 1.60329
Main model duration, sec: 25.0184
Draft model duration, %: 6.02248
Main model duration, %: 93.9775
AVG acceptance rate, %: 21.6809
===============================
REQUEST_ID: 0
Main model iterations: 47
Token per sec: 3.75633
AVG acceptance rate, %: 21.6809
Accepted tokens by draft model: 51
Generated tokens: 100
Accepted token rate, %: 51
===============================
Request_id: 0 ||| 40 0 40 20 0 0 40 40 0 20 20 20 0 40 0 0 20 80 0 80 20 0 0 0 40 80 0 40 60 40 80 0 0 0 0 40 20 20 0 40 20 40 0 20 0 0 0
```


When GGUF model passed to LLMPipeline, the details debug info will be also printed.

For example:
```
[GGUF Reader]: Loading and unpacking model from: gguf_models/qwen2.5-0.5b-instruct-q4_0.gguf
[GGUF Reader]: Loading and unpacking model done. Time: 196ms
[GGUF Reader]: Start generating OpenVINO model...
[GGUF Reader]: Save generated OpenVINO model to: gguf_models/openvino_model.xml done. Time: 466 ms
[GGUF Reader]: Model generation done. Time: 757ms
```

```

================================================================================


--- FILE: HOW_IT_WORKS.md ---
Path: documentation/HOW_IT_WORKS.md

```markdown 
# OpenVINO™ GenAI: How it works

## Stateful LLM

A common optimization for LLM inference is using a past KV (key/value)-cache. This cache is represented by the corresponding inputs and outputs in a model originally implemented in a DL framework (e.g. PyTorch models from Hugging Face). For further optimization and easier use, the model is transformed to a stateful form. This transformation improves inference performance and decreases the allocated runtime memory in long-running text generation scenarios. It is achieved by hiding inputs and outputs of the model that represent past KV-cache tensors and handling them inside the model in a more efficient way. Although the cache is still accessible with state API. It is opposed to stateless model approach requiring manipulating these inputs and outputs explicitly. An introduction to the stateful models can be found in the [Stateful Models article](https://docs.openvino.ai/2025/openvino-workflow/running-inference/stateful-models.html).

Hiding KV-cache introduces a peculiarity for beam search algorithm. Beam search suggests batched inference of multiple beams. The design described here so far would result in generating multiple independent sequences of tokens. Beam search algorithm, on the other hand, requires removing some of the ongoing beams and splitting other beams to multiple branches. Beam removal requires deleting corresponding KV-cache entry and beam splitting requires copying corresponding KV-cache values.

To provide the possibility to implement beam search without accessing model's internal state, a stateful LLM converted with `optimum-intel` or [llm_bench](../../tools/llm_bench) introduces an additional 1-dimentional `beam_idx` input. `beam_idx` must contain indexes of elements in a batch which are intended to be selected and will evolve during the next beam search iteration. There's only one beam when the generation starts. That beam corresponds to the initial prompt. `beam_idx` must have values: `[0, 0]` to keep the initial beam and introduce its copy. The dynamic batch size enables to change the number of beams dynamically. `beam_idx` must have `[1]` as the value to remove zeroth sequence and keep the second beam only.

Assume there are two running beams. To proceed with generating both beams at the next iteration, `beam_idx` values must be `[0, 1]`, pointing to batch elements `0` and `1`. To drop the last beam and split the other beam in two, `beam_idx` must be set to `[0, 0]`. This results in utilizing only the part of KV cache corresponding to the zeroth element in the batch. The process of selecting proper entries in cache is called Cache Reorder.

![](beam_idx-fork.gif)
![](beam_idx-drop.gif)

The images below represent stateless and stateful LLM pipelines. The model has 4 inputs:
1. `input_ids` contains the next selected token
2. `attention_mask` is filled with `1`
3. `position_ids` encodes a position of currently generating token in the sequence
4. `beam_idx` selects beams

The model has 1 output `logits` describing the predicted distribution over the next tokens. And there's KV cache state.

![](stateless.jpg)
![](stateful.jpg)

```

================================================================================


################################################################################
# python_bindings DIRECTORY
################################################################################


--- FILE: py_generation_config.cpp ---
Path: python_bindings/py_generation_config.cpp

```cpp 
// Copyright (C) 2023-2025 Intel Corporation
// SPDX-License-Identifier: Apache-2.0

#include <filesystem>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <pybind11/stl_bind.h>
#include <pybind11/stl/filesystem.h>
#include <pybind11/functional.h>

#include "py_utils.hpp"

namespace py = pybind11;
namespace pyutils = ov::genai::pybind::utils;

using ov::genai::StopCriteria;
using ov::genai::StructuralTagItem;
using ov::genai::StructuralTagsConfig;
using ov::genai::StructuredOutputConfig;
using ov::genai::GenerationConfig;

namespace {

auto stop_criteria_docstring =  R"(
    StopCriteria controls the stopping condition for grouped beam search.

    The following values are possible:
        "openvino_genai.StopCriteria.EARLY" stops as soon as there are `num_beams` complete candidates.
        "openvino_genai.StopCriteria.HEURISTIC" stops when is it unlikely to find better candidates.
        "openvino_genai.StopCriteria.NEVER" stops when there cannot be better candidates.
)";

auto structured_output_config_docstring = R"(
    Structure to keep generation config parameters for structured output generation.
    It is used to store the configuration for structured generation, which includes
    the JSON schema and other related parameters.

    Structured output parameters:
    json_schema:           if set, the output will be a JSON string constraint by the specified json-schema.
    regex:          if set, the output will be constraint by specified regex.
    grammar:        if set, the output will be constraint by specified grammar.
    structural_tags_config: if set, the output will be constraint by specified structural tags configuration.

)";

auto structured_tags_config_docstring = R"(
    Configures structured output generation by combining regular sampling with structural tags.

    When the model generates a trigger string, it switches to structured output mode and produces output
    based on the defined structural tags. Afterward, regular sampling resumes.

    Example:
      - Trigger "<func=" activates tags with begin "<func=sum>" or "<func=multiply>".

    Note:
      - Simple triggers like "<" may activate structured output unexpectedly if present in regular text.
      - Very specific or long triggers may be difficult for the model to generate,
      so structured output may not be triggered.

    Parameters:
    structural_tags: List of StructuralTagItem objects defining structural tags.
    triggers:        List of strings that trigger structured output generation.
                     Triggers may match the beginning or part of a tag's begin string.
)";

auto structured_tags_item_docstring = R"(
    Structure to keep generation config parameters for structural tags in structured output generation.
    It is used to store the configuration for a single structural tag item, which includes the begin string,
    schema, and end string.

    Parameters:
    begin:  the string that marks the beginning of the structural tag.
    schema: the JSON schema that defines the structure of the tag.
    end:    the string that marks the end of the structural tag.
)";

} // namespace

char generation_config_docstring[] = R"(
    Structure to keep generation config parameters. For a selected method of decoding, only parameters from that group
    and generic parameters are used. For example, if do_sample is set to true, then only generic parameters and random sampling parameters will
    be used while greedy and beam search parameters will not affect decoding at all.

    Parameters:
    max_length:    the maximum length the generated tokens can have. Corresponds to the length of the input prompt +
                   max_new_tokens. Its effect is overridden by `max_new_tokens`, if also set.
    max_new_tokens: the maximum numbers of tokens to generate, excluding the number of tokens in the prompt. max_new_tokens has priority over max_length.
    min_new_tokens: set 0 probability for eos_token_id for the first eos_token_id generated tokens.
    ignore_eos:    if set to true, then generation will not stop even if <eos> token is met.
    eos_token_id:  token_id of <eos> (end of sentence)
    stop_strings: a set of strings that will cause pipeline to stop generating further tokens.
    include_stop_str_in_output: if set to true stop string that matched generation will be included in generation output (default: false)
    stop_token_ids: a set of tokens that will cause pipeline to stop generating further tokens.
    echo:           if set to true, the model will echo the prompt in the output.
    logprobs:       number of top logprobs computed for each position, if set to 0, logprobs are not computed and value 0.0 is returned.
                    Currently only single top logprob can be returned, so any logprobs > 1 is treated as logprobs == 1. (default: 0).
    apply_chat_template: whether to apply chat_template for non-chat scenarios

    repetition_penalty: the parameter for repetition penalty. 1.0 means no penalty.
    presence_penalty: reduces absolute log prob if the token was generated at least once.
    frequency_penalty: reduces absolute log prob as many times as the token was generated.

    Beam search specific parameters:
    num_beams:         number of beams for beam search. 1 disables beam search.
    num_beam_groups:   number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.
    diversity_penalty: value is subtracted from a beam's score if it generates the same token as any beam from other group at a particular time.
    length_penalty:    exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to
        the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log
        likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while
        length_penalty < 0.0 encourages shorter sequences.
    num_return_sequences: the number of sequences to return for grouped beam search decoding.
    no_repeat_ngram_size: if set to int > 0, all ngrams of that size can only occur once.
    stop_criteria:        controls the stopping condition for grouped beam search. It accepts the following values:
        "openvino_genai.StopCriteria.EARLY", where the generation stops as soon as there are `num_beams` complete candidates;
        "openvino_genai.StopCriteria.HEURISTIC" is applied and the generation stops when is it very unlikely to find better candidates;
        "openvino_genai.StopCriteria.NEVER", where the beam search procedure only stops when there cannot be better candidates (canonical beam search algorithm).

    Random sampling parameters:
    temperature:        the value used to modulate token probabilities for random sampling.
    top_p:              if set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.
    top_k:              the number of highest probability vocabulary tokens to keep for top-k-filtering.
    do_sample:          whether or not to use multinomial random sampling that add up to `top_p` or higher are kept.
    num_return_sequences: the number of sequences to generate from a single prompt.
)";

void init_generation_config(py::module_& m) {
    // Binding for StopCriteria
    py::enum_<StopCriteria>(m, "StopCriteria", stop_criteria_docstring)
        .value("EARLY", StopCriteria::EARLY)
        .value("HEURISTIC", StopCriteria::HEURISTIC)
        .value("NEVER", StopCriteria::NEVER);


    py::class_<StructuralTagItem>(m, "StructuralTagItem", structured_tags_item_docstring)
        .def(py::init<>(), "Default constructor for StructuralTagItem")
        .def(py::init([](py::kwargs kwargs) {
            return StructuralTagItem(pyutils::kwargs_to_any_map(kwargs));
        }), "Constructor that initializes the structured tags configuration with kwargs.")
        .def_readwrite("begin", &StructuralTagItem::begin, "Begin string for Structural Tag Item")
        .def_readwrite("schema", &StructuralTagItem::schema, "Json schema for Structural Tag Item")
        .def_readwrite("end", &StructuralTagItem::end, "End string for Structural Tag Item")
        .def("__repr__",
            [](const StructuralTagItem &self) {
                return "StructuralTagItem(begin=" + py::repr(py::cast(self.begin)).cast<std::string>() +
                       ", schema=" + py::repr(py::cast(self.schema)).cast<std::string>() +
                       ", end=" + py::repr(py::cast(self.end)).cast<std::string>() + ")";
            }
        );


    py::class_<StructuralTagsConfig>(m, "StructuralTagsConfig", structured_tags_config_docstring)
        .def(py::init<>(), "Default constructor for StructuralTagsConfig")
        .def(py::init([](py::kwargs kwargs) {
            return StructuralTagsConfig(pyutils::kwargs_to_any_map(kwargs));
        }), "Constructor that initializes the structured tags configuration with kwargs.")
        .def_readwrite("structural_tags", &StructuralTagsConfig::structural_tags, "List of structural tag items for structured output generation")
        .def_readwrite("triggers", &StructuralTagsConfig::triggers, "List of strings that will trigger generation of structured output")
        .def("__repr__",
            [](const StructuralTagsConfig &self) {
                return "StructuralTagsConfig(structural_tags=" + py::repr(py::cast(self.structural_tags)).cast<std::string>() +
                       ", triggers=" + py::repr(py::cast(self.triggers)).cast<std::string>() + ")";
            }
        );

    py::class_<StructuredOutputConfig>(m, "StructuredOutputConfig", structured_output_config_docstring)
        .def(py::init<>(), "Default constructor for StructuredOutputConfig")
        .def(py::init([](py::kwargs kwargs) {
            return StructuredOutputConfig(pyutils::kwargs_to_any_map(kwargs));
        }), "Constructor that initializes the structured output configuration with kwargs.")
        .def_readwrite("json_schema", &StructuredOutputConfig::json_schema, "JSON schema for structured output generation")
        .def_readwrite("regex", &StructuredOutputConfig::regex, "Regular expression for structured output generation")
        .def_readwrite("grammar", &StructuredOutputConfig::grammar, "Grammar for structured output generation")
        .def_readwrite("structural_tags_config", &StructuredOutputConfig::structural_tags_config, "Configuration for structural tags in structured output generation")
        .def("__repr__",
            [](const StructuredOutputConfig &self) {
                return "StructuredOutputConfig(json_schema=" + py::repr(py::cast(self.json_schema)).cast<std::string>() +
                       ", regex=" + py::repr(py::cast(self.regex)).cast<std::string>() +
                       ", grammar=" + py::repr(py::cast(self.grammar)).cast<std::string>() +
                       ", structural_tags_config=" + py::repr(py::cast(self.structural_tags_config)).cast<std::string>() + ")";
            }
        );

     // Binding for GenerationConfig
    py::class_<GenerationConfig>(m, "GenerationConfig", generation_config_docstring)
        .def(py::init<std::filesystem::path>(), py::arg("json_path"), "path where generation_config.json is stored")
        .def(py::init([](py::kwargs kwargs) { return *pyutils::update_config_from_kwargs(GenerationConfig(), kwargs); }))
        .def_readwrite("max_new_tokens", &GenerationConfig::max_new_tokens)
        .def_readwrite("max_length", &GenerationConfig::max_length)
        .def_readwrite("ignore_eos", &GenerationConfig::ignore_eos)
        .def_readwrite("min_new_tokens", &GenerationConfig::min_new_tokens)
        .def_readwrite("num_beam_groups", &GenerationConfig::num_beam_groups)
        .def_readwrite("num_beams", &GenerationConfig::num_beams)
        .def_readwrite("diversity_penalty", &GenerationConfig::diversity_penalty)
        .def_readwrite("length_penalty", &GenerationConfig::length_penalty)
        .def_readwrite("num_return_sequences", &GenerationConfig::num_return_sequences)
        .def_readwrite("no_repeat_ngram_size", &GenerationConfig::no_repeat_ngram_size)
        .def_readwrite("stop_criteria", &GenerationConfig::stop_criteria)
        .def_readwrite("temperature", &GenerationConfig::temperature)
        .def_readwrite("top_p", &GenerationConfig::top_p)
        .def_readwrite("top_k", &GenerationConfig::top_k)
        .def_readwrite("do_sample", &GenerationConfig::do_sample)
        .def_readwrite("repetition_penalty", &GenerationConfig::repetition_penalty)
        .def_readwrite("eos_token_id", &GenerationConfig::eos_token_id)
        .def_readwrite("presence_penalty", &GenerationConfig::presence_penalty)
        .def_readwrite("frequency_penalty", &GenerationConfig::frequency_penalty)
        .def_readwrite("rng_seed", &GenerationConfig::rng_seed)
        .def_readwrite("stop_strings", &GenerationConfig::stop_strings)
        .def_readwrite("echo", &GenerationConfig::echo)
        .def_readwrite("logprobs", &GenerationConfig::logprobs)
        .def_readwrite("assistant_confidence_threshold", &GenerationConfig::assistant_confidence_threshold)
        .def_readwrite("num_assistant_tokens", &GenerationConfig::num_assistant_tokens)
        .def_readwrite("max_ngram_size", &GenerationConfig::max_ngram_size)
        .def_readwrite("include_stop_str_in_output", &GenerationConfig::include_stop_str_in_output)
        .def_readwrite("stop_token_ids", &GenerationConfig::stop_token_ids)
        .def_readwrite("structured_output_config", &GenerationConfig::structured_output_config)
        .def_readwrite("adapters", &GenerationConfig::adapters)
        .def_readwrite("apply_chat_template", &GenerationConfig::apply_chat_template)
        .def("set_eos_token_id", &GenerationConfig::set_eos_token_id, py::arg("tokenizer_eos_token_id"))
        .def("is_beam_search", &GenerationConfig::is_beam_search)
        .def("is_greedy_decoding", &GenerationConfig::is_greedy_decoding)
        .def("is_multinomial", &GenerationConfig::is_multinomial)
        .def("is_assisting_generation", &GenerationConfig::is_assisting_generation)
        .def("is_prompt_lookup", &GenerationConfig::is_prompt_lookup)
        .def("validate", &GenerationConfig::validate)
        .def("update_generation_config", [](
            ov::genai::GenerationConfig& config,
            const py::kwargs& kwargs) {
            config.update_generation_config(pyutils::kwargs_to_any_map(kwargs));
        });
   }

```

================================================================================


--- FILE: py_llm_pipeline.cpp ---
Path: python_bindings/py_llm_pipeline.cpp

```cpp 
// Copyright (C) 2023-2025 Intel Corporation
// SPDX-License-Identifier: Apache-2.0

#include <filesystem>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <pybind11/stl_bind.h>
#include <pybind11/stl/filesystem.h>
#include <pybind11/functional.h>

#include "openvino/genai/llm_pipeline.hpp"

#include "tokenizer/tokenizers_path.hpp"
#include "py_utils.hpp"

namespace py = pybind11;
namespace pyutils = ov::genai::pybind::utils;

using ov::genai::OptionalGenerationConfig;
using ov::genai::LLMPipeline;
using ov::genai::TokenizedInputs;
using ov::genai::EncodedInputs;
using ov::genai::StreamerVariant;
using ov::genai::DecodedResults;
using ov::genai::Tokenizer;
using ov::genai::draft_model;

namespace {

auto generate_docstring = R"(
    Generates sequences or tokens for LLMs. If input is a string or list of strings then resulting sequences will be already detokenized.

    :param inputs: inputs in the form of string, list of strings or tokenized input_ids
    :type inputs: str, list[str], ov.genai.TokenizedInputs, or ov.Tensor

    :param generation_config: generation_config
    :type generation_config: GenerationConfig or a dict

    :param streamer: streamer either as a lambda with a boolean returning flag whether generation should be stopped
    :type : Callable[[str], bool], ov.genai.StreamerBase

    :param kwargs: arbitrary keyword arguments with keys corresponding to GenerationConfig fields.
    :type : dict

    :return: return results in encoded, or decoded form depending on inputs type
    :rtype: DecodedResults, EncodedResults, str
)";

py::object call_common_generate(
    LLMPipeline& pipe,
    const std::variant<ov::Tensor, TokenizedInputs, std::string, std::vector<std::string>>& inputs,
    const OptionalGenerationConfig& config,
    const pyutils::PyBindStreamerVariant& py_streamer,
    const py::kwargs& kwargs
) {
    ov::genai::GenerationConfig default_config = config.has_value() ? *config : pipe.get_generation_config();
    auto updated_config = pyutils::update_config_from_kwargs(default_config, kwargs);

    py::object results;
    StreamerVariant streamer = pyutils::pystreamer_to_streamer(py_streamer);

    // Call suitable generate overload for each type of input.
    std::visit(pyutils::overloaded {
    [&](ov::Tensor ov_tensor) {
        ov::genai::EncodedResults encoded_results;
        {
            py::gil_scoped_release rel;
            encoded_results = pipe.generate(ov_tensor, updated_config, streamer);
        }
        results = py::cast(encoded_results);
    },
    [&](TokenizedInputs tokenized_input) {
        ov::genai::EncodedResults encoded_results;
        {
            py::gil_scoped_release rel;
            encoded_results = pipe.generate(tokenized_input, updated_config, streamer);
        }
        results = py::cast(encoded_results);
    },
    [&](std::string string_input) {
        DecodedResults res;
        {
            py::gil_scoped_release rel;
            res = pipe.generate(string_input, updated_config, streamer);
        }
        // If input was a string return a single string otherwise return DecodedResults.
        if (updated_config.has_value() && (*updated_config).num_return_sequences == 1) {
            results = py::cast<py::object>(pyutils::handle_utf8(res.texts[0]));
        } else {
            results = py::cast(res);
        }
    },
    [&](std::vector<std::string> string_input) {
        // For DecodedResults texts getter already handles utf8 decoding.
        DecodedResults res;
        {
            py::gil_scoped_release rel;
            res = pipe.generate(string_input, updated_config, streamer);
        }
        results = py::cast(res);
    }},
    inputs);
    return results;
}

} // namespace

extern char generation_config_docstring[];

void init_llm_pipeline(py::module_& m) {
    py::class_<LLMPipeline>(m, "LLMPipeline", "This class is used for generation with LLMs")
        // init(model_path, tokenizer, device, config, kwargs) should be defined before init(model_path, device, config, kwargs) 
        // to prevent tokenizer treated as kwargs argument
        .def(py::init([](
            const std::filesystem::path& models_path,
            const Tokenizer& tokenizer,
            const std::string& device,
            const std::map<std::string, py::object>& config,
            const py::kwargs& kwargs
        ) {
            ScopedVar env_manager(pyutils::ov_tokenizers_module_path());
            ov::AnyMap properties = pyutils::kwargs_to_any_map(kwargs);
            if (config.size()) {
                PyErr_WarnEx(PyExc_DeprecationWarning, 
                         "'config' parameters is deprecated, please use kwargs to pass config properties instead.", 
                         1);
                auto config_properties = pyutils::properties_to_any_map(config);
                properties.insert(config_properties.begin(), config_properties.end());
            }
            return std::make_unique<LLMPipeline>(models_path, tokenizer, device, properties);
        }),
        py::arg("models_path"),
        py::arg("tokenizer"),
        py::arg("device"),
        py::arg("config") = ov::AnyMap({}), "openvino.properties map",
        R"(
            LLMPipeline class constructor for manually created openvino_genai.Tokenizer.
            models_path (os.PathLike): Path to the model file.
            tokenizer (openvino_genai.Tokenizer): tokenizer object.
            device (str): Device to run the model on (e.g., CPU, GPU). Default is 'CPU'.
            Add {"scheduler_config": ov_genai.SchedulerConfig} to config properties to create continuous batching pipeline.
            kwargs: Device properties.
        )")

        .def(py::init([](
            const std::filesystem::path& models_path,
            const std::string& device,
            const std::map<std::string, py::object>& config,
            const py::kwargs& kwargs
        ) {
            ScopedVar env_manager(pyutils::ov_tokenizers_module_path());
            ov::AnyMap properties = pyutils::kwargs_to_any_map(kwargs);
            if (config.size()) {
                PyErr_WarnEx(PyExc_DeprecationWarning, 
                         "'config' parameters is deprecated, please use kwargs to pass config properties instead.", 
                         1);
                auto config_properties = pyutils::properties_to_any_map(config);
                properties.insert(config_properties.begin(), config_properties.end());
            }
            return std::make_unique<LLMPipeline>(models_path, device, properties);
        }),
        py::arg("models_path"), "folder with openvino_model.xml and openvino_tokenizer[detokenizer].xml files",
        py::arg("device"), "device on which inference will be done",
        py::arg("config") = ov::AnyMap({}), "openvino.properties map",
        R"(
            LLMPipeline class constructor.
            models_path (os.PathLike): Path to the model file.
            device (str): Device to run the model on (e.g., CPU, GPU). Default is 'CPU'.
            Add {"scheduler_config": ov_genai.SchedulerConfig} to config properties to create continuous batching pipeline.
            kwargs: Device properties.
        )")

        .def(py::init([](
            const std::string& model,
            const ov::Tensor& weights,
            const ov::genai::Tokenizer& tokenizer,
            const std::string& device,
            OptionalGenerationConfig generation_config,
            const py::kwargs& kwargs
        ) {
            ScopedVar env_manager(pyutils::ov_tokenizers_module_path());
            ov::AnyMap properties = pyutils::kwargs_to_any_map(kwargs);
            if (!generation_config.has_value()) {
                generation_config = ov::genai::GenerationConfig();
            }
            return std::make_unique<LLMPipeline>(model, weights, tokenizer, device, properties, *generation_config);
        }),
        py::arg("model"), "string with pre-read model",
        py::arg("weights"), "ov::Tensor with pre-read model weights",
        py::arg("tokenizer"), "genai Tokenizers",
        py::arg("device"), "device on which inference will be done",
        py::arg("generation_config") = py::none(), "genai GenerationConfig (default: None, will use empty config)",
        R"(
            LLMPipeline class constructor.
            model (str): Pre-read model.
            weights (ov.Tensor): Pre-read model weights.
            tokenizer (str): Genai Tokenizers.
            device (str): Device to run the model on (e.g., CPU, GPU).
            generation_config {ov_genai.GenerationConfig} Genai GenerationConfig. Default is an empty config.
            kwargs: Device properties.
        )")

        .def(
            "generate",
            [](LLMPipeline& pipe,
                const std::variant<ov::Tensor, TokenizedInputs, std::string, std::vector<std::string>>& inputs,
                const OptionalGenerationConfig& generation_config,
                const pyutils::PyBindStreamerVariant& streamer,
                const py::kwargs& kwargs
            ) -> py::typing::Union<ov::genai::EncodedResults, ov::genai::DecodedResults> {
                return call_common_generate(pipe, inputs, generation_config, streamer, kwargs);
            },
            py::arg("inputs"), "Input string, or list of string or encoded tokens",
            py::arg("generation_config") = std::nullopt, "generation_config",
            py::arg("streamer") = std::monostate(), "streamer",
            (generate_docstring + std::string(" \n ") + generation_config_docstring).c_str()
        )

        .def(
            "__call__",
            [](LLMPipeline& pipe,
                const std::variant<ov::Tensor, TokenizedInputs, std::string, std::vector<std::string>>& inputs,
                const OptionalGenerationConfig& generation_config,
                const pyutils::PyBindStreamerVariant& streamer,
                const py::kwargs& kwargs
            ) -> py::typing::Union<ov::genai::EncodedResults, ov::genai::DecodedResults> {
                return call_common_generate(pipe, inputs, generation_config, streamer, kwargs);
            },
            py::arg("inputs"), "Input string, or list of string or encoded tokens",
            py::arg("generation_config") = std::nullopt, "generation_config",
            py::arg("streamer") = std::monostate(), "streamer",
            (generate_docstring + std::string(" \n ") + generation_config_docstring).c_str()
        )

        .def("get_tokenizer", &LLMPipeline::get_tokenizer)
        .def("start_chat", &LLMPipeline::start_chat, py::arg("system_message") = "")
        .def("finish_chat", &LLMPipeline::finish_chat)
        .def("get_generation_config", &LLMPipeline::get_generation_config, py::return_value_policy::copy)
        .def("set_generation_config", &LLMPipeline::set_generation_config, py::arg("config"));

    m.def("draft_model", [](
            const std::filesystem::path& models_path,
            const std::string& device,
            const py::kwargs& kwargs
        ) {
            ScopedVar env_manager(pyutils::ov_tokenizers_module_path());
            return draft_model(models_path, device, pyutils::kwargs_to_any_map(kwargs)).second;
        },
        py::arg("models_path"), "folder with openvino_model.xml and openvino_tokenizer[detokenizer].xml files",
        py::arg("device") = "", "device on which inference will be performed");
}

```

================================================================================


--- FILE: py_streamers.cpp ---
Path: python_bindings/py_streamers.cpp

```cpp 
// Copyright (C) 2023-2025 Intel Corporation
// SPDX-License-Identifier: Apache-2.0

#include <filesystem>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <pybind11/stl_bind.h>
#include <pybind11/functional.h>

#include "openvino/genai/streamer_base.hpp"
#include "openvino/genai/text_streamer.hpp"
#include "py_utils.hpp"

namespace py = pybind11;

using ov::genai::CallbackTypeVariant;
using ov::genai::StreamingStatus;
using ov::genai::TextStreamer;
using ov::genai::Tokenizer;

namespace pyutils = ov::genai::pybind::utils;

namespace {

auto streamer_base_docstring =  R"(
    Base class for streamers. In order to use inherit from from this class and implement write and end methods.
)";

auto text_streamer_docstring =  R"(
TextStreamer is used to decode tokens into text and call a user-defined callback function.

tokenizer: Tokenizer object to decode tokens into text.
callback: User-defined callback function to process the decoded text, callback should return either boolean flag or StreamingStatus.

)";

class ConstructableStreamer: public StreamerBase {
    OPENVINO_SUPPRESS_DEPRECATED_START
    bool put(int64_t token) override {
        PYBIND11_OVERRIDE(
            bool,  // Return type
            StreamerBase,  // Parent class
            put,  // Name of function in C++ (must match Python name)
            token  // Argument(s)
        );
    }
    OPENVINO_SUPPRESS_DEPRECATED_END
    StreamingStatus write(int64_t token) override {
        PYBIND11_OVERRIDE(
            StreamingStatus,  // Return type
            StreamerBase,  // Parent class
            write,  // Name of function in C++ (must match Python name)
            token  // Argument(s)
        );
    }
    StreamingStatus write(const std::vector<int64_t>& token) override {
        PYBIND11_OVERRIDE(
            StreamingStatus,  // Return type
            StreamerBase,  // Parent class
            write,  // Name of function in C++ (must match Python name)
            token  // Argument(s)
        );
    }
    void end() override {
        PYBIND11_OVERRIDE_PURE(void, StreamerBase, end);
    }
};

} // namespace

void init_streamers(py::module_& m) {
    py::enum_<ov::genai::StreamingStatus>(m, "StreamingStatus")
        .value("RUNNING", ov::genai::StreamingStatus::RUNNING)
        .value("CANCEL", ov::genai::StreamingStatus::CANCEL)
        .value("STOP", ov::genai::StreamingStatus::STOP);

    auto streamer = py::class_<StreamerBase, ConstructableStreamer, std::shared_ptr<StreamerBase>>(m, "StreamerBase", streamer_base_docstring)  // Change the holder form unique_ptr to shared_ptr
        .def(py::init<>())
        .def("write",
            [](StreamerBase& self, std::variant<int64_t, std::vector<int64_t>> token) {
               if (auto _token = std::get_if<int64_t>(&token)) {
                   return self.write(*_token);
               } else {
                   auto tokens = std::get<std::vector<int64_t>>(token);
                   return self.write(tokens);
               }
            },
            "Write is called every time new token or vector of tokens is decoded. Returns a StreamingStatus flag to indicate whether generation should be stopped or cancelled",
            py::arg("token"))
        .def("end", &StreamerBase::end, "End is called at the end of generation. It can be used to flush cache if your own streamer has one");
    OPENVINO_SUPPRESS_DEPRECATED_START
    streamer.def("put", &StreamerBase::put, "Put is called every time new token is decoded. Returns a bool flag to indicate whether generation should be stopped, if return true generation stops", py::arg("token"));
    OPENVINO_SUPPRESS_DEPRECATED_END

    py::class_<TextStreamer, std::shared_ptr<TextStreamer>, StreamerBase>(m, "TextStreamer", text_streamer_docstring)
        .def(py::init<const Tokenizer&, std::function<CallbackTypeVariant(std::string)>>(), py::arg("tokenizer"), py::arg("callback"))
        .def("write", 
            [](TextStreamer& self, std::variant<int64_t, std::vector<int64_t>> token) {
                if (auto _token = std::get_if<int64_t>(&token)) {
                    return self.write(*_token);
                } else {
                    auto tokens = std::get<std::vector<int64_t>>(token);
                    return self.write(tokens);
                }
            },
            py::arg("token"))
        .def("end", &TextStreamer::end);
}

```

================================================================================


################################################################################
# CONTEXT README - USAGE GUIDE
################################################################################

```markdown
# OpenVINO GenAI Context for LLM Copilot

**Generated**: January 8, 2025  
**Version**: 1.0.0  
**Purpose**: Curated essential files for LLM understanding of OpenVINO GenAI

## 🎯 Overview

This context folder contains the most critical OpenVINO GenAI files for building robust, performance-optimized Gradio chat applications. Each file has been carefully selected based on its importance for understanding API patterns, avoiding common pitfalls, and implementing advanced features.

---

## 📁 Complete Directory Structure

```
context/
├── python_samples/          # 🟢 Python API examples (4 files)
│   ├── chat_sample.py ⭐⭐⭐⭐⭐          # Session management API
│   ├── benchmark_genai.py ⭐⭐⭐⭐⭐      # Performance patterns  
│   ├── greedy_causal_lm.py ⭐⭐⭐⭐       # Basic generation patterns
│   └── multinomial_causal_lm.py ⭐⭐⭐⭐   # Advanced sampling techniques
├── test_configs/           # 🔧 Configuration patterns (3 files)
│   ├── generation_config.py ⭐⭐⭐⭐⭐     # Complete config reference
│   ├── ov_genai_pipelines.py ⭐⭐⭐⭐      # Pipeline initialization
│   └── hugging_face.py ⭐⭐⭐             # Tokenizer integration
├── core_cpp/              # ⚙️ C++ implementation (3 files)
│   ├── utils.cpp ⭐⭐⭐⭐                 # NPU detection & NPUW
│   ├── pipeline_stateful.cpp ⭐⭐⭐       # Stateful pipeline logic
│   └── generation_config.cpp ⭐⭐⭐       # Config internals
├── documentation/         # 📚 Architecture guides (2 files)
│   ├── HOW_IT_WORKS.md ⭐⭐⭐⭐           # Architecture guide
│   └── DEBUG_LOG.md ⭐⭐⭐               # Troubleshooting guide
├── python_bindings/       # 🔗 C++ to Python bindings (3 files)
│   ├── py_llm_pipeline.cpp ⭐⭐⭐⭐        # LLMPipeline binding
│   ├── py_generation_config.cpp ⭐⭐⭐⭐   # GenerationConfig binding
│   └── py_streamers.cpp ⭐⭐⭐           # Streaming implementation
├── gradio_patterns/        # 🎨 Official Gradio integration (5 files)
│   ├── chatbot_streaming.py ⭐⭐⭐⭐⭐     # Official streaming patterns
│   ├── chatinterface_advanced.py ⭐⭐⭐⭐⭐ # Advanced chat interfaces
│   ├── performance_dashboard.py ⭐⭐⭐⭐   # Monitoring & analytics
│   ├── concurrency_queue.py ⭐⭐⭐⭐      # Resource management
│   └── llm_hf_integration.py ⭐⭐⭐       # HuggingFace integration
├── gradio_testing/         # 🧪 Professional testing (2 files)
│   ├── chat_interface_tests.py ⭐⭐⭐⭐    # Interface validation
│   └── streaming_tests.py ⭐⭐⭐⭐        # Streaming performance
├── qwen3_model_context/    # 🎯 Qwen3-specific optimization (4 files)
│   ├── model_architecture.py ⭐⭐⭐⭐⭐    # Complete Qwen3 specs
│   ├── special_tokens.py ⭐⭐⭐⭐⭐       # Token handling & templates
│   ├── npu_optimization.py ⭐⭐⭐⭐⭐     # NPU deployment guide
│   └── README.md ⭐⭐⭐⭐                # Model-specific guide
└── README.md             # 📋 This comprehensive guide

Total: 24 files across 7 directories
```

---

## 🥇 Priority 5 Files (Critical)

### `python_samples/chat_sample.py`
**What it is**: The official chat implementation pattern  
**Why critical**: Shows the **correct** way to handle chat sessions  
**Key insights**: 
- `pipe.start_chat()` / `pipe.finish_chat()` session management
- Proper streaming with `StreamingStatus.RUNNING`
- Minimal, working configuration patterns

**How to use**: Study this before building any chat interface. Your current Gradio scripts are missing the session management!

### `python_samples/benchmark_genai.py`
**What it is**: Performance optimization and metrics collection  
**Why critical**: Professional-grade performance patterns  
**Key insights**:
- Advanced configuration with `SchedulerConfig`
- Performance metrics collection (`perf_metrics`)
- Batch processing and warmup strategies
- Proper tokenizer integration

**How to use**: Copy the performance measurement patterns into your Gradio apps.

### `test_configs/generation_config.py`
**What it is**: Complete catalog of all `GenerationConfig` parameters  
**Why critical**: Prevents C++ binding errors (like your `pad_token_id` issue!)  
**Key insights**:
- Every supported parameter with working examples
- Proper construction patterns (empty constructor + attribute setting)
- Beam search, sampling, and penalty configurations
- Safe parameter combinations

**How to use**: Reference this whenever setting generation parameters. Never guess - always verify here first!

---

## 🥈 Priority 4 Files (High Value)

### `core_cpp/utils.cpp`
**What it is**: Core C++ utilities including NPU device handling  
**Why important**: Explains NPU auto-configuration and NPUW settings  
**Key insights**: 
- `update_npu_config()` function (lines 76-95)
- Automatic NPUW property setting
- Device detection and validation logic
- Cache management implementation

**How to use**: Understand why your NPU configurations succeed or fail.

### `documentation/HOW_IT_WORKS.md`
**What it is**: Core architecture documentation  
**Why important**: Deep understanding of stateful models and KV-cache  
**Key insights**:
- Stateful vs stateless model differences  
- KV-cache mechanics and memory management
- Beam search implementation details
- Token limit implications

**How to use**: Read this to understand why certain token limits exist and how to optimize memory usage.

---

## 🟢 Gradio Integration Patterns (New)

### `gradio_patterns/` ⭐⭐⭐⭐⭐
**What it contains**: Official Gradio patterns extracted and optimized for OpenVINO GenAI  
**Why critical**: Bridges gap between official Gradio best practices and OpenVINO implementation  
**Key components**:
- `chatbot_streaming.py` - Official streaming patterns with ChatInterface
- `chatinterface_advanced.py` - System prompts, parameter controls, multi-modal
- `performance_dashboard.py` - Professional monitoring and analytics
- `concurrency_queue.py` - NPU resource management and queue handling
- `llm_hf_integration.py` - HuggingFace tokenizer integration patterns

**How to use**: Copy these patterns directly into your Gradio applications for production-quality interfaces.

### `gradio_testing/` ⭐⭐⭐⭐
**What it contains**: Comprehensive testing patterns for chat interfaces  
**Why important**: Professional testing methodologies for streaming and chat functionality  
**Key components**:
- `chat_interface_tests.py` - Chat interface validation and testing
- `streaming_tests.py` - Performance and reliability testing for streaming

**How to use**: Use these test patterns to validate your OpenVINO GenAI applications.

### `qwen3_model_context/` ⭐⭐⭐⭐⭐
**What it contains**: Model-specific knowledge for Qwen3-8B optimization  
**Why critical**: Addresses model-specific requirements and NPU optimization  
**Key components**:
- `model_architecture.py` - Complete Qwen3 specs and configuration
- `special_tokens.py` - 26+ special tokens, chat templates, filtering
- `npu_optimization.py` - NPUW configuration, deployment, troubleshooting

**How to use**: Essential for Qwen3 deployments - provides missing model-specific context.

---

## 🥈 Priority 4 Files (High Value)

### `python_samples/greedy_causal_lm.py`
**What it is**: Basic text generation without sampling  
**Why important**: Simplest, most reliable generation pattern  
**Key insights**: Greedy decoding, minimal configuration, deterministic output  
**How to use**: Start with this pattern before adding sampling complexity.

### `test_configs/ov_genai_pipelines.py`
**What it is**: Pipeline initialization and configuration patterns  
**Why important**: Shows proper pipeline setup with various devices and configs  
**Key insights**: Device handling, scheduler configuration, error handling  
**How to use**: Reference for robust pipeline initialization.

### `python_bindings/py_generation_config.cpp`
**What it is**: C++ binding for GenerationConfig class  
**Why important**: Explains why certain parameters work/fail  
**Key insights**: Supported attributes, type validation, binding implementation  
**How to use**: Debug C++ binding errors and understand parameter limitations.

### `python_bindings/py_streamers.cpp`
**What it is**: C++ implementation of streaming classes  
**Why important**: Shows proper streaming implementation patterns  
**Key insights**: Token-level streaming, cleanup, thread safety  
**How to use**: Understand streaming internals and build custom streamers.

### `core_cpp/pipeline_stateful.cpp`
**What it is**: Core stateful pipeline implementation  
**Why important**: Token limit handling, state management, memory optimization  
**Key insights**: KV-cache handling, conversation management, NPU constraints  
**How to use**: Understand why token limits exist and how to work within them.

### `core_cpp/generation_config.cpp`
**What it is**: Core GenerationConfig implementation  
**Why important**: Parameter validation, defaults, internal logic  
**Key insights**: Default values, parameter interactions, validation rules  
**How to use**: Understand config parameter behavior and defaults.

### `documentation/DEBUG_LOG.md`
**What it is**: Debug logging and troubleshooting guide  
**Why important**: Professional debugging techniques  
**Key insights**: Log levels, performance debugging, error diagnosis  
**How to use**: Enable detailed logging for troubleshooting complex issues.

---

## 🥉 Priority 3 Files (Supporting)

### `python_samples/multinomial_causal_lm.py`
**Advanced sampling techniques for creative and diverse text generation**

### `test_configs/hugging_face.py`  
**Integration patterns with Hugging Face tokenizers and model compatibility**

### `python_bindings/py_llm_pipeline.cpp`
**C++ binding implementation details for advanced troubleshooting and customization**

---

## 🚀 How to Use This Context

### For Your Current Gradio Scripts:

1. **Fix Session Management** (HIGH PRIORITY)
   ```python
   # WRONG (your current approach):
   pipe.generate(full_conversation_prompt, config, streamer)
   
   # RIGHT (from chat_sample.py):
   pipe.start_chat()
   pipe.generate(user_message, config, streamer)  # Only current message!
   pipe.finish_chat()
   ```

2. **Implement Qwen3-Specific Optimizations** (CRITICAL)
   ```python
   # Use complete NPUW configuration from qwen3_model_context/
   from qwen3_model_context.npu_optimization import Qwen3NPUConfigBuilder
   
   builder = Qwen3NPUConfigBuilder("balanced")
   npu_config = builder.build_complete_config()
   pipe = ov_genai.LLMPipeline(model_path, "NPU", **npu_config)
   ```

3. **Add Proper Token Filtering** (HIGH PRIORITY)
   ```python
   # Filter Qwen3's 26+ special tokens in streaming
   from qwen3_model_context.special_tokens import Qwen3StreamingFilter
   
   class EnhancedGradioStreamer(ov_genai.StreamerBase):
       def __init__(self, tokenizer):
           self.filter = Qwen3StreamingFilter()
       
       def put(self, token_id):
           display_text = self.filter.process_token(token_id, token_text)
           if display_text:  # Only show filtered tokens
               self.text_queue.put(display_text)
   ```

4. **Use Official Gradio Patterns**
   - Copy streaming patterns from `gradio_patterns/chatbot_streaming.py`
   - Implement advanced features from `gradio_patterns/chatinterface_advanced.py`
   - Add monitoring from `gradio_patterns/performance_dashboard.py`

5. **Verify GenerationConfig Parameters**
   - Before setting any config parameter, check `test_configs/generation_config.py`
   - Never set unsupported attributes like `pad_token_id`

### For New Development:

1. **Start with** `gradio_patterns/chatbot_streaming.py` for Gradio chat interfaces
2. **Use** `qwen3_model_context/model_architecture.py` for Qwen3-specific configurations
3. **Reference** `generation_config.py` for all configuration options
4. **Study** `HOW_IT_WORKS.md` for architecture understanding  
5. **Benchmark with** `benchmark_genai.py` patterns
6. **Test with** patterns from `gradio_testing/` directory

---

## 🔍 Key Discoveries for Your Project

### 1. Missing Session Management
Your Gradio scripts don't use `start_chat()/finish_chat()` - this could explain token management issues!

### 2. Incomplete NPUW Configuration  
Your NPU config is missing critical Qwen3-specific NPUW settings required for compilation success.

### 3. No Special Token Filtering
Qwen3 has 26+ special tokens that appear in streaming output - you need proper filtering.

### 4. Missing Official Gradio Patterns
Official Gradio demos show more robust patterns than basic streaming implementations.

### 5. Model-Specific Requirements
Qwen3 has unique chat templates, token handling, and memory constraints not covered in general OpenVINO docs.

---

## 📋 Next Steps

### Immediate Actions (Critical):
1. **Implement complete NPUW configuration** from `qwen3_model_context/npu_optimization.py`
2. **Add Qwen3 token filtering** from `qwen3_model_context/special_tokens.py`
3. **Update to proper chat templates** using Qwen3ChatTemplate class
4. **Add session management** with `start_chat()/finish_chat()`

### Integration Improvements:
5. **Copy official Gradio patterns** from `gradio_patterns/` directory
6. **Implement professional monitoring** from `gradio_patterns/performance_dashboard.py`
7. **Add comprehensive testing** using `gradio_testing/` patterns
8. **Study model architecture** in `qwen3_model_context/model_architecture.py`

### Advanced Features:
9. **Add tool calling support** using Qwen3 tool tokens
10. **Implement thinking mode** with Qwen3 reasoning tokens
11. **Create multi-modal interfaces** using vision tokens
12. **Add conversation memory management** within NPU token limits

This enhanced context now provides complete coverage:
- **OpenVINO GenAI implementation** (original context)
- **Official Gradio best practices** (new gradio_patterns/)
- **Professional testing methodologies** (new gradio_testing/)
- **Qwen3-specific optimizations** (new qwen3_model_context/)

🎯 **Everything needed for production-quality Qwen3 + OpenVINO GenAI + Gradio applications!**```

================================================================================
# END OF CONTEXT CONSOLIDATION
================================================================================
